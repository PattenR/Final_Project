Filling queue with 20000 CIFAR images before starting to train. This will take a few minutes.
images.get_shape()
(128, 24, 24, 3)
conv1.get_shape()
(128, 24, 24, 64)
pool1.get_shape()
(128, 12, 12, 64)
norm1.get_shape()
(128, 12, 12, 64)
conv2.get_shape()
(128, 12, 12, 64)
norm2.get_shape()
(128, 12, 12, 64)
2018-03-22 11:32:37.140160: step 0, loss = 4.67 (178.0 examples/sec; 0.719 sec/batch)
2018-03-22 11:32:43.289863: step 10, loss = 4.61 (208.1 examples/sec; 0.615 sec/batch)
2018-03-22 11:32:48.688449: step 20, loss = 4.48 (237.1 examples/sec; 0.540 sec/batch)
2018-03-22 11:32:53.729350: step 30, loss = 4.31 (253.9 examples/sec; 0.504 sec/batch)
2018-03-22 11:32:59.268890: step 40, loss = 4.44 (231.1 examples/sec; 0.554 sec/batch)
2018-03-22 11:33:04.558447: step 50, loss = 4.33 (242.0 examples/sec; 0.529 sec/batch)
2018-03-22 11:33:09.973833: step 60, loss = 4.32 (236.4 examples/sec; 0.542 sec/batch)
2018-03-22 11:33:15.376110: step 70, loss = 4.12 (236.9 examples/sec; 0.540 sec/batch)
2018-03-22 11:33:20.560354: step 80, loss = 4.21 (246.9 examples/sec; 0.518 sec/batch)
2018-03-22 11:33:25.668356: step 90, loss = 3.99 (250.6 examples/sec; 0.511 sec/batch)
2018-03-22 11:33:31.834586: step 100, loss = 4.10 (207.6 examples/sec; 0.617 sec/batch)
2018-03-22 11:33:37.439546: step 110, loss = 4.03 (228.4 examples/sec; 0.560 sec/batch)
2018-03-22 11:33:42.864369: step 120, loss = 3.99 (235.9 examples/sec; 0.543 sec/batch)
2018-03-22 11:33:48.316363: step 130, loss = 3.98 (234.8 examples/sec; 0.545 sec/batch)
2018-03-22 11:33:53.781182: step 140, loss = 4.02 (234.2 examples/sec; 0.546 sec/batch)
2018-03-22 11:33:59.270622: step 150, loss = 3.95 (233.2 examples/sec; 0.549 sec/batch)
2018-03-22 11:34:03.820390: step 160, loss = 3.91 (281.3 examples/sec; 0.455 sec/batch)
2018-03-22 11:34:09.281427: step 170, loss = 3.83 (234.4 examples/sec; 0.546 sec/batch)
2018-03-22 11:34:15.053847: step 180, loss = 3.87 (221.7 examples/sec; 0.577 sec/batch)
2018-03-22 11:34:20.432519: step 190, loss = 3.70 (238.0 examples/sec; 0.538 sec/batch)
2018-03-22 11:34:25.199956: step 200, loss = 3.78 (268.5 examples/sec; 0.477 sec/batch)
2018-03-22 11:34:30.874968: step 210, loss = 3.74 (225.5 examples/sec; 0.568 sec/batch)
2018-03-22 11:34:35.766400: step 220, loss = 3.61 (261.7 examples/sec; 0.489 sec/batch)
2018-03-22 11:34:41.049485: step 230, loss = 3.79 (242.3 examples/sec; 0.528 sec/batch)
2018-03-22 11:34:45.732003: step 240, loss = 3.75 (273.4 examples/sec; 0.468 sec/batch)
2018-03-22 11:34:51.013311: step 250, loss = 3.68 (242.4 examples/sec; 0.528 sec/batch)
2018-03-22 11:34:55.849416: step 260, loss = 3.71 (264.7 examples/sec; 0.484 sec/batch)
2018-03-22 11:35:00.407956: step 270, loss = 3.64 (280.8 examples/sec; 0.456 sec/batch)
2018-03-22 11:35:04.781405: step 280, loss = 3.59 (292.7 examples/sec; 0.437 sec/batch)
2018-03-22 11:35:10.045332: step 290, loss = 3.49 (243.2 examples/sec; 0.526 sec/batch)
2018-03-22 11:35:15.701255: step 300, loss = 3.58 (226.3 examples/sec; 0.566 sec/batch)
2018-03-22 11:35:20.999292: step 310, loss = 3.74 (241.6 examples/sec; 0.530 sec/batch)
2018-03-22 11:35:26.468683: step 320, loss = 3.47 (234.0 examples/sec; 0.547 sec/batch)
2018-03-22 11:35:31.574225: step 330, loss = 3.46 (250.7 examples/sec; 0.511 sec/batch)
2018-03-22 11:35:36.425938: step 340, loss = 3.54 (263.8 examples/sec; 0.485 sec/batch)
2018-03-22 11:35:41.759404: step 350, loss = 3.42 (240.0 examples/sec; 0.533 sec/batch)
2018-03-22 11:35:46.769452: step 360, loss = 3.45 (255.5 examples/sec; 0.501 sec/batch)
2018-03-22 11:35:52.326378: step 370, loss = 3.48 (230.3 examples/sec; 0.556 sec/batch)
2018-03-22 11:35:57.155102: step 380, loss = 3.37 (265.1 examples/sec; 0.483 sec/batch)
2018-03-22 11:36:02.467450: step 390, loss = 3.41 (240.9 examples/sec; 0.531 sec/batch)
2018-03-22 11:36:08.023885: step 400, loss = 3.31 (230.4 examples/sec; 0.556 sec/batch)
2018-03-22 11:36:13.083364: step 410, loss = 3.27 (253.0 examples/sec; 0.506 sec/batch)
2018-03-22 11:36:18.205175: step 420, loss = 3.50 (249.9 examples/sec; 0.512 sec/batch)
2018-03-22 11:36:23.096115: step 430, loss = 3.36 (261.7 examples/sec; 0.489 sec/batch)
2018-03-22 11:36:27.678984: step 440, loss = 3.27 (279.3 examples/sec; 0.458 sec/batch)
2018-03-22 11:36:32.904662: step 450, loss = 3.42 (244.9 examples/sec; 0.523 sec/batch)
2018-03-22 11:36:38.135397: step 460, loss = 3.37 (244.7 examples/sec; 0.523 sec/batch)
2018-03-22 11:36:43.423418: step 470, loss = 3.24 (242.1 examples/sec; 0.529 sec/batch)
2018-03-22 11:36:48.331780: step 480, loss = 3.05 (260.8 examples/sec; 0.491 sec/batch)
2018-03-22 11:36:53.314464: step 490, loss = 3.14 (256.9 examples/sec; 0.498 sec/batch)
2018-03-22 11:36:58.443619: step 500, loss = 2.99 (249.6 examples/sec; 0.513 sec/batch)
2018-03-22 11:37:04.403546: step 510, loss = 3.06 (214.8 examples/sec; 0.596 sec/batch)
2018-03-22 11:37:09.480173: step 520, loss = 3.13 (252.1 examples/sec; 0.508 sec/batch)
2018-03-22 11:37:14.347317: step 530, loss = 3.11 (263.0 examples/sec; 0.487 sec/batch)
2018-03-22 11:37:19.915974: step 540, loss = 3.17 (229.9 examples/sec; 0.557 sec/batch)
2018-03-22 11:37:24.968423: step 550, loss = 3.13 (253.3 examples/sec; 0.505 sec/batch)
2018-03-22 11:37:30.075197: step 560, loss = 3.21 (250.6 examples/sec; 0.511 sec/batch)
2018-03-22 11:37:35.076350: step 570, loss = 3.10 (255.9 examples/sec; 0.500 sec/batch)
2018-03-22 11:37:40.779336: step 580, loss = 3.19 (224.4 examples/sec; 0.570 sec/batch)
2018-03-22 11:37:45.486266: step 590, loss = 3.21 (271.9 examples/sec; 0.471 sec/batch)
2018-03-22 11:37:50.959585: step 600, loss = 3.21 (233.9 examples/sec; 0.547 sec/batch)
2018-03-22 11:37:55.926396: step 610, loss = 3.04 (257.7 examples/sec; 0.497 sec/batch)
2018-03-22 11:38:01.644367: step 620, loss = 2.90 (223.9 examples/sec; 0.572 sec/batch)
2018-03-22 11:38:06.736361: step 630, loss = 2.83 (251.4 examples/sec; 0.509 sec/batch)
2018-03-22 11:38:11.889347: step 640, loss = 2.89 (248.4 examples/sec; 0.515 sec/batch)
2018-03-22 11:38:16.581703: step 650, loss = 2.85 (272.8 examples/sec; 0.469 sec/batch)
2018-03-22 11:38:22.162523: step 660, loss = 2.94 (229.4 examples/sec; 0.558 sec/batch)
2018-03-22 11:38:27.527412: step 670, loss = 2.83 (238.6 examples/sec; 0.536 sec/batch)
2018-03-22 11:38:32.381342: step 680, loss = 3.00 (263.7 examples/sec; 0.485 sec/batch)
2018-03-22 11:38:37.048397: step 690, loss = 2.67 (274.3 examples/sec; 0.467 sec/batch)
2018-03-22 11:38:42.367641: step 700, loss = 2.86 (240.6 examples/sec; 0.532 sec/batch)
2018-03-22 11:38:46.924157: step 710, loss = 2.76 (280.9 examples/sec; 0.456 sec/batch)
2018-03-22 11:38:51.446484: step 720, loss = 2.80 (283.0 examples/sec; 0.452 sec/batch)
2018-03-22 11:38:56.001429: step 730, loss = 2.81 (281.0 examples/sec; 0.455 sec/batch)
2018-03-22 11:39:02.034752: step 740, loss = 2.68 (212.2 examples/sec; 0.603 sec/batch)
2018-03-22 11:39:07.197145: step 750, loss = 2.63 (247.9 examples/sec; 0.516 sec/batch)
2018-03-22 11:39:12.543446: step 760, loss = 2.89 (239.4 examples/sec; 0.535 sec/batch)
2018-03-22 11:39:17.378392: step 770, loss = 2.78 (264.7 examples/sec; 0.483 sec/batch)
2018-03-22 11:39:22.334125: step 780, loss = 2.70 (258.3 examples/sec; 0.496 sec/batch)
2018-03-22 11:39:27.263499: step 790, loss = 2.58 (259.7 examples/sec; 0.493 sec/batch)
2018-03-22 11:39:32.544999: step 800, loss = 2.60 (242.4 examples/sec; 0.528 sec/batch)
2018-03-22 11:39:37.263286: step 810, loss = 2.68 (271.3 examples/sec; 0.472 sec/batch)
2018-03-22 11:39:42.636675: step 820, loss = 2.71 (238.2 examples/sec; 0.537 sec/batch)
2018-03-22 11:39:48.009339: step 830, loss = 2.56 (238.2 examples/sec; 0.537 sec/batch)
2018-03-22 11:39:53.733438: step 840, loss = 2.49 (223.6 examples/sec; 0.572 sec/batch)
2018-03-22 11:39:58.965406: step 850, loss = 2.63 (244.6 examples/sec; 0.523 sec/batch)
2018-03-22 11:40:04.029418: step 860, loss = 2.57 (252.8 examples/sec; 0.506 sec/batch)
2018-03-22 11:40:09.195630: step 870, loss = 2.72 (247.8 examples/sec; 0.517 sec/batch)
2018-03-22 11:40:14.490860: step 880, loss = 2.65 (241.7 examples/sec; 0.530 sec/batch)
2018-03-22 11:40:19.551388: step 890, loss = 2.40 (252.9 examples/sec; 0.506 sec/batch)
2018-03-22 11:40:24.945193: step 900, loss = 2.45 (237.3 examples/sec; 0.539 sec/batch)
2018-03-22 11:40:30.389380: step 910, loss = 2.47 (235.1 examples/sec; 0.544 sec/batch)
2018-03-22 11:40:35.306419: step 920, loss = 2.63 (260.3 examples/sec; 0.492 sec/batch)
2018-03-22 11:40:40.476370: step 930, loss = 2.57 (247.6 examples/sec; 0.517 sec/batch)
2018-03-22 11:40:45.567434: step 940, loss = 2.56 (251.4 examples/sec; 0.509 sec/batch)
2018-03-22 11:40:50.915535: step 950, loss = 2.46 (239.3 examples/sec; 0.535 sec/batch)
2018-03-22 11:40:55.832935: step 960, loss = 2.55 (260.3 examples/sec; 0.492 sec/batch)
2018-03-22 11:41:00.952136: step 970, loss = 2.48 (250.0 examples/sec; 0.512 sec/batch)
2018-03-22 11:41:06.172393: step 980, loss = 2.43 (245.2 examples/sec; 0.522 sec/batch)
2018-03-22 11:41:11.572082: step 990, loss = 2.43 (237.1 examples/sec; 0.540 sec/batch)
2018-03-22 11:41:17.105738: step 1000, loss = 2.61 (231.3 examples/sec; 0.553 sec/batch)
2018-03-22 11:41:22.291439: step 1010, loss = 2.18 (246.8 examples/sec; 0.519 sec/batch)
2018-03-22 11:41:27.427406: step 1020, loss = 2.61 (249.2 examples/sec; 0.514 sec/batch)
2018-03-22 11:41:32.496957: step 1030, loss = 2.39 (252.5 examples/sec; 0.507 sec/batch)
2018-03-22 11:41:37.505894: step 1040, loss = 2.41 (255.5 examples/sec; 0.501 sec/batch)
2018-03-22 11:41:42.804804: step 1050, loss = 2.19 (241.6 examples/sec; 0.530 sec/batch)
2018-03-22 11:41:48.119380: step 1060, loss = 2.48 (240.8 examples/sec; 0.531 sec/batch)
2018-03-22 11:41:53.397611: step 1070, loss = 2.21 (242.5 examples/sec; 0.528 sec/batch)
2018-03-22 11:41:58.850495: step 1080, loss = 2.24 (234.7 examples/sec; 0.545 sec/batch)
2018-03-22 11:42:03.762420: step 1090, loss = 2.21 (260.6 examples/sec; 0.491 sec/batch)
2018-03-22 11:42:08.953563: step 1100, loss = 2.42 (246.6 examples/sec; 0.519 sec/batch)
2018-03-22 11:42:14.394956: step 1110, loss = 2.27 (235.2 examples/sec; 0.544 sec/batch)
2018-03-22 11:42:19.914194: step 1120, loss = 2.30 (231.9 examples/sec; 0.552 sec/batch)
2018-03-22 11:42:24.586742: step 1130, loss = 2.40 (273.9 examples/sec; 0.467 sec/batch)
2018-03-22 11:42:29.116778: step 1140, loss = 2.44 (282.6 examples/sec; 0.453 sec/batch)
2018-03-22 11:42:33.663032: step 1150, loss = 2.10 (281.5 examples/sec; 0.455 sec/batch)
2018-03-22 11:42:38.555711: step 1160, loss = 2.55 (261.6 examples/sec; 0.489 sec/batch)
2018-03-22 11:42:43.663415: step 1170, loss = 2.48 (250.6 examples/sec; 0.511 sec/batch)
2018-03-22 11:42:48.496986: step 1180, loss = 2.41 (264.8 examples/sec; 0.483 sec/batch)
2018-03-22 11:42:53.287407: step 1190, loss = 2.30 (267.2 examples/sec; 0.479 sec/batch)
2018-03-22 11:42:58.950369: step 1200, loss = 2.23 (226.0 examples/sec; 0.566 sec/batch)
2018-03-22 11:43:04.425822: step 1210, loss = 2.12 (233.8 examples/sec; 0.548 sec/batch)
2018-03-22 11:43:09.106233: step 1220, loss = 2.24 (273.5 examples/sec; 0.468 sec/batch)
2018-03-22 11:43:13.587350: step 1230, loss = 2.43 (285.6 examples/sec; 0.448 sec/batch)
2018-03-22 11:43:19.234370: step 1240, loss = 2.10 (226.7 examples/sec; 0.565 sec/batch)
2018-03-22 11:43:24.149132: step 1250, loss = 2.29 (260.4 examples/sec; 0.491 sec/batch)
2018-03-22 11:43:30.033358: step 1260, loss = 2.13 (217.5 examples/sec; 0.588 sec/batch)
2018-03-22 11:43:35.437466: step 1270, loss = 2.09 (236.9 examples/sec; 0.540 sec/batch)
2018-03-22 11:43:40.278321: step 1280, loss = 2.23 (264.4 examples/sec; 0.484 sec/batch)
2018-03-22 11:43:45.443288: step 1290, loss = 2.21 (247.8 examples/sec; 0.516 sec/batch)
2018-03-22 11:43:51.265775: step 1300, loss = 2.19 (219.8 examples/sec; 0.582 sec/batch)
2018-03-22 11:43:56.264590: step 1310, loss = 2.15 (256.1 examples/sec; 0.500 sec/batch)
2018-03-22 11:44:01.714110: step 1320, loss = 2.03 (234.9 examples/sec; 0.545 sec/batch)
2018-03-22 11:44:06.928056: step 1330, loss = 2.12 (245.5 examples/sec; 0.521 sec/batch)
2018-03-22 11:44:11.790407: step 1340, loss = 2.29 (263.2 examples/sec; 0.486 sec/batch)
2018-03-22 11:44:17.069396: step 1350, loss = 1.93 (242.5 examples/sec; 0.528 sec/batch)
2018-03-22 11:44:22.012197: step 1360, loss = 2.39 (259.0 examples/sec; 0.494 sec/batch)
2018-03-22 11:44:27.465401: step 1370, loss = 2.17 (234.7 examples/sec; 0.545 sec/batch)
2018-03-22 11:44:32.621477: step 1380, loss = 2.01 (248.3 examples/sec; 0.516 sec/batch)
2018-03-22 11:44:37.515421: step 1390, loss = 2.07 (261.5 examples/sec; 0.489 sec/batch)
2018-03-22 11:44:42.590934: step 1400, loss = 2.16 (252.2 examples/sec; 0.508 sec/batch)
2018-03-22 11:44:47.467338: step 1410, loss = 1.82 (262.5 examples/sec; 0.488 sec/batch)
2018-03-22 11:44:52.557364: step 1420, loss = 1.95 (251.5 examples/sec; 0.509 sec/batch)
2018-03-22 11:44:57.320306: step 1430, loss = 1.96 (268.7 examples/sec; 0.476 sec/batch)
2018-03-22 11:45:02.140562: step 1440, loss = 1.95 (265.5 examples/sec; 0.482 sec/batch)
2018-03-22 11:45:07.041420: step 1450, loss = 2.09 (261.2 examples/sec; 0.490 sec/batch)
2018-03-22 11:45:12.111119: step 1460, loss = 1.97 (252.5 examples/sec; 0.507 sec/batch)
2018-03-22 11:45:16.859993: step 1470, loss = 1.96 (269.5 examples/sec; 0.475 sec/batch)
2018-03-22 11:45:22.141360: step 1480, loss = 1.84 (242.4 examples/sec; 0.528 sec/batch)
2018-03-22 11:45:27.258370: step 1490, loss = 2.10 (250.1 examples/sec; 0.512 sec/batch)
2018-03-22 11:45:32.789142: step 1500, loss = 1.98 (231.4 examples/sec; 0.553 sec/batch)
2018-03-22 11:45:38.250505: step 1510, loss = 1.84 (234.4 examples/sec; 0.546 sec/batch)
2018-03-22 11:45:43.411415: step 1520, loss = 2.00 (248.0 examples/sec; 0.516 sec/batch)
2018-03-22 11:45:48.067384: step 1530, loss = 1.90 (274.9 examples/sec; 0.466 sec/batch)
2018-03-22 11:45:53.116222: step 1540, loss = 2.02 (253.5 examples/sec; 0.505 sec/batch)
2018-03-22 11:45:58.252805: step 1550, loss = 1.96 (249.2 examples/sec; 0.514 sec/batch)
2018-03-22 11:46:03.598068: step 1560, loss = 1.99 (239.5 examples/sec; 0.535 sec/batch)
2018-03-22 11:46:08.927513: step 1570, loss = 1.96 (240.2 examples/sec; 0.533 sec/batch)
2018-03-22 11:46:13.645461: step 1580, loss = 1.75 (271.3 examples/sec; 0.472 sec/batch)
2018-03-22 11:46:18.409724: step 1590, loss = 2.04 (268.7 examples/sec; 0.476 sec/batch)
2018-03-22 11:46:23.309834: step 1600, loss = 1.85 (261.2 examples/sec; 0.490 sec/batch)
2018-03-22 11:46:27.862312: step 1610, loss = 1.71 (281.2 examples/sec; 0.455 sec/batch)
2018-03-22 11:46:32.907456: step 1620, loss = 1.85 (253.7 examples/sec; 0.505 sec/batch)
2018-03-22 11:46:38.108296: step 1630, loss = 1.85 (246.1 examples/sec; 0.520 sec/batch)
2018-03-22 11:46:42.847430: step 1640, loss = 1.67 (270.1 examples/sec; 0.474 sec/batch)
2018-03-22 11:46:47.975388: step 1650, loss = 1.89 (249.6 examples/sec; 0.513 sec/batch)
2018-03-22 11:46:52.978136: step 1660, loss = 1.84 (255.9 examples/sec; 0.500 sec/batch)
2018-03-22 11:46:57.916433: step 1670, loss = 1.87 (259.2 examples/sec; 0.494 sec/batch)
2018-03-22 11:47:02.998356: step 1680, loss = 1.98 (251.9 examples/sec; 0.508 sec/batch)
2018-03-22 11:47:07.910431: step 1690, loss = 1.85 (260.6 examples/sec; 0.491 sec/batch)
2018-03-22 11:47:13.125833: step 1700, loss = 1.83 (245.4 examples/sec; 0.522 sec/batch)
2018-03-22 11:47:18.351324: step 1710, loss = 1.81 (245.0 examples/sec; 0.523 sec/batch)
2018-03-22 11:47:23.851304: step 1720, loss = 1.99 (232.7 examples/sec; 0.550 sec/batch)
2018-03-22 11:47:29.149405: step 1730, loss = 1.72 (241.6 examples/sec; 0.530 sec/batch)
2018-03-22 11:47:34.197426: step 1740, loss = 1.89 (253.6 examples/sec; 0.505 sec/batch)
2018-03-22 11:47:39.190671: step 1750, loss = 1.82 (256.5 examples/sec; 0.499 sec/batch)
2018-03-22 11:47:44.741141: step 1760, loss = 1.73 (230.5 examples/sec; 0.555 sec/batch)
2018-03-22 11:47:50.015004: step 1770, loss = 1.68 (242.7 examples/sec; 0.527 sec/batch)
2018-03-22 11:47:55.235464: step 1780, loss = 1.58 (245.2 examples/sec; 0.522 sec/batch)
2018-03-22 11:48:00.458323: step 1790, loss = 1.67 (245.1 examples/sec; 0.522 sec/batch)
2018-03-22 11:48:05.306225: step 1800, loss = 1.83 (264.0 examples/sec; 0.485 sec/batch)
2018-03-22 11:48:10.540070: step 1810, loss = 1.71 (244.6 examples/sec; 0.523 sec/batch)
2018-03-22 11:48:15.246398: step 1820, loss = 1.96 (272.0 examples/sec; 0.471 sec/batch)
2018-03-22 11:48:20.800456: step 1830, loss = 1.66 (230.5 examples/sec; 0.555 sec/batch)
2018-03-22 11:48:26.321484: step 1840, loss = 1.66 (231.8 examples/sec; 0.552 sec/batch)
2018-03-22 11:48:31.572471: step 1850, loss = 1.91 (243.8 examples/sec; 0.525 sec/batch)
2018-03-22 11:48:36.338431: step 1860, loss = 1.62 (268.6 examples/sec; 0.477 sec/batch)
2018-03-22 11:48:41.278372: step 1870, loss = 1.64 (259.1 examples/sec; 0.494 sec/batch)
2018-03-22 11:48:46.781443: step 1880, loss = 1.61 (232.6 examples/sec; 0.550 sec/batch)
2018-03-22 11:48:52.045433: step 1890, loss = 1.57 (243.2 examples/sec; 0.526 sec/batch)
2018-03-22 11:48:57.864650: step 1900, loss = 1.67 (220.0 examples/sec; 0.582 sec/batch)
2018-03-22 11:49:02.889897: step 1910, loss = 1.92 (254.7 examples/sec; 0.503 sec/batch)
2018-03-22 11:49:07.764443: step 1920, loss = 1.57 (262.6 examples/sec; 0.487 sec/batch)
2018-03-22 11:49:12.736672: step 1930, loss = 1.68 (257.4 examples/sec; 0.497 sec/batch)
2018-03-22 11:49:17.848448: step 1940, loss = 1.69 (250.4 examples/sec; 0.511 sec/batch)
2018-03-22 11:49:23.601383: step 1950, loss = 1.72 (222.5 examples/sec; 0.575 sec/batch)
2018-03-22 11:49:28.451929: step 1960, loss = 1.61 (263.9 examples/sec; 0.485 sec/batch)
2018-03-22 11:49:34.012972: step 1970, loss = 1.67 (230.2 examples/sec; 0.556 sec/batch)
2018-03-22 11:49:39.272240: step 1980, loss = 1.76 (243.4 examples/sec; 0.526 sec/batch)
2018-03-22 11:49:44.735821: step 1990, loss = 1.77 (234.3 examples/sec; 0.546 sec/batch)
2018-03-22 11:49:49.618508: step 2000, loss = 1.66 (262.2 examples/sec; 0.488 sec/batch)
2018-03-22 11:49:54.613464: step 2010, loss = 1.64 (256.3 examples/sec; 0.499 sec/batch)
2018-03-22 11:49:59.776401: step 2020, loss = 1.75 (247.9 examples/sec; 0.516 sec/batch)
2018-03-22 11:50:04.680911: step 2030, loss = 1.60 (261.0 examples/sec; 0.490 sec/batch)
2018-03-22 11:50:08.898210: step 2040, loss = 1.65 (303.5 examples/sec; 0.422 sec/batch)
2018-03-22 11:50:13.290852: step 2050, loss = 1.50 (291.4 examples/sec; 0.439 sec/batch)
2018-03-22 11:50:18.459311: step 2060, loss = 1.50 (247.7 examples/sec; 0.517 sec/batch)
2018-03-22 11:50:24.004428: step 2070, loss = 1.54 (230.8 examples/sec; 0.555 sec/batch)
2018-03-22 11:50:29.036458: step 2080, loss = 1.42 (254.4 examples/sec; 0.503 sec/batch)
2018-03-22 11:50:33.951111: step 2090, loss = 1.59 (260.4 examples/sec; 0.491 sec/batch)
2018-03-22 11:50:39.404328: step 2100, loss = 1.46 (234.7 examples/sec; 0.545 sec/batch)
2018-03-22 11:50:44.762684: step 2110, loss = 1.65 (238.9 examples/sec; 0.536 sec/batch)
2018-03-22 11:50:50.228306: step 2120, loss = 1.73 (234.2 examples/sec; 0.547 sec/batch)
2018-03-22 11:50:54.801522: step 2130, loss = 1.63 (279.9 examples/sec; 0.457 sec/batch)
2018-03-22 11:51:00.238946: step 2140, loss = 1.62 (235.4 examples/sec; 0.544 sec/batch)
2018-03-22 11:51:05.695708: step 2150, loss = 1.59 (234.6 examples/sec; 0.546 sec/batch)
2018-03-22 11:51:10.583377: step 2160, loss = 1.48 (261.9 examples/sec; 0.489 sec/batch)
2018-03-22 11:51:15.237400: step 2170, loss = 1.54 (275.0 examples/sec; 0.465 sec/batch)
2018-03-22 11:51:20.518368: step 2180, loss = 1.39 (242.4 examples/sec; 0.528 sec/batch)
2018-03-22 11:51:25.798439: step 2190, loss = 1.66 (242.4 examples/sec; 0.528 sec/batch)
2018-03-22 11:51:30.995052: step 2200, loss = 1.56 (246.3 examples/sec; 0.520 sec/batch)
2018-03-22 11:51:35.665369: step 2210, loss = 1.51 (274.1 examples/sec; 0.467 sec/batch)
2018-03-22 11:51:40.717830: step 2220, loss = 1.56 (253.3 examples/sec; 0.505 sec/batch)
2018-03-22 11:51:45.949380: step 2230, loss = 1.49 (244.7 examples/sec; 0.523 sec/batch)
2018-03-22 11:51:51.790419: step 2240, loss = 1.50 (219.1 examples/sec; 0.584 sec/batch)
2018-03-22 11:51:57.050707: step 2250, loss = 1.51 (243.3 examples/sec; 0.526 sec/batch)
2018-03-22 11:52:01.783360: step 2260, loss = 1.51 (270.5 examples/sec; 0.473 sec/batch)
2018-03-22 11:52:06.361881: step 2270, loss = 1.54 (279.6 examples/sec; 0.458 sec/batch)
2018-03-22 11:52:11.826402: step 2280, loss = 1.53 (234.2 examples/sec; 0.546 sec/batch)
2018-03-22 11:52:17.073325: step 2290, loss = 1.34 (244.0 examples/sec; 0.525 sec/batch)
2018-03-22 11:52:22.234164: step 2300, loss = 1.51 (248.0 examples/sec; 0.516 sec/batch)
2018-03-22 11:52:27.287112: step 2310, loss = 1.59 (253.3 examples/sec; 0.505 sec/batch)
2018-03-22 11:52:33.178784: step 2320, loss = 1.71 (217.3 examples/sec; 0.589 sec/batch)
2018-03-22 11:52:38.711723: step 2330, loss = 1.44 (231.3 examples/sec; 0.553 sec/batch)
2018-03-22 11:52:43.561379: step 2340, loss = 1.60 (263.9 examples/sec; 0.485 sec/batch)
2018-03-22 11:52:49.080434: step 2350, loss = 1.62 (231.9 examples/sec; 0.552 sec/batch)
2018-03-22 11:52:54.557661: step 2360, loss = 1.66 (233.7 examples/sec; 0.548 sec/batch)
2018-03-22 11:52:59.654368: step 2370, loss = 1.46 (251.1 examples/sec; 0.510 sec/batch)
2018-03-22 11:53:04.642387: step 2380, loss = 1.47 (256.6 examples/sec; 0.499 sec/batch)
2018-03-22 11:53:09.703408: step 2390, loss = 1.48 (252.9 examples/sec; 0.506 sec/batch)
2018-03-22 11:53:15.243957: step 2400, loss = 1.46 (231.0 examples/sec; 0.554 sec/batch)
2018-03-22 11:53:20.039351: step 2410, loss = 1.51 (266.9 examples/sec; 0.480 sec/batch)
2018-03-22 11:53:24.502935: step 2420, loss = 1.65 (286.8 examples/sec; 0.446 sec/batch)
2018-03-22 11:53:29.530408: step 2430, loss = 1.43 (254.6 examples/sec; 0.503 sec/batch)
2018-03-22 11:53:34.530424: step 2440, loss = 1.47 (256.0 examples/sec; 0.500 sec/batch)
2018-03-22 11:53:39.214639: step 2450, loss = 1.39 (273.3 examples/sec; 0.468 sec/batch)
2018-03-22 11:53:43.699566: step 2460, loss = 1.40 (285.4 examples/sec; 0.448 sec/batch)
2018-03-22 11:53:48.195355: step 2470, loss = 1.42 (284.7 examples/sec; 0.450 sec/batch)
2018-03-22 11:53:53.493358: step 2480, loss = 1.49 (241.6 examples/sec; 0.530 sec/batch)
2018-03-22 11:53:57.746957: step 2490, loss = 1.52 (300.9 examples/sec; 0.425 sec/batch)
2018-03-22 11:54:02.789297: step 2500, loss = 1.44 (253.9 examples/sec; 0.504 sec/batch)
2018-03-22 11:54:08.225438: step 2510, loss = 1.56 (235.5 examples/sec; 0.544 sec/batch)
2018-03-22 11:54:14.060406: step 2520, loss = 1.40 (219.4 examples/sec; 0.583 sec/batch)
2018-03-22 11:54:18.879369: step 2530, loss = 1.49 (265.6 examples/sec; 0.482 sec/batch)
2018-03-22 11:54:23.449369: step 2540, loss = 1.40 (280.1 examples/sec; 0.457 sec/batch)
2018-03-22 11:54:28.166496: step 2550, loss = 1.40 (271.4 examples/sec; 0.472 sec/batch)
2018-03-22 11:54:33.320402: step 2560, loss = 1.39 (248.4 examples/sec; 0.515 sec/batch)
2018-03-22 11:54:38.274414: step 2570, loss = 1.51 (258.4 examples/sec; 0.495 sec/batch)
2018-03-22 11:54:43.596349: step 2580, loss = 1.37 (240.5 examples/sec; 0.532 sec/batch)
2018-03-22 11:54:48.859410: step 2590, loss = 1.44 (243.2 examples/sec; 0.526 sec/batch)
2018-03-22 11:54:54.330343: step 2600, loss = 1.50 (234.0 examples/sec; 0.547 sec/batch)
2018-03-22 11:54:59.138409: step 2610, loss = 1.57 (266.2 examples/sec; 0.481 sec/batch)
2018-03-22 11:55:04.422414: step 2620, loss = 1.31 (242.2 examples/sec; 0.528 sec/batch)
2018-03-22 11:55:10.112385: step 2630, loss = 1.53 (225.0 examples/sec; 0.569 sec/batch)
2018-03-22 11:55:15.014402: step 2640, loss = 1.25 (261.1 examples/sec; 0.490 sec/batch)
2018-03-22 11:55:20.354453: step 2650, loss = 1.50 (239.7 examples/sec; 0.534 sec/batch)
2018-03-22 11:55:25.498478: step 2660, loss = 1.28 (248.8 examples/sec; 0.514 sec/batch)
2018-03-22 11:55:30.726396: step 2670, loss = 1.52 (244.8 examples/sec; 0.523 sec/batch)
2018-03-22 11:55:35.554436: step 2680, loss = 1.35 (265.1 examples/sec; 0.483 sec/batch)
2018-03-22 11:55:40.886392: step 2690, loss = 1.27 (240.1 examples/sec; 0.533 sec/batch)
2018-03-22 11:55:46.280906: step 2700, loss = 1.24 (237.3 examples/sec; 0.539 sec/batch)
2018-03-22 11:55:50.897008: step 2710, loss = 1.29 (277.3 examples/sec; 0.462 sec/batch)
2018-03-22 11:55:56.167386: step 2720, loss = 1.49 (242.9 examples/sec; 0.527 sec/batch)
2018-03-22 11:56:01.528446: step 2730, loss = 1.35 (238.8 examples/sec; 0.536 sec/batch)
2018-03-22 11:56:06.725941: step 2740, loss = 1.44 (246.3 examples/sec; 0.520 sec/batch)
2018-03-22 11:56:11.830350: step 2750, loss = 1.31 (250.8 examples/sec; 0.510 sec/batch)
2018-03-22 11:56:16.671585: step 2760, loss = 1.26 (264.4 examples/sec; 0.484 sec/batch)
2018-03-22 11:56:21.924504: step 2770, loss = 1.22 (243.7 examples/sec; 0.525 sec/batch)
2018-03-22 11:56:26.978403: step 2780, loss = 1.32 (253.3 examples/sec; 0.505 sec/batch)
2018-03-22 11:56:32.682641: step 2790, loss = 1.30 (224.4 examples/sec; 0.570 sec/batch)
2018-03-22 11:56:38.655458: step 2800, loss = 1.63 (214.3 examples/sec; 0.597 sec/batch)
2018-03-22 11:56:43.806925: step 2810, loss = 1.10 (248.5 examples/sec; 0.515 sec/batch)
2018-03-22 11:56:48.587399: step 2820, loss = 1.27 (267.8 examples/sec; 0.478 sec/batch)
2018-03-22 11:56:53.408849: step 2830, loss = 1.50 (265.5 examples/sec; 0.482 sec/batch)
2018-03-22 11:56:58.825009: step 2840, loss = 1.33 (236.3 examples/sec; 0.542 sec/batch)
2018-03-22 11:57:03.599297: step 2850, loss = 1.33 (268.1 examples/sec; 0.477 sec/batch)
2018-03-22 11:57:08.798270: step 2860, loss = 1.41 (246.2 examples/sec; 0.520 sec/batch)
2018-03-22 11:57:13.614419: step 2870, loss = 1.21 (265.8 examples/sec; 0.482 sec/batch)
2018-03-22 11:57:18.814370: step 2880, loss = 1.40 (246.2 examples/sec; 0.520 sec/batch)
2018-03-22 11:57:23.928887: step 2890, loss = 1.18 (250.3 examples/sec; 0.511 sec/batch)
2018-03-22 11:57:29.685617: step 2900, loss = 1.33 (222.3 examples/sec; 0.576 sec/batch)
2018-03-22 11:57:34.627821: step 2910, loss = 1.19 (259.0 examples/sec; 0.494 sec/batch)
2018-03-22 11:57:39.206035: step 2920, loss = 1.46 (279.6 examples/sec; 0.458 sec/batch)
2018-03-22 11:57:44.075376: step 2930, loss = 1.34 (262.9 examples/sec; 0.487 sec/batch)
2018-03-22 11:57:49.664223: step 2940, loss = 1.53 (229.0 examples/sec; 0.559 sec/batch)
2018-03-22 11:57:55.000434: step 2950, loss = 1.32 (239.9 examples/sec; 0.534 sec/batch)
2018-03-22 11:58:00.015442: step 2960, loss = 1.19 (255.2 examples/sec; 0.502 sec/batch)
2018-03-22 11:58:04.642435: step 2970, loss = 1.34 (276.6 examples/sec; 0.463 sec/batch)
2018-03-22 11:58:09.692817: step 2980, loss = 1.32 (253.4 examples/sec; 0.505 sec/batch)
2018-03-22 11:58:14.923627: step 2990, loss = 1.23 (244.7 examples/sec; 0.523 sec/batch)
2018-03-22 11:58:20.233509: step 3000, loss = 1.24 (241.1 examples/sec; 0.531 sec/batch)
2018-03-22 11:58:25.093799: step 3010, loss = 1.40 (263.4 examples/sec; 0.486 sec/batch)
2018-03-22 11:58:30.189332: step 3020, loss = 1.41 (251.2 examples/sec; 0.510 sec/batch)
2018-03-22 11:58:34.965702: step 3030, loss = 1.20 (268.0 examples/sec; 0.478 sec/batch)
2018-03-22 11:58:39.998059: step 3040, loss = 1.52 (254.4 examples/sec; 0.503 sec/batch)
2018-03-22 11:58:45.324546: step 3050, loss = 1.29 (240.3 examples/sec; 0.533 sec/batch)
2018-03-22 11:58:50.946413: step 3060, loss = 1.18 (227.7 examples/sec; 0.562 sec/batch)
2018-03-22 11:58:55.709297: step 3070, loss = 1.31 (268.7 examples/sec; 0.476 sec/batch)
2018-03-22 11:59:00.960732: step 3080, loss = 1.17 (243.7 examples/sec; 0.525 sec/batch)
2018-03-22 11:59:06.367467: step 3090, loss = 1.19 (236.7 examples/sec; 0.541 sec/batch)
2018-03-22 11:59:11.716999: step 3100, loss = 1.20 (239.3 examples/sec; 0.535 sec/batch)
2018-03-22 11:59:16.880661: step 3110, loss = 1.27 (247.9 examples/sec; 0.516 sec/batch)
2018-03-22 11:59:21.805110: step 3120, loss = 1.33 (259.9 examples/sec; 0.492 sec/batch)
2018-03-22 11:59:26.348489: step 3130, loss = 1.22 (281.7 examples/sec; 0.454 sec/batch)
2018-03-22 11:59:31.450394: step 3140, loss = 1.31 (250.9 examples/sec; 0.510 sec/batch)
2018-03-22 11:59:36.326396: step 3150, loss = 1.26 (262.5 examples/sec; 0.488 sec/batch)
2018-03-22 11:59:41.549351: step 3160, loss = 1.27 (245.1 examples/sec; 0.522 sec/batch)
2018-03-22 11:59:46.776473: step 3170, loss = 1.23 (244.9 examples/sec; 0.523 sec/batch)
2018-03-22 11:59:52.101421: step 3180, loss = 1.33 (240.4 examples/sec; 0.532 sec/batch)
2018-03-22 11:59:56.861762: step 3190, loss = 1.18 (268.9 examples/sec; 0.476 sec/batch)
2018-03-22 12:00:02.228390: step 3200, loss = 1.14 (238.5 examples/sec; 0.537 sec/batch)
2018-03-22 12:00:07.633833: step 3210, loss = 1.20 (236.8 examples/sec; 0.541 sec/batch)
2018-03-22 12:00:12.597399: step 3220, loss = 1.14 (257.9 examples/sec; 0.496 sec/batch)
2018-03-22 12:00:17.647410: step 3230, loss = 1.15 (253.5 examples/sec; 0.505 sec/batch)
2018-03-22 12:00:22.730386: step 3240, loss = 1.39 (251.8 examples/sec; 0.508 sec/batch)
2018-03-22 12:00:27.578340: step 3250, loss = 1.19 (264.0 examples/sec; 0.485 sec/batch)
2018-03-22 12:00:32.618420: step 3260, loss = 1.46 (254.0 examples/sec; 0.504 sec/batch)
2018-03-22 12:00:37.513381: step 3270, loss = 1.21 (261.5 examples/sec; 0.489 sec/batch)
2018-03-22 12:00:42.481379: step 3280, loss = 1.44 (257.6 examples/sec; 0.497 sec/batch)
2018-03-22 12:00:47.421345: step 3290, loss = 1.26 (259.1 examples/sec; 0.494 sec/batch)
2018-03-22 12:00:52.488337: step 3300, loss = 1.20 (252.6 examples/sec; 0.507 sec/batch)
2018-03-22 12:00:57.478369: step 3310, loss = 1.28 (256.5 examples/sec; 0.499 sec/batch)
2018-03-22 12:01:02.632219: step 3320, loss = 1.27 (248.4 examples/sec; 0.515 sec/batch)
2018-03-22 12:01:07.423370: step 3330, loss = 1.26 (267.2 examples/sec; 0.479 sec/batch)
2018-03-22 12:01:12.741403: step 3340, loss = 1.14 (240.7 examples/sec; 0.532 sec/batch)
2018-03-22 12:01:17.620451: step 3350, loss = 1.24 (262.3 examples/sec; 0.488 sec/batch)
2018-03-22 12:01:22.133242: step 3360, loss = 1.06 (283.6 examples/sec; 0.451 sec/batch)
2018-03-22 12:01:26.680621: step 3370, loss = 1.29 (281.5 examples/sec; 0.455 sec/batch)
2018-03-22 12:01:31.578370: step 3380, loss = 1.29 (261.3 examples/sec; 0.490 sec/batch)
2018-03-22 12:01:36.979393: step 3390, loss = 1.03 (237.0 examples/sec; 0.540 sec/batch)
2018-03-22 12:01:42.725510: step 3400, loss = 1.35 (222.8 examples/sec; 0.575 sec/batch)
2018-03-22 12:01:47.801881: step 3410, loss = 1.01 (252.1 examples/sec; 0.508 sec/batch)
2018-03-22 12:01:53.020762: step 3420, loss = 1.06 (245.3 examples/sec; 0.522 sec/batch)
2018-03-22 12:01:58.023396: step 3430, loss = 1.33 (255.9 examples/sec; 0.500 sec/batch)
2018-03-22 12:02:03.240965: step 3440, loss = 1.35 (245.3 examples/sec; 0.522 sec/batch)
2018-03-22 12:02:08.358414: step 3450, loss = 1.22 (250.1 examples/sec; 0.512 sec/batch)
2018-03-22 12:02:13.228654: step 3460, loss = 1.13 (262.8 examples/sec; 0.487 sec/batch)
2018-03-22 12:02:18.561457: step 3470, loss = 1.17 (240.0 examples/sec; 0.533 sec/batch)
2018-03-22 12:02:23.865256: step 3480, loss = 1.21 (241.3 examples/sec; 0.530 sec/batch)
2018-03-22 12:02:28.228367: step 3490, loss = 1.27 (293.4 examples/sec; 0.436 sec/batch)
2018-03-22 12:02:33.920155: step 3500, loss = 1.23 (224.9 examples/sec; 0.569 sec/batch)
2018-03-22 12:02:39.559998: step 3510, loss = 1.16 (227.0 examples/sec; 0.564 sec/batch)
2018-03-22 12:02:45.178310: step 3520, loss = 1.11 (227.8 examples/sec; 0.562 sec/batch)
2018-03-22 12:02:50.755405: step 3530, loss = 1.20 (229.5 examples/sec; 0.558 sec/batch)
2018-03-22 12:02:55.820380: step 3540, loss = 1.09 (252.7 examples/sec; 0.506 sec/batch)
2018-03-22 12:03:01.273405: step 3550, loss = 1.15 (234.7 examples/sec; 0.545 sec/batch)
2018-03-22 12:03:06.295323: step 3560, loss = 1.08 (254.9 examples/sec; 0.502 sec/batch)
2018-03-22 12:03:11.443912: step 3570, loss = 1.02 (248.6 examples/sec; 0.515 sec/batch)
2018-03-22 12:03:16.528714: step 3580, loss = 1.15 (251.7 examples/sec; 0.508 sec/batch)
2018-03-22 12:03:21.488517: step 3590, loss = 1.20 (258.1 examples/sec; 0.496 sec/batch)
2018-03-22 12:03:26.300968: step 3600, loss = 1.25 (266.0 examples/sec; 0.481 sec/batch)
2018-03-22 12:03:31.714149: step 3610, loss = 1.30 (236.5 examples/sec; 0.541 sec/batch)
2018-03-22 12:03:36.422451: step 3620, loss = 1.11 (271.9 examples/sec; 0.471 sec/batch)
2018-03-22 12:03:41.472402: step 3630, loss = 1.27 (253.5 examples/sec; 0.505 sec/batch)
2018-03-22 12:03:46.099795: step 3640, loss = 1.30 (276.6 examples/sec; 0.463 sec/batch)
2018-03-22 12:03:51.743655: step 3650, loss = 1.32 (226.8 examples/sec; 0.564 sec/batch)
2018-03-22 12:03:56.831524: step 3660, loss = 1.09 (251.6 examples/sec; 0.509 sec/batch)
2018-03-22 12:04:02.390385: step 3670, loss = 1.31 (230.3 examples/sec; 0.556 sec/batch)
2018-03-22 12:04:07.269467: step 3680, loss = 1.30 (262.3 examples/sec; 0.488 sec/batch)
2018-03-22 12:04:12.161568: step 3690, loss = 1.17 (261.6 examples/sec; 0.489 sec/batch)
2018-03-22 12:04:16.980459: step 3700, loss = 1.25 (265.6 examples/sec; 0.482 sec/batch)
2018-03-22 12:04:22.559817: step 3710, loss = 1.28 (229.4 examples/sec; 0.558 sec/batch)
2018-03-22 12:04:27.557697: step 3720, loss = 1.22 (256.1 examples/sec; 0.500 sec/batch)
2018-03-22 12:04:33.407431: step 3730, loss = 1.21 (218.8 examples/sec; 0.585 sec/batch)
2018-03-22 12:04:38.056421: step 3740, loss = 1.13 (275.3 examples/sec; 0.465 sec/batch)
2018-03-22 12:04:42.871376: step 3750, loss = 1.32 (265.8 examples/sec; 0.481 sec/batch)
2018-03-22 12:04:47.620423: step 3760, loss = 1.22 (269.5 examples/sec; 0.475 sec/batch)
2018-03-22 12:04:52.736600: step 3770, loss = 1.04 (250.2 examples/sec; 0.512 sec/batch)
2018-03-22 12:04:57.313426: step 3780, loss = 1.21 (279.7 examples/sec; 0.458 sec/batch)
2018-03-22 12:05:02.802372: step 3790, loss = 1.14 (233.2 examples/sec; 0.549 sec/batch)
2018-03-22 12:05:07.628820: step 3800, loss = 1.02 (265.2 examples/sec; 0.483 sec/batch)
2018-03-22 12:05:12.175381: step 3810, loss = 1.17 (281.5 examples/sec; 0.455 sec/batch)
2018-03-22 12:05:16.612237: step 3820, loss = 1.13 (288.5 examples/sec; 0.444 sec/batch)
2018-03-22 12:05:21.706414: step 3830, loss = 1.12 (251.3 examples/sec; 0.509 sec/batch)
2018-03-22 12:05:26.791384: step 3840, loss = 1.15 (251.7 examples/sec; 0.508 sec/batch)
2018-03-22 12:05:32.161009: step 3850, loss = 1.20 (238.4 examples/sec; 0.537 sec/batch)
2018-03-22 12:05:37.273336: step 3860, loss = 1.17 (250.4 examples/sec; 0.511 sec/batch)
2018-03-22 12:05:42.737305: step 3870, loss = 1.10 (234.3 examples/sec; 0.546 sec/batch)
2018-03-22 12:05:47.229614: step 3880, loss = 1.22 (284.9 examples/sec; 0.449 sec/batch)
2018-03-22 12:05:52.398393: step 3890, loss = 1.06 (247.6 examples/sec; 0.517 sec/batch)
2018-03-22 12:05:57.534262: step 3900, loss = 1.01 (249.2 examples/sec; 0.514 sec/batch)
2018-03-22 12:06:02.762951: step 3910, loss = 1.08 (244.8 examples/sec; 0.523 sec/batch)
2018-03-22 12:06:07.374071: step 3920, loss = 1.08 (277.6 examples/sec; 0.461 sec/batch)
2018-03-22 12:06:12.375498: step 3930, loss = 1.14 (255.9 examples/sec; 0.500 sec/batch)
2018-03-22 12:06:17.760677: step 3940, loss = 0.98 (237.7 examples/sec; 0.539 sec/batch)
2018-03-22 12:06:23.135400: step 3950, loss = 1.12 (238.2 examples/sec; 0.537 sec/batch)
2018-03-22 12:06:28.552427: step 3960, loss = 1.10 (236.3 examples/sec; 0.542 sec/batch)
2018-03-22 12:06:33.961395: step 3970, loss = 1.14 (236.6 examples/sec; 0.541 sec/batch)
2018-03-22 12:06:38.973414: step 3980, loss = 1.20 (255.4 examples/sec; 0.501 sec/batch)
2018-03-22 12:06:43.732348: step 3990, loss = 1.16 (269.0 examples/sec; 0.476 sec/batch)
2018-03-22 12:06:49.761046: step 4000, loss = 1.19 (212.3 examples/sec; 0.603 sec/batch)
2018-03-22 12:06:55.412605: step 4010, loss = 1.08 (226.5 examples/sec; 0.565 sec/batch)
2018-03-22 12:07:00.661476: step 4020, loss = 1.17 (243.9 examples/sec; 0.525 sec/batch)
2018-03-22 12:07:04.979197: step 4030, loss = 1.02 (296.5 examples/sec; 0.432 sec/batch)
2018-03-22 12:07:10.715434: step 4040, loss = 0.93 (223.1 examples/sec; 0.574 sec/batch)
2018-03-22 12:07:16.266147: step 4050, loss = 0.98 (230.6 examples/sec; 0.555 sec/batch)
2018-03-22 12:07:21.243817: step 4060, loss = 1.07 (257.1 examples/sec; 0.498 sec/batch)
2018-03-22 12:07:25.937970: step 4070, loss = 1.07 (272.7 examples/sec; 0.469 sec/batch)
2018-03-22 12:07:30.976755: step 4080, loss = 0.88 (254.0 examples/sec; 0.504 sec/batch)
2018-03-22 12:07:36.188346: step 4090, loss = 1.16 (245.6 examples/sec; 0.521 sec/batch)
2018-03-22 12:07:41.685250: step 4100, loss = 1.08 (232.9 examples/sec; 0.550 sec/batch)
2018-03-22 12:07:46.588738: step 4110, loss = 1.05 (261.0 examples/sec; 0.490 sec/batch)
2018-03-22 12:07:51.439506: step 4120, loss = 1.03 (263.9 examples/sec; 0.485 sec/batch)
2018-03-22 12:07:56.331406: step 4130, loss = 1.12 (261.7 examples/sec; 0.489 sec/batch)
2018-03-22 12:08:01.790943: step 4140, loss = 0.87 (234.5 examples/sec; 0.546 sec/batch)
2018-03-22 12:08:06.599450: step 4150, loss = 0.94 (266.2 examples/sec; 0.481 sec/batch)
2018-03-22 12:08:11.524353: step 4160, loss = 1.18 (259.9 examples/sec; 0.492 sec/batch)
2018-03-22 12:08:16.879716: step 4170, loss = 1.02 (239.0 examples/sec; 0.536 sec/batch)
2018-03-22 12:08:22.052834: step 4180, loss = 0.97 (247.4 examples/sec; 0.517 sec/batch)
2018-03-22 12:08:26.758416: step 4190, loss = 1.11 (272.0 examples/sec; 0.471 sec/batch)
2018-03-22 12:08:32.188431: step 4200, loss = 0.88 (235.7 examples/sec; 0.543 sec/batch)
2018-03-22 12:08:36.690089: step 4210, loss = 0.97 (284.3 examples/sec; 0.450 sec/batch)
2018-03-22 12:08:41.812943: step 4220, loss = 0.97 (249.9 examples/sec; 0.512 sec/batch)
2018-03-22 12:08:46.493853: step 4230, loss = 1.18 (273.5 examples/sec; 0.468 sec/batch)
2018-03-22 12:08:51.717406: step 4240, loss = 1.10 (245.0 examples/sec; 0.522 sec/batch)
2018-03-22 12:08:56.714183: step 4250, loss = 1.00 (256.2 examples/sec; 0.500 sec/batch)
2018-03-22 12:09:01.560419: step 4260, loss = 1.19 (264.1 examples/sec; 0.485 sec/batch)
2018-03-22 12:09:05.883102: step 4270, loss = 1.20 (296.1 examples/sec; 0.432 sec/batch)
2018-03-22 12:09:11.014432: step 4280, loss = 1.08 (249.4 examples/sec; 0.513 sec/batch)
2018-03-22 12:09:15.812475: step 4290, loss = 1.04 (266.9 examples/sec; 0.480 sec/batch)
2018-03-22 12:09:21.331401: step 4300, loss = 1.23 (231.8 examples/sec; 0.552 sec/batch)
2018-03-22 12:09:26.480390: step 4310, loss = 0.90 (248.6 examples/sec; 0.515 sec/batch)
2018-03-22 12:09:31.625464: step 4320, loss = 1.01 (248.8 examples/sec; 0.515 sec/batch)
2018-03-22 12:09:36.566647: step 4330, loss = 1.09 (259.0 examples/sec; 0.494 sec/batch)
2018-03-22 12:09:42.321468: step 4340, loss = 1.05 (222.4 examples/sec; 0.575 sec/batch)
2018-03-22 12:09:47.555472: step 4350, loss = 1.22 (244.6 examples/sec; 0.523 sec/batch)
2018-03-22 12:09:52.658701: step 4360, loss = 1.04 (250.8 examples/sec; 0.510 sec/batch)
2018-03-22 12:09:57.479360: step 4370, loss = 1.15 (265.5 examples/sec; 0.482 sec/batch)
2018-03-22 12:10:02.622916: step 4380, loss = 0.97 (248.9 examples/sec; 0.514 sec/batch)
2018-03-22 12:10:07.814422: step 4390, loss = 1.05 (246.6 examples/sec; 0.519 sec/batch)
2018-03-22 12:10:12.983657: step 4400, loss = 0.98 (247.6 examples/sec; 0.517 sec/batch)
2018-03-22 12:10:17.878419: step 4410, loss = 0.92 (261.5 examples/sec; 0.489 sec/batch)
2018-03-22 12:10:23.425920: step 4420, loss = 1.12 (230.7 examples/sec; 0.555 sec/batch)
2018-03-22 12:10:28.531386: step 4430, loss = 1.01 (250.7 examples/sec; 0.511 sec/batch)
2018-03-22 12:10:33.679366: step 4440, loss = 1.00 (248.6 examples/sec; 0.515 sec/batch)
2018-03-22 12:10:38.839454: step 4450, loss = 1.29 (248.1 examples/sec; 0.516 sec/batch)
2018-03-22 12:10:44.142495: step 4460, loss = 1.06 (241.4 examples/sec; 0.530 sec/batch)
2018-03-22 12:10:48.738469: step 4470, loss = 0.90 (278.5 examples/sec; 0.460 sec/batch)
2018-03-22 12:10:53.863819: step 4480, loss = 1.00 (249.7 examples/sec; 0.513 sec/batch)
2018-03-22 12:10:59.279027: step 4490, loss = 1.02 (236.4 examples/sec; 0.542 sec/batch)
2018-03-22 12:11:05.205257: step 4500, loss = 0.88 (216.0 examples/sec; 0.593 sec/batch)
2018-03-22 12:11:10.358787: step 4510, loss = 0.98 (248.4 examples/sec; 0.515 sec/batch)
2018-03-22 12:11:15.030702: step 4520, loss = 1.10 (274.0 examples/sec; 0.467 sec/batch)
2018-03-22 12:11:20.017827: step 4530, loss = 1.07 (256.7 examples/sec; 0.499 sec/batch)
2018-03-22 12:11:25.482427: step 4540, loss = 0.97 (234.2 examples/sec; 0.546 sec/batch)
2018-03-22 12:11:30.394395: step 4550, loss = 0.92 (260.6 examples/sec; 0.491 sec/batch)
2018-03-22 12:11:36.009380: step 4560, loss = 1.34 (228.0 examples/sec; 0.561 sec/batch)
2018-03-22 12:11:41.204476: step 4570, loss = 1.10 (246.4 examples/sec; 0.520 sec/batch)
2018-03-22 12:11:45.450378: step 4580, loss = 1.04 (301.5 examples/sec; 0.425 sec/batch)
2018-03-22 12:11:50.996372: step 4590, loss = 1.14 (230.8 examples/sec; 0.555 sec/batch)
2018-03-22 12:11:56.059322: step 4600, loss = 0.91 (252.8 examples/sec; 0.506 sec/batch)
2018-03-22 12:12:01.190435: step 4610, loss = 0.90 (249.5 examples/sec; 0.513 sec/batch)
2018-03-22 12:12:05.986075: step 4620, loss = 0.98 (266.9 examples/sec; 0.480 sec/batch)
2018-03-22 12:12:11.324480: step 4630, loss = 0.99 (239.8 examples/sec; 0.534 sec/batch)
2018-03-22 12:12:16.048621: step 4640, loss = 0.91 (270.9 examples/sec; 0.472 sec/batch)
2018-03-22 12:12:21.087763: step 4650, loss = 1.01 (254.0 examples/sec; 0.504 sec/batch)
2018-03-22 12:12:25.551431: step 4660, loss = 1.14 (286.8 examples/sec; 0.446 sec/batch)
2018-03-22 12:12:30.431267: step 4670, loss = 0.94 (262.3 examples/sec; 0.488 sec/batch)
2018-03-22 12:12:34.818443: step 4680, loss = 0.85 (291.8 examples/sec; 0.439 sec/batch)
2018-03-22 12:12:39.883179: step 4690, loss = 1.03 (252.7 examples/sec; 0.506 sec/batch)
2018-03-22 12:12:44.904782: step 4700, loss = 1.02 (254.9 examples/sec; 0.502 sec/batch)
2018-03-22 12:12:49.485414: step 4710, loss = 0.94 (279.4 examples/sec; 0.458 sec/batch)
2018-03-22 12:12:54.066504: step 4720, loss = 1.10 (279.4 examples/sec; 0.458 sec/batch)
2018-03-22 12:12:59.564440: step 4730, loss = 1.25 (232.8 examples/sec; 0.550 sec/batch)
2018-03-22 12:13:05.204401: step 4740, loss = 1.05 (227.0 examples/sec; 0.564 sec/batch)
2018-03-22 12:13:10.013494: step 4750, loss = 0.89 (266.2 examples/sec; 0.481 sec/batch)
2018-03-22 12:13:14.805197: step 4760, loss = 1.04 (267.1 examples/sec; 0.479 sec/batch)
2018-03-22 12:13:19.902420: step 4770, loss = 1.24 (251.1 examples/sec; 0.510 sec/batch)
2018-03-22 12:13:25.081273: step 4780, loss = 0.99 (247.2 examples/sec; 0.518 sec/batch)
2018-03-22 12:13:30.234472: step 4790, loss = 1.05 (248.4 examples/sec; 0.515 sec/batch)
2018-03-22 12:13:35.177520: step 4800, loss = 1.10 (258.9 examples/sec; 0.494 sec/batch)
2018-03-22 12:13:40.629409: step 4810, loss = 1.20 (234.8 examples/sec; 0.545 sec/batch)
2018-03-22 12:13:45.310407: step 4820, loss = 1.09 (273.4 examples/sec; 0.468 sec/batch)
2018-03-22 12:13:50.212366: step 4830, loss = 0.88 (261.1 examples/sec; 0.490 sec/batch)
2018-03-22 12:13:55.368357: step 4840, loss = 1.24 (248.3 examples/sec; 0.516 sec/batch)
2018-03-22 12:14:00.510201: step 4850, loss = 1.04 (248.9 examples/sec; 0.514 sec/batch)
2018-03-22 12:14:05.433440: step 4860, loss = 1.15 (260.0 examples/sec; 0.492 sec/batch)
2018-03-22 12:14:10.705439: step 4870, loss = 1.06 (242.8 examples/sec; 0.527 sec/batch)
2018-03-22 12:14:15.370456: step 4880, loss = 1.07 (274.4 examples/sec; 0.467 sec/batch)
2018-03-22 12:14:21.009384: step 4890, loss = 1.17 (227.0 examples/sec; 0.564 sec/batch)
2018-03-22 12:14:26.188943: step 4900, loss = 1.10 (247.1 examples/sec; 0.518 sec/batch)
2018-03-22 12:14:31.415423: step 4910, loss = 0.94 (244.9 examples/sec; 0.523 sec/batch)
2018-03-22 12:14:36.269794: step 4920, loss = 1.08 (263.7 examples/sec; 0.485 sec/batch)
2018-03-22 12:14:41.407573: step 4930, loss = 0.85 (249.1 examples/sec; 0.514 sec/batch)
2018-03-22 12:14:46.578342: step 4940, loss = 0.93 (247.5 examples/sec; 0.517 sec/batch)
2018-03-22 12:14:51.628410: step 4950, loss = 1.04 (253.5 examples/sec; 0.505 sec/batch)
2018-03-22 12:14:56.516378: step 4960, loss = 0.97 (261.9 examples/sec; 0.489 sec/batch)
2018-03-22 12:15:01.969427: step 4970, loss = 1.00 (234.7 examples/sec; 0.545 sec/batch)
2018-03-22 12:15:07.085338: step 4980, loss = 0.94 (250.2 examples/sec; 0.512 sec/batch)
2018-03-22 12:15:11.638181: step 4990, loss = 1.22 (281.1 examples/sec; 0.455 sec/batch)
2018-03-22 12:15:17.214058: step 5000, loss = 0.87 (229.6 examples/sec; 0.558 sec/batch)
2018-03-22 12:15:21.859390: step 5010, loss = 1.02 (275.5 examples/sec; 0.465 sec/batch)
2018-03-22 12:15:26.938430: step 5020, loss = 1.18 (252.0 examples/sec; 0.508 sec/batch)
2018-03-22 12:15:32.094910: step 5030, loss = 1.01 (248.2 examples/sec; 0.516 sec/batch)
2018-03-22 12:15:36.782447: step 5040, loss = 1.00 (273.1 examples/sec; 0.469 sec/batch)
2018-03-22 12:15:42.274377: step 5050, loss = 0.97 (233.1 examples/sec; 0.549 sec/batch)
2018-03-22 12:15:47.557454: step 5060, loss = 0.98 (242.3 examples/sec; 0.528 sec/batch)
2018-03-22 12:15:52.800692: step 5070, loss = 1.13 (244.1 examples/sec; 0.524 sec/batch)
2018-03-22 12:15:57.610502: step 5080, loss = 1.04 (266.1 examples/sec; 0.481 sec/batch)
2018-03-22 12:16:02.450427: step 5090, loss = 0.95 (264.5 examples/sec; 0.484 sec/batch)
2018-03-22 12:16:07.651324: step 5100, loss = 1.07 (246.1 examples/sec; 0.520 sec/batch)
2018-03-22 12:16:12.544407: step 5110, loss = 1.01 (261.6 examples/sec; 0.489 sec/batch)
2018-03-22 12:16:17.617794: step 5120, loss = 1.02 (252.3 examples/sec; 0.507 sec/batch)
2018-03-22 12:16:22.640448: step 5130, loss = 0.90 (254.8 examples/sec; 0.502 sec/batch)
2018-03-22 12:16:27.484219: step 5140, loss = 1.12 (264.3 examples/sec; 0.484 sec/batch)
2018-03-22 12:16:32.063307: step 5150, loss = 1.06 (279.5 examples/sec; 0.458 sec/batch)
2018-03-22 12:16:37.165397: step 5160, loss = 0.81 (250.9 examples/sec; 0.510 sec/batch)
2018-03-22 12:16:41.961222: step 5170, loss = 1.10 (266.9 examples/sec; 0.480 sec/batch)
2018-03-22 12:16:47.075072: step 5180, loss = 0.98 (250.3 examples/sec; 0.511 sec/batch)
2018-03-22 12:16:52.133929: step 5190, loss = 0.98 (253.0 examples/sec; 0.506 sec/batch)
2018-03-22 12:16:57.393988: step 5200, loss = 1.02 (243.3 examples/sec; 0.526 sec/batch)
2018-03-22 12:17:02.258762: step 5210, loss = 0.97 (263.1 examples/sec; 0.486 sec/batch)
2018-03-22 12:17:07.219214: step 5220, loss = 0.90 (258.0 examples/sec; 0.496 sec/batch)
2018-03-22 12:17:11.693396: step 5230, loss = 1.09 (286.1 examples/sec; 0.447 sec/batch)
2018-03-22 12:17:16.274433: step 5240, loss = 0.78 (279.4 examples/sec; 0.458 sec/batch)
2018-03-22 12:17:21.431359: step 5250, loss = 0.93 (248.2 examples/sec; 0.516 sec/batch)
2018-03-22 12:17:26.507547: step 5260, loss = 1.02 (252.2 examples/sec; 0.508 sec/batch)
2018-03-22 12:17:31.201374: step 5270, loss = 0.83 (272.7 examples/sec; 0.469 sec/batch)
2018-03-22 12:17:36.074371: step 5280, loss = 0.90 (262.7 examples/sec; 0.487 sec/batch)
2018-03-22 12:17:41.205477: step 5290, loss = 1.18 (249.5 examples/sec; 0.513 sec/batch)
2018-03-22 12:17:46.657987: step 5300, loss = 1.13 (234.8 examples/sec; 0.545 sec/batch)
2018-03-22 12:17:51.654394: step 5310, loss = 1.09 (256.2 examples/sec; 0.500 sec/batch)
2018-03-22 12:17:56.639332: step 5320, loss = 0.91 (256.8 examples/sec; 0.498 sec/batch)
2018-03-22 12:18:01.795578: step 5330, loss = 1.00 (248.3 examples/sec; 0.516 sec/batch)
2018-03-22 12:18:06.531274: step 5340, loss = 1.01 (270.3 examples/sec; 0.474 sec/batch)
2018-03-22 12:18:11.248369: step 5350, loss = 1.07 (271.4 examples/sec; 0.472 sec/batch)
2018-03-22 12:18:16.474535: step 5360, loss = 0.95 (244.9 examples/sec; 0.523 sec/batch)
2018-03-22 12:18:21.668207: step 5370, loss = 1.04 (246.5 examples/sec; 0.519 sec/batch)
2018-03-22 12:18:26.489186: step 5380, loss = 0.89 (265.5 examples/sec; 0.482 sec/batch)
2018-03-22 12:18:31.805380: step 5390, loss = 1.07 (240.8 examples/sec; 0.532 sec/batch)
2018-03-22 12:18:37.263060: step 5400, loss = 1.04 (234.5 examples/sec; 0.546 sec/batch)
2018-03-22 12:18:42.259404: step 5410, loss = 0.84 (256.2 examples/sec; 0.500 sec/batch)
2018-03-22 12:18:47.407400: step 5420, loss = 0.89 (248.6 examples/sec; 0.515 sec/batch)
2018-03-22 12:18:52.896393: step 5430, loss = 0.91 (233.2 examples/sec; 0.549 sec/batch)
2018-03-22 12:18:57.622408: step 5440, loss = 1.05 (270.8 examples/sec; 0.473 sec/batch)
2018-03-22 12:19:02.727378: step 5450, loss = 1.04 (250.7 examples/sec; 0.510 sec/batch)
2018-03-22 12:19:07.698659: step 5460, loss = 0.99 (257.5 examples/sec; 0.497 sec/batch)
2018-03-22 12:19:12.535243: step 5470, loss = 0.96 (264.6 examples/sec; 0.484 sec/batch)
2018-03-22 12:19:17.767377: step 5480, loss = 0.81 (244.6 examples/sec; 0.523 sec/batch)
2018-03-22 12:19:22.499460: step 5490, loss = 1.01 (270.5 examples/sec; 0.473 sec/batch)
2018-03-22 12:19:27.693798: step 5500, loss = 0.98 (246.4 examples/sec; 0.519 sec/batch)
2018-03-22 12:19:33.200673: step 5510, loss = 0.93 (232.4 examples/sec; 0.551 sec/batch)
2018-03-22 12:19:38.239374: step 5520, loss = 1.03 (254.0 examples/sec; 0.504 sec/batch)
2018-03-22 12:19:43.522157: step 5530, loss = 1.09 (242.3 examples/sec; 0.528 sec/batch)
2018-03-22 12:19:49.085578: step 5540, loss = 0.95 (230.1 examples/sec; 0.556 sec/batch)
2018-03-22 12:19:53.762717: step 5550, loss = 0.96 (273.7 examples/sec; 0.468 sec/batch)
2018-03-22 12:19:58.062370: step 5560, loss = 0.89 (297.7 examples/sec; 0.430 sec/batch)
2018-03-22 12:20:02.928388: step 5570, loss = 1.09 (263.0 examples/sec; 0.487 sec/batch)
2018-03-22 12:20:07.543411: step 5580, loss = 1.07 (277.4 examples/sec; 0.462 sec/batch)
2018-03-22 12:20:12.664415: step 5590, loss = 0.99 (250.0 examples/sec; 0.512 sec/batch)
2018-03-22 12:20:17.653625: step 5600, loss = 1.04 (256.6 examples/sec; 0.499 sec/batch)
2018-03-22 12:20:22.568836: step 5610, loss = 1.17 (260.4 examples/sec; 0.492 sec/batch)
2018-03-22 12:20:27.327383: step 5620, loss = 0.94 (269.0 examples/sec; 0.476 sec/batch)
2018-03-22 12:20:32.160148: step 5630, loss = 0.84 (264.9 examples/sec; 0.483 sec/batch)
2018-03-22 12:20:36.771447: step 5640, loss = 0.90 (277.6 examples/sec; 0.461 sec/batch)
2018-03-22 12:20:42.254328: step 5650, loss = 1.12 (233.5 examples/sec; 0.548 sec/batch)
2018-03-22 12:20:47.505608: step 5660, loss = 1.00 (243.8 examples/sec; 0.525 sec/batch)
2018-03-22 12:20:52.756419: step 5670, loss = 0.92 (243.8 examples/sec; 0.525 sec/batch)
2018-03-22 12:20:57.355226: step 5680, loss = 0.89 (278.3 examples/sec; 0.460 sec/batch)
2018-03-22 12:21:02.039364: step 5690, loss = 1.00 (273.3 examples/sec; 0.468 sec/batch)
2018-03-22 12:21:07.553877: step 5700, loss = 0.97 (232.1 examples/sec; 0.551 sec/batch)
2018-03-22 12:21:12.600458: step 5710, loss = 0.80 (253.6 examples/sec; 0.505 sec/batch)
2018-03-22 12:21:17.287233: step 5720, loss = 0.89 (273.1 examples/sec; 0.469 sec/batch)
2018-03-22 12:21:22.337035: step 5730, loss = 1.04 (253.5 examples/sec; 0.505 sec/batch)
2018-03-22 12:21:27.295468: step 5740, loss = 0.77 (258.1 examples/sec; 0.496 sec/batch)
2018-03-22 12:21:32.144389: step 5750, loss = 1.06 (264.0 examples/sec; 0.485 sec/batch)
2018-03-22 12:21:37.496425: step 5760, loss = 0.99 (239.2 examples/sec; 0.535 sec/batch)
2018-03-22 12:21:43.162430: step 5770, loss = 1.18 (225.9 examples/sec; 0.567 sec/batch)
2018-03-22 12:21:48.676341: step 5780, loss = 1.29 (232.1 examples/sec; 0.551 sec/batch)
2018-03-22 12:21:53.282361: step 5790, loss = 1.04 (277.9 examples/sec; 0.461 sec/batch)
2018-03-22 12:21:58.409309: step 5800, loss = 0.91 (249.7 examples/sec; 0.513 sec/batch)
2018-03-22 12:22:03.727450: step 5810, loss = 1.03 (240.7 examples/sec; 0.532 sec/batch)
2018-03-22 12:22:09.305385: step 5820, loss = 0.97 (229.5 examples/sec; 0.558 sec/batch)
2018-03-22 12:22:14.033774: step 5830, loss = 0.98 (270.7 examples/sec; 0.473 sec/batch)
2018-03-22 12:22:19.005349: step 5840, loss = 1.07 (257.5 examples/sec; 0.497 sec/batch)
2018-03-22 12:22:24.713225: step 5850, loss = 1.10 (224.3 examples/sec; 0.571 sec/batch)
2018-03-22 12:22:30.009665: step 5860, loss = 1.06 (241.7 examples/sec; 0.530 sec/batch)
2018-03-22 12:22:34.927355: step 5870, loss = 0.88 (260.3 examples/sec; 0.492 sec/batch)
2018-03-22 12:22:40.265895: step 5880, loss = 0.88 (239.8 examples/sec; 0.534 sec/batch)
2018-03-22 12:22:44.758442: step 5890, loss = 0.97 (284.9 examples/sec; 0.449 sec/batch)
2018-03-22 12:22:50.401891: step 5900, loss = 1.06 (226.8 examples/sec; 0.564 sec/batch)
2018-03-22 12:22:55.610290: step 5910, loss = 0.73 (245.8 examples/sec; 0.521 sec/batch)
2018-03-22 12:23:01.092440: step 5920, loss = 1.00 (233.5 examples/sec; 0.548 sec/batch)
2018-03-22 12:23:06.285404: step 5930, loss = 0.89 (246.5 examples/sec; 0.519 sec/batch)
2018-03-22 12:23:11.172485: step 5940, loss = 1.01 (261.9 examples/sec; 0.489 sec/batch)
2018-03-22 12:23:16.183077: step 5950, loss = 1.06 (255.5 examples/sec; 0.501 sec/batch)
2018-03-22 12:23:21.215347: step 5960, loss = 0.95 (254.4 examples/sec; 0.503 sec/batch)
2018-03-22 12:23:26.037480: step 5970, loss = 1.04 (265.4 examples/sec; 0.482 sec/batch)
2018-03-22 12:23:31.229274: step 5980, loss = 0.93 (246.5 examples/sec; 0.519 sec/batch)
2018-03-22 12:23:36.172306: step 5990, loss = 0.99 (259.0 examples/sec; 0.494 sec/batch)
2018-03-22 12:23:41.413717: step 6000, loss = 1.03 (244.2 examples/sec; 0.524 sec/batch)
2018-03-22 12:23:46.016208: step 6010, loss = 0.95 (278.1 examples/sec; 0.460 sec/batch)
2018-03-22 12:23:50.873040: step 6020, loss = 0.93 (263.5 examples/sec; 0.486 sec/batch)
2018-03-22 12:23:55.442169: step 6030, loss = 1.08 (280.1 examples/sec; 0.457 sec/batch)
2018-03-22 12:24:00.398373: step 6040, loss = 0.88 (258.3 examples/sec; 0.496 sec/batch)
2018-03-22 12:24:05.143406: step 6050, loss = 0.96 (269.8 examples/sec; 0.475 sec/batch)
2018-03-22 12:24:09.508347: step 6060, loss = 0.89 (293.2 examples/sec; 0.436 sec/batch)
2018-03-22 12:24:14.197247: step 6070, loss = 0.95 (273.0 examples/sec; 0.469 sec/batch)
2018-03-22 12:24:19.717133: step 6080, loss = 1.12 (231.9 examples/sec; 0.552 sec/batch)
2018-03-22 12:24:25.000885: step 6090, loss = 1.27 (242.3 examples/sec; 0.528 sec/batch)
2018-03-22 12:24:30.411928: step 6100, loss = 0.96 (236.6 examples/sec; 0.541 sec/batch)
2018-03-22 12:24:35.210434: step 6110, loss = 0.98 (266.7 examples/sec; 0.480 sec/batch)
2018-03-22 12:24:40.837115: step 6120, loss = 0.98 (227.5 examples/sec; 0.563 sec/batch)
2018-03-22 12:24:45.741757: step 6130, loss = 1.04 (261.0 examples/sec; 0.490 sec/batch)
2018-03-22 12:24:50.188225: step 6140, loss = 0.84 (287.9 examples/sec; 0.445 sec/batch)
2018-03-22 12:24:55.100445: step 6150, loss = 0.93 (260.6 examples/sec; 0.491 sec/batch)
2018-03-22 12:25:00.693348: step 6160, loss = 0.94 (228.9 examples/sec; 0.559 sec/batch)
2018-03-22 12:25:05.779396: step 6170, loss = 0.99 (251.7 examples/sec; 0.509 sec/batch)
2018-03-22 12:25:11.136312: step 6180, loss = 0.84 (238.9 examples/sec; 0.536 sec/batch)
2018-03-22 12:25:15.497749: step 6190, loss = 1.01 (293.5 examples/sec; 0.436 sec/batch)
2018-03-22 12:25:21.253723: step 6200, loss = 0.95 (222.4 examples/sec; 0.576 sec/batch)
2018-03-22 12:25:25.872383: step 6210, loss = 0.86 (277.1 examples/sec; 0.462 sec/batch)
2018-03-22 12:25:31.107350: step 6220, loss = 0.95 (244.5 examples/sec; 0.523 sec/batch)
2018-03-22 12:25:35.862417: step 6230, loss = 0.95 (269.2 examples/sec; 0.476 sec/batch)
2018-03-22 12:25:41.458320: step 6240, loss = 0.92 (228.7 examples/sec; 0.560 sec/batch)
2018-03-22 12:25:46.547641: step 6250, loss = 0.96 (251.5 examples/sec; 0.509 sec/batch)
2018-03-22 12:25:51.565707: step 6260, loss = 1.03 (255.1 examples/sec; 0.502 sec/batch)
2018-03-22 12:25:56.605922: step 6270, loss = 1.01 (254.0 examples/sec; 0.504 sec/batch)
2018-03-22 12:26:02.529431: step 6280, loss = 0.85 (216.1 examples/sec; 0.592 sec/batch)
2018-03-22 12:26:07.211342: step 6290, loss = 1.11 (273.4 examples/sec; 0.468 sec/batch)
2018-03-22 12:26:12.289781: step 6300, loss = 0.91 (252.0 examples/sec; 0.508 sec/batch)
2018-03-22 12:26:17.536969: step 6310, loss = 0.93 (243.9 examples/sec; 0.525 sec/batch)
2018-03-22 12:26:22.154355: step 6320, loss = 1.12 (277.2 examples/sec; 0.462 sec/batch)
2018-03-22 12:26:27.331170: step 6330, loss = 0.91 (247.3 examples/sec; 0.518 sec/batch)
2018-03-22 12:26:32.975990: step 6340, loss = 0.91 (226.8 examples/sec; 0.564 sec/batch)
2018-03-22 12:26:37.952434: step 6350, loss = 0.89 (257.2 examples/sec; 0.498 sec/batch)
2018-03-22 12:26:42.856235: step 6360, loss = 0.95 (261.0 examples/sec; 0.490 sec/batch)
2018-03-22 12:26:47.575456: step 6370, loss = 1.01 (271.2 examples/sec; 0.472 sec/batch)
2018-03-22 12:26:52.886482: step 6380, loss = 1.17 (241.0 examples/sec; 0.531 sec/batch)
2018-03-22 12:26:57.788313: step 6390, loss = 0.93 (261.1 examples/sec; 0.490 sec/batch)
2018-03-22 12:27:02.815685: step 6400, loss = 0.94 (254.6 examples/sec; 0.503 sec/batch)
2018-03-22 12:27:07.785382: step 6410, loss = 1.07 (257.6 examples/sec; 0.497 sec/batch)
2018-03-22 12:27:13.696911: step 6420, loss = 1.06 (216.5 examples/sec; 0.591 sec/batch)
2018-03-22 12:27:18.360426: step 6430, loss = 0.68 (274.5 examples/sec; 0.466 sec/batch)
2018-03-22 12:27:23.560487: step 6440, loss = 1.00 (246.1 examples/sec; 0.520 sec/batch)
2018-03-22 12:27:28.560780: step 6450, loss = 0.83 (256.0 examples/sec; 0.500 sec/batch)
2018-03-22 12:27:33.385640: step 6460, loss = 1.01 (265.3 examples/sec; 0.482 sec/batch)
2018-03-22 12:27:37.659364: step 6470, loss = 0.93 (299.5 examples/sec; 0.427 sec/batch)
2018-03-22 12:27:42.574403: step 6480, loss = 1.17 (260.4 examples/sec; 0.492 sec/batch)
2018-03-22 12:27:47.629429: step 6490, loss = 0.87 (253.2 examples/sec; 0.506 sec/batch)
2018-03-22 12:27:52.640252: step 6500, loss = 0.93 (255.4 examples/sec; 0.501 sec/batch)
2018-03-22 12:27:57.459203: step 6510, loss = 1.01 (265.6 examples/sec; 0.482 sec/batch)
2018-03-22 12:28:02.023772: step 6520, loss = 0.84 (280.4 examples/sec; 0.456 sec/batch)
2018-03-22 12:28:06.541206: step 6530, loss = 0.88 (283.3 examples/sec; 0.452 sec/batch)
2018-03-22 12:28:11.104646: step 6540, loss = 0.86 (280.5 examples/sec; 0.456 sec/batch)
2018-03-22 12:28:15.671516: step 6550, loss = 1.02 (280.3 examples/sec; 0.457 sec/batch)
2018-03-22 12:28:20.716283: step 6560, loss = 0.98 (253.7 examples/sec; 0.504 sec/batch)
2018-03-22 12:28:25.878454: step 6570, loss = 0.87 (248.0 examples/sec; 0.516 sec/batch)
2018-03-22 12:28:30.966340: step 6580, loss = 0.90 (251.6 examples/sec; 0.509 sec/batch)
2018-03-22 12:28:35.676319: step 6590, loss = 0.96 (271.8 examples/sec; 0.471 sec/batch)
2018-03-22 12:28:40.761219: step 6600, loss = 1.02 (251.7 examples/sec; 0.508 sec/batch)
2018-03-22 12:28:45.657393: step 6610, loss = 1.00 (261.4 examples/sec; 0.490 sec/batch)
2018-03-22 12:28:51.028476: step 6620, loss = 0.87 (238.3 examples/sec; 0.537 sec/batch)
2018-03-22 12:28:55.529423: step 6630, loss = 1.05 (284.4 examples/sec; 0.450 sec/batch)
2018-03-22 12:29:00.996402: step 6640, loss = 0.92 (234.1 examples/sec; 0.547 sec/batch)
2018-03-22 12:29:05.860329: step 6650, loss = 0.68 (263.2 examples/sec; 0.486 sec/batch)
2018-03-22 12:29:11.191618: step 6660, loss = 0.94 (240.1 examples/sec; 0.533 sec/batch)
2018-03-22 12:29:16.028180: step 6670, loss = 0.95 (264.7 examples/sec; 0.484 sec/batch)
2018-03-22 12:29:21.640369: step 6680, loss = 0.88 (228.1 examples/sec; 0.561 sec/batch)
2018-03-22 12:29:26.486390: step 6690, loss = 0.83 (264.1 examples/sec; 0.485 sec/batch)
2018-03-22 12:29:31.803080: step 6700, loss = 0.88 (240.8 examples/sec; 0.532 sec/batch)
2018-03-22 12:29:36.711614: step 6710, loss = 0.90 (260.8 examples/sec; 0.491 sec/batch)
2018-03-22 12:29:41.492435: step 6720, loss = 1.11 (267.7 examples/sec; 0.478 sec/batch)
2018-03-22 12:29:46.457430: step 6730, loss = 0.98 (257.8 examples/sec; 0.496 sec/batch)
2018-03-22 12:29:51.608414: step 6740, loss = 0.98 (248.5 examples/sec; 0.515 sec/batch)
2018-03-22 12:29:56.934395: step 6750, loss = 1.02 (240.3 examples/sec; 0.533 sec/batch)
2018-03-22 12:30:01.783222: step 6760, loss = 0.90 (264.0 examples/sec; 0.485 sec/batch)
2018-03-22 12:30:06.748067: step 6770, loss = 0.78 (257.8 examples/sec; 0.496 sec/batch)
2018-03-22 12:30:11.797362: step 6780, loss = 0.98 (253.5 examples/sec; 0.505 sec/batch)
2018-03-22 12:30:17.035388: step 6790, loss = 0.92 (244.4 examples/sec; 0.524 sec/batch)
2018-03-22 12:30:22.565155: step 6800, loss = 0.98 (231.5 examples/sec; 0.553 sec/batch)
2018-03-22 12:30:27.588440: step 6810, loss = 0.82 (254.8 examples/sec; 0.502 sec/batch)
2018-03-22 12:30:32.541757: step 6820, loss = 0.85 (258.4 examples/sec; 0.495 sec/batch)
2018-03-22 12:30:37.766766: step 6830, loss = 1.09 (245.0 examples/sec; 0.523 sec/batch)
2018-03-22 12:30:43.362996: step 6840, loss = 1.18 (228.7 examples/sec; 0.560 sec/batch)
2018-03-22 12:30:47.994197: step 6850, loss = 0.91 (276.4 examples/sec; 0.463 sec/batch)
2018-03-22 12:30:53.104376: step 6860, loss = 1.16 (250.5 examples/sec; 0.511 sec/batch)
2018-03-22 12:30:58.047390: step 6870, loss = 1.05 (259.0 examples/sec; 0.494 sec/batch)
2018-03-22 12:31:03.145431: step 6880, loss = 0.82 (251.1 examples/sec; 0.510 sec/batch)
2018-03-22 12:31:08.175887: step 6890, loss = 0.80 (254.5 examples/sec; 0.503 sec/batch)
2018-03-22 12:31:13.569233: step 6900, loss = 0.86 (237.3 examples/sec; 0.539 sec/batch)
2018-03-22 12:31:18.542444: step 6910, loss = 0.97 (257.4 examples/sec; 0.497 sec/batch)
2018-03-22 12:31:23.711232: step 6920, loss = 0.92 (247.6 examples/sec; 0.517 sec/batch)
2018-03-22 12:31:28.117363: step 6930, loss = 0.80 (290.5 examples/sec; 0.441 sec/batch)
2018-03-22 12:31:32.826514: step 6940, loss = 0.77 (271.8 examples/sec; 0.471 sec/batch)
2018-03-22 12:31:37.629372: step 6950, loss = 1.02 (266.5 examples/sec; 0.480 sec/batch)
2018-03-22 12:31:42.565432: step 6960, loss = 0.90 (259.3 examples/sec; 0.494 sec/batch)
2018-03-22 12:31:46.688255: step 6970, loss = 0.82 (310.5 examples/sec; 0.412 sec/batch)
2018-03-22 12:31:51.676337: step 6980, loss = 0.92 (256.6 examples/sec; 0.499 sec/batch)
2018-03-22 12:31:56.573194: step 6990, loss = 1.00 (261.4 examples/sec; 0.490 sec/batch)
2018-03-22 12:32:02.276207: step 7000, loss = 0.93 (224.4 examples/sec; 0.570 sec/batch)
2018-03-22 12:32:07.007378: step 7010, loss = 0.97 (270.5 examples/sec; 0.473 sec/batch)
2018-03-22 12:32:12.270396: step 7020, loss = 0.88 (243.2 examples/sec; 0.526 sec/batch)
2018-03-22 12:32:17.731220: step 7030, loss = 0.96 (234.4 examples/sec; 0.546 sec/batch)
2018-03-22 12:32:23.129411: step 7040, loss = 0.83 (237.1 examples/sec; 0.540 sec/batch)
2018-03-22 12:32:28.404559: step 7050, loss = 0.91 (242.6 examples/sec; 0.528 sec/batch)
2018-03-22 12:32:32.769917: step 7060, loss = 1.21 (293.2 examples/sec; 0.437 sec/batch)
2018-03-22 12:32:38.307246: step 7070, loss = 1.04 (231.2 examples/sec; 0.554 sec/batch)
2018-03-22 12:32:43.693938: step 7080, loss = 1.12 (237.6 examples/sec; 0.539 sec/batch)
2018-03-22 12:32:49.008419: step 7090, loss = 1.02 (240.9 examples/sec; 0.531 sec/batch)
2018-03-22 12:32:54.365579: step 7100, loss = 0.98 (238.9 examples/sec; 0.536 sec/batch)
2018-03-22 12:32:59.313094: step 7110, loss = 0.97 (258.7 examples/sec; 0.495 sec/batch)
2018-03-22 12:33:04.380475: step 7120, loss = 0.95 (252.6 examples/sec; 0.507 sec/batch)
2018-03-22 12:33:09.187770: step 7130, loss = 0.88 (266.3 examples/sec; 0.481 sec/batch)
2018-03-22 12:33:14.200787: step 7140, loss = 0.81 (255.3 examples/sec; 0.501 sec/batch)
2018-03-22 12:33:18.924446: step 7150, loss = 1.00 (271.0 examples/sec; 0.472 sec/batch)
2018-03-22 12:33:23.790472: step 7160, loss = 1.01 (263.0 examples/sec; 0.487 sec/batch)
2018-03-22 12:33:29.267453: step 7170, loss = 0.88 (233.7 examples/sec; 0.548 sec/batch)
2018-03-22 12:33:34.185387: step 7180, loss = 1.00 (260.3 examples/sec; 0.492 sec/batch)
2018-03-22 12:33:39.555243: step 7190, loss = 0.90 (238.4 examples/sec; 0.537 sec/batch)
2018-03-22 12:33:44.487665: step 7200, loss = 0.89 (259.5 examples/sec; 0.493 sec/batch)
2018-03-22 12:33:50.285834: step 7210, loss = 0.93 (220.8 examples/sec; 0.580 sec/batch)
2018-03-22 12:33:55.332427: step 7220, loss = 0.77 (253.6 examples/sec; 0.505 sec/batch)
2018-03-22 12:34:00.785401: step 7230, loss = 0.97 (234.7 examples/sec; 0.545 sec/batch)
2018-03-22 12:34:05.716058: step 7240, loss = 0.91 (259.6 examples/sec; 0.493 sec/batch)
2018-03-22 12:34:11.138334: step 7250, loss = 0.77 (236.1 examples/sec; 0.542 sec/batch)
2018-03-22 12:34:16.239020: step 7260, loss = 0.75 (250.9 examples/sec; 0.510 sec/batch)
2018-03-22 12:34:21.246904: step 7270, loss = 1.01 (255.6 examples/sec; 0.501 sec/batch)
2018-03-22 12:34:26.288526: step 7280, loss = 1.12 (253.9 examples/sec; 0.504 sec/batch)
2018-03-22 12:34:31.078557: step 7290, loss = 1.02 (267.2 examples/sec; 0.479 sec/batch)
2018-03-22 12:34:36.287534: step 7300, loss = 0.78 (245.7 examples/sec; 0.521 sec/batch)
2018-03-22 12:34:41.199069: step 7310, loss = 0.89 (260.6 examples/sec; 0.491 sec/batch)
2018-03-22 12:34:46.289493: step 7320, loss = 0.78 (251.5 examples/sec; 0.509 sec/batch)
2018-03-22 12:34:51.269379: step 7330, loss = 0.94 (257.0 examples/sec; 0.498 sec/batch)
2018-03-22 12:34:56.294601: step 7340, loss = 0.97 (254.7 examples/sec; 0.503 sec/batch)
2018-03-22 12:35:01.824373: step 7350, loss = 1.00 (231.5 examples/sec; 0.553 sec/batch)
2018-03-22 12:35:07.020584: step 7360, loss = 0.83 (246.3 examples/sec; 0.520 sec/batch)
2018-03-22 12:35:11.804381: step 7370, loss = 1.05 (267.6 examples/sec; 0.478 sec/batch)
2018-03-22 12:35:16.599390: step 7380, loss = 0.96 (266.9 examples/sec; 0.480 sec/batch)
2018-03-22 12:35:21.334411: step 7390, loss = 1.05 (270.3 examples/sec; 0.474 sec/batch)
2018-03-22 12:35:26.615024: step 7400, loss = 1.05 (242.4 examples/sec; 0.528 sec/batch)
2018-03-22 12:35:31.895935: step 7410, loss = 1.01 (242.4 examples/sec; 0.528 sec/batch)
2018-03-22 12:35:37.126357: step 7420, loss = 1.15 (244.7 examples/sec; 0.523 sec/batch)
2018-03-22 12:35:42.187346: step 7430, loss = 0.78 (252.9 examples/sec; 0.506 sec/batch)
2018-03-22 12:35:46.864277: step 7440, loss = 0.79 (273.7 examples/sec; 0.468 sec/batch)
2018-03-22 12:35:51.769414: step 7450, loss = 0.83 (261.0 examples/sec; 0.491 sec/batch)
2018-03-22 12:35:56.675119: step 7460, loss = 0.85 (260.9 examples/sec; 0.491 sec/batch)
2018-03-22 12:36:01.483466: step 7470, loss = 0.74 (266.2 examples/sec; 0.481 sec/batch)
2018-03-22 12:36:06.871464: step 7480, loss = 0.87 (237.6 examples/sec; 0.539 sec/batch)
2018-03-22 12:36:12.162495: step 7490, loss = 0.79 (241.9 examples/sec; 0.529 sec/batch)
2018-03-22 12:36:17.590940: step 7500, loss = 0.84 (235.8 examples/sec; 0.543 sec/batch)
2018-03-22 12:36:23.028346: step 7510, loss = 0.91 (235.4 examples/sec; 0.544 sec/batch)
2018-03-22 12:36:27.680396: step 7520, loss = 0.96 (275.1 examples/sec; 0.465 sec/batch)
2018-03-22 12:36:32.834266: step 7530, loss = 1.02 (248.4 examples/sec; 0.515 sec/batch)
2018-03-22 12:36:37.922397: step 7540, loss = 0.79 (251.6 examples/sec; 0.509 sec/batch)
2018-03-22 12:36:43.360875: step 7550, loss = 0.93 (235.4 examples/sec; 0.544 sec/batch)
2018-03-22 12:36:47.527220: step 7560, loss = 0.90 (307.2 examples/sec; 0.417 sec/batch)
2018-03-22 12:36:52.591353: step 7570, loss = 0.89 (252.8 examples/sec; 0.506 sec/batch)
2018-03-22 12:36:57.616191: step 7580, loss = 1.05 (254.7 examples/sec; 0.502 sec/batch)
2018-03-22 12:37:02.539847: step 7590, loss = 0.95 (260.0 examples/sec; 0.492 sec/batch)
2018-03-22 12:37:07.897372: step 7600, loss = 0.92 (238.9 examples/sec; 0.536 sec/batch)
2018-03-22 12:37:13.291336: step 7610, loss = 0.93 (237.3 examples/sec; 0.539 sec/batch)
2018-03-22 12:37:18.545360: step 7620, loss = 0.93 (243.6 examples/sec; 0.525 sec/batch)
2018-03-22 12:37:24.236399: step 7630, loss = 1.07 (224.9 examples/sec; 0.569 sec/batch)
2018-03-22 12:37:28.832573: step 7640, loss = 1.02 (278.5 examples/sec; 0.460 sec/batch)
2018-03-22 12:37:33.950427: step 7650, loss = 0.83 (250.1 examples/sec; 0.512 sec/batch)
2018-03-22 12:37:38.912416: step 7660, loss = 1.02 (258.0 examples/sec; 0.496 sec/batch)
2018-03-22 12:37:43.996438: step 7670, loss = 0.94 (251.8 examples/sec; 0.508 sec/batch)
2018-03-22 12:37:49.127090: step 7680, loss = 0.98 (249.5 examples/sec; 0.513 sec/batch)
2018-03-22 12:37:53.976266: step 7690, loss = 0.91 (264.0 examples/sec; 0.485 sec/batch)
2018-03-22 12:37:59.495418: step 7700, loss = 0.74 (231.9 examples/sec; 0.552 sec/batch)
2018-03-22 12:38:04.643631: step 7710, loss = 0.97 (248.6 examples/sec; 0.515 sec/batch)
2018-03-22 12:38:09.857986: step 7720, loss = 0.94 (245.5 examples/sec; 0.521 sec/batch)
2018-03-22 12:38:14.398384: step 7730, loss = 0.81 (281.9 examples/sec; 0.454 sec/batch)
2018-03-22 12:38:19.833348: step 7740, loss = 0.80 (235.5 examples/sec; 0.543 sec/batch)
2018-03-22 12:38:25.169393: step 7750, loss = 0.91 (239.9 examples/sec; 0.534 sec/batch)
2018-03-22 12:38:30.467691: step 7760, loss = 0.73 (241.6 examples/sec; 0.530 sec/batch)
2018-03-22 12:38:35.645664: step 7770, loss = 0.78 (247.2 examples/sec; 0.518 sec/batch)
2018-03-22 12:38:41.493398: step 7780, loss = 0.95 (218.9 examples/sec; 0.585 sec/batch)
2018-03-22 12:38:46.606483: step 7790, loss = 1.05 (250.3 examples/sec; 0.511 sec/batch)
2018-03-22 12:38:51.992505: step 7800, loss = 0.97 (237.7 examples/sec; 0.539 sec/batch)
2018-03-22 12:38:56.457367: step 7810, loss = 0.87 (286.7 examples/sec; 0.446 sec/batch)
2018-03-22 12:39:01.737780: step 7820, loss = 0.81 (242.4 examples/sec; 0.528 sec/batch)
2018-03-22 12:39:06.432404: step 7830, loss = 0.98 (272.7 examples/sec; 0.469 sec/batch)
2018-03-22 12:39:11.239458: step 7840, loss = 1.00 (266.3 examples/sec; 0.481 sec/batch)
2018-03-22 12:39:15.821097: step 7850, loss = 0.84 (279.4 examples/sec; 0.458 sec/batch)
2018-03-22 12:39:21.141252: step 7860, loss = 1.03 (240.6 examples/sec; 0.532 sec/batch)
2018-03-22 12:39:26.315835: step 7870, loss = 0.96 (247.4 examples/sec; 0.517 sec/batch)
2018-03-22 12:39:31.083368: step 7880, loss = 1.01 (268.5 examples/sec; 0.477 sec/batch)
2018-03-22 12:39:35.632364: step 7890, loss = 1.07 (281.4 examples/sec; 0.455 sec/batch)
2018-03-22 12:39:40.511736: step 7900, loss = 0.70 (262.3 examples/sec; 0.488 sec/batch)
2018-03-22 12:39:45.729443: step 7910, loss = 0.79 (245.3 examples/sec; 0.522 sec/batch)
2018-03-22 12:39:50.930420: step 7920, loss = 0.94 (246.1 examples/sec; 0.520 sec/batch)
2018-03-22 12:39:55.821372: step 7930, loss = 0.94 (261.7 examples/sec; 0.489 sec/batch)
2018-03-22 12:40:00.893537: step 7940, loss = 1.00 (252.4 examples/sec; 0.507 sec/batch)
2018-03-22 12:40:05.709485: step 7950, loss = 1.03 (265.8 examples/sec; 0.482 sec/batch)
2018-03-22 12:40:11.078209: step 7960, loss = 0.81 (238.4 examples/sec; 0.537 sec/batch)
2018-03-22 12:40:16.351411: step 7970, loss = 1.02 (242.7 examples/sec; 0.527 sec/batch)
2018-03-22 12:40:21.242519: step 7980, loss = 0.83 (261.7 examples/sec; 0.489 sec/batch)
2018-03-22 12:40:26.294460: step 7990, loss = 1.02 (253.4 examples/sec; 0.505 sec/batch)
2018-03-22 12:40:31.652118: step 8000, loss = 0.91 (238.9 examples/sec; 0.536 sec/batch)
2018-03-22 12:40:36.786489: step 8010, loss = 1.04 (249.3 examples/sec; 0.513 sec/batch)
2018-03-22 12:40:41.659162: step 8020, loss = 0.82 (262.7 examples/sec; 0.487 sec/batch)
2018-03-22 12:40:46.415412: step 8030, loss = 0.81 (269.1 examples/sec; 0.476 sec/batch)
2018-03-22 12:40:51.492371: step 8040, loss = 0.93 (252.1 examples/sec; 0.508 sec/batch)
2018-03-22 12:40:56.599393: step 8050, loss = 0.94 (250.6 examples/sec; 0.511 sec/batch)
2018-03-22 12:41:01.759437: step 8060, loss = 0.90 (248.1 examples/sec; 0.516 sec/batch)
2018-03-22 12:41:06.178258: step 8070, loss = 0.89 (289.7 examples/sec; 0.442 sec/batch)
2018-03-22 12:41:11.355392: step 8080, loss = 0.99 (247.2 examples/sec; 0.518 sec/batch)
2018-03-22 12:41:16.563119: step 8090, loss = 0.97 (245.8 examples/sec; 0.521 sec/batch)
2018-03-22 12:41:22.312962: step 8100, loss = 0.98 (222.6 examples/sec; 0.575 sec/batch)
2018-03-22 12:41:27.535922: step 8110, loss = 0.89 (245.1 examples/sec; 0.522 sec/batch)
2018-03-22 12:41:32.572405: step 8120, loss = 0.76 (254.1 examples/sec; 0.504 sec/batch)
2018-03-22 12:41:37.631335: step 8130, loss = 1.00 (253.0 examples/sec; 0.506 sec/batch)
2018-03-22 12:41:42.818367: step 8140, loss = 1.03 (246.8 examples/sec; 0.519 sec/batch)
2018-03-22 12:41:47.907440: step 8150, loss = 0.87 (251.5 examples/sec; 0.509 sec/batch)
2018-03-22 12:41:52.891359: step 8160, loss = 1.07 (256.8 examples/sec; 0.498 sec/batch)
2018-03-22 12:41:57.505380: step 8170, loss = 0.86 (277.4 examples/sec; 0.461 sec/batch)
2018-03-22 12:42:02.638380: step 8180, loss = 0.88 (249.4 examples/sec; 0.513 sec/batch)
2018-03-22 12:42:07.594209: step 8190, loss = 0.89 (258.3 examples/sec; 0.496 sec/batch)
2018-03-22 12:42:12.734327: step 8200, loss = 0.82 (249.0 examples/sec; 0.514 sec/batch)
2018-03-22 12:42:17.477669: step 8210, loss = 0.98 (269.9 examples/sec; 0.474 sec/batch)
2018-03-22 12:42:22.840357: step 8220, loss = 0.93 (238.7 examples/sec; 0.536 sec/batch)
2018-03-22 12:42:27.884456: step 8230, loss = 0.78 (253.8 examples/sec; 0.504 sec/batch)
2018-03-22 12:42:32.909375: step 8240, loss = 0.86 (254.7 examples/sec; 0.502 sec/batch)
2018-03-22 12:42:37.674437: step 8250, loss = 0.78 (268.6 examples/sec; 0.477 sec/batch)
2018-03-22 12:42:42.924647: step 8260, loss = 0.96 (243.8 examples/sec; 0.525 sec/batch)
2018-03-22 12:42:47.814028: step 8270, loss = 1.09 (261.8 examples/sec; 0.489 sec/batch)
2018-03-22 12:42:53.356714: step 8280, loss = 0.93 (230.9 examples/sec; 0.554 sec/batch)
2018-03-22 12:42:58.187390: step 8290, loss = 0.87 (265.0 examples/sec; 0.483 sec/batch)
2018-03-22 12:43:03.856263: step 8300, loss = 0.90 (225.8 examples/sec; 0.567 sec/batch)
2018-03-22 12:43:08.679366: step 8310, loss = 0.84 (265.4 examples/sec; 0.482 sec/batch)
2018-03-22 12:43:14.052216: step 8320, loss = 0.92 (238.2 examples/sec; 0.537 sec/batch)
2018-03-22 12:43:18.285479: step 8330, loss = 0.98 (302.4 examples/sec; 0.423 sec/batch)
2018-03-22 12:43:23.603401: step 8340, loss = 0.72 (240.7 examples/sec; 0.532 sec/batch)
2018-03-22 12:43:29.164845: step 8350, loss = 0.81 (230.2 examples/sec; 0.556 sec/batch)
2018-03-22 12:43:34.712386: step 8360, loss = 1.07 (230.7 examples/sec; 0.555 sec/batch)
2018-03-22 12:43:39.620499: step 8370, loss = 0.88 (260.8 examples/sec; 0.491 sec/batch)
2018-03-22 12:43:44.284090: step 8380, loss = 0.98 (274.5 examples/sec; 0.466 sec/batch)
2018-03-22 12:43:49.141188: step 8390, loss = 0.86 (263.5 examples/sec; 0.486 sec/batch)
2018-03-22 12:43:54.554103: step 8400, loss = 0.96 (236.5 examples/sec; 0.541 sec/batch)
2018-03-22 12:43:59.668417: step 8410, loss = 0.77 (250.3 examples/sec; 0.511 sec/batch)
2018-03-22 12:44:04.325352: step 8420, loss = 0.89 (274.9 examples/sec; 0.466 sec/batch)
2018-03-22 12:44:09.383095: step 8430, loss = 0.76 (253.1 examples/sec; 0.506 sec/batch)
2018-03-22 12:44:14.996617: step 8440, loss = 0.83 (228.0 examples/sec; 0.561 sec/batch)
2018-03-22 12:44:19.825038: step 8450, loss = 1.15 (265.1 examples/sec; 0.483 sec/batch)
2018-03-22 12:44:24.933770: step 8460, loss = 0.84 (250.6 examples/sec; 0.511 sec/batch)
2018-03-22 12:44:30.323357: step 8470, loss = 1.12 (237.5 examples/sec; 0.539 sec/batch)
2018-03-22 12:44:35.610390: step 8480, loss = 0.85 (242.1 examples/sec; 0.529 sec/batch)
2018-03-22 12:44:40.287078: step 8490, loss = 0.98 (273.7 examples/sec; 0.468 sec/batch)
2018-03-22 12:44:45.156843: step 8500, loss = 0.94 (262.8 examples/sec; 0.487 sec/batch)
2018-03-22 12:44:50.788373: step 8510, loss = 0.94 (227.3 examples/sec; 0.563 sec/batch)
2018-03-22 12:44:56.027391: step 8520, loss = 0.94 (244.3 examples/sec; 0.524 sec/batch)
2018-03-22 12:45:00.862747: step 8530, loss = 0.90 (264.7 examples/sec; 0.484 sec/batch)
2018-03-22 12:45:05.253413: step 8540, loss = 0.89 (291.5 examples/sec; 0.439 sec/batch)
2018-03-22 12:45:09.941923: step 8550, loss = 1.00 (273.0 examples/sec; 0.469 sec/batch)
2018-03-22 12:45:14.825409: step 8560, loss = 0.91 (262.1 examples/sec; 0.488 sec/batch)
2018-03-22 12:45:19.787322: step 8570, loss = 0.95 (258.0 examples/sec; 0.496 sec/batch)
2018-03-22 12:45:24.662481: step 8580, loss = 0.70 (262.6 examples/sec; 0.488 sec/batch)
2018-03-22 12:45:30.131417: step 8590, loss = 1.01 (234.0 examples/sec; 0.547 sec/batch)
2018-03-22 12:45:35.081485: step 8600, loss = 1.03 (258.6 examples/sec; 0.495 sec/batch)
2018-03-22 12:45:40.477360: step 8610, loss = 0.94 (237.2 examples/sec; 0.540 sec/batch)
2018-03-22 12:45:45.699282: step 8620, loss = 0.89 (245.1 examples/sec; 0.522 sec/batch)
2018-03-22 12:45:50.479028: step 8630, loss = 0.99 (267.8 examples/sec; 0.478 sec/batch)
2018-03-22 12:45:55.285243: step 8640, loss = 0.78 (266.3 examples/sec; 0.481 sec/batch)
2018-03-22 12:46:00.796614: step 8650, loss = 0.91 (232.2 examples/sec; 0.551 sec/batch)
2018-03-22 12:46:06.121445: step 8660, loss = 0.90 (240.4 examples/sec; 0.532 sec/batch)
2018-03-22 12:46:11.114642: step 8670, loss = 0.81 (256.4 examples/sec; 0.499 sec/batch)
2018-03-22 12:46:16.015043: step 8680, loss = 0.83 (261.2 examples/sec; 0.490 sec/batch)
2018-03-22 12:46:21.234579: step 8690, loss = 0.95 (245.2 examples/sec; 0.522 sec/batch)
2018-03-22 12:46:26.381230: step 8700, loss = 0.82 (248.7 examples/sec; 0.515 sec/batch)
2018-03-22 12:46:31.265772: step 8710, loss = 0.85 (262.1 examples/sec; 0.488 sec/batch)
2018-03-22 12:46:36.635414: step 8720, loss = 1.00 (238.4 examples/sec; 0.537 sec/batch)
2018-03-22 12:46:41.957406: step 8730, loss = 1.23 (240.5 examples/sec; 0.532 sec/batch)
2018-03-22 12:46:46.671451: step 8740, loss = 0.83 (271.5 examples/sec; 0.471 sec/batch)
2018-03-22 12:46:52.133081: step 8750, loss = 0.76 (234.4 examples/sec; 0.546 sec/batch)
2018-03-22 12:46:56.941380: step 8760, loss = 0.90 (266.2 examples/sec; 0.481 sec/batch)
2018-03-22 12:47:01.209068: step 8770, loss = 1.07 (299.9 examples/sec; 0.427 sec/batch)
2018-03-22 12:47:05.333421: step 8780, loss = 0.86 (310.4 examples/sec; 0.412 sec/batch)
2018-03-22 12:47:11.069546: step 8790, loss = 0.83 (223.1 examples/sec; 0.574 sec/batch)
2018-03-22 12:47:16.432056: step 8800, loss = 0.78 (238.7 examples/sec; 0.536 sec/batch)
2018-03-22 12:47:21.468378: step 8810, loss = 0.90 (254.2 examples/sec; 0.504 sec/batch)
2018-03-22 12:47:26.401006: step 8820, loss = 0.91 (259.5 examples/sec; 0.493 sec/batch)
2018-03-22 12:47:31.575202: step 8830, loss = 0.82 (247.4 examples/sec; 0.517 sec/batch)
2018-03-22 12:47:36.232479: step 8840, loss = 0.85 (274.8 examples/sec; 0.466 sec/batch)
2018-03-22 12:47:41.374317: step 8850, loss = 0.83 (248.9 examples/sec; 0.514 sec/batch)
2018-03-22 12:47:46.016383: step 8860, loss = 0.73 (275.7 examples/sec; 0.464 sec/batch)
2018-03-22 12:47:51.071423: step 8870, loss = 0.96 (253.2 examples/sec; 0.506 sec/batch)
2018-03-22 12:47:56.436879: step 8880, loss = 0.86 (238.6 examples/sec; 0.537 sec/batch)
2018-03-22 12:48:01.300519: step 8890, loss = 1.00 (263.2 examples/sec; 0.486 sec/batch)
2018-03-22 12:48:06.287346: step 8900, loss = 0.70 (256.7 examples/sec; 0.499 sec/batch)
2018-03-22 12:48:11.434809: step 8910, loss = 0.81 (248.7 examples/sec; 0.515 sec/batch)
2018-03-22 12:48:16.256422: step 8920, loss = 0.90 (265.5 examples/sec; 0.482 sec/batch)
2018-03-22 12:48:21.261366: step 8930, loss = 0.95 (255.7 examples/sec; 0.500 sec/batch)
2018-03-22 12:48:26.260450: step 8940, loss = 0.81 (256.0 examples/sec; 0.500 sec/batch)
2018-03-22 12:48:31.102873: step 8950, loss = 0.82 (264.3 examples/sec; 0.484 sec/batch)
2018-03-22 12:48:36.235419: step 8960, loss = 1.01 (249.4 examples/sec; 0.513 sec/batch)
2018-03-22 12:48:41.275401: step 8970, loss = 1.13 (254.0 examples/sec; 0.504 sec/batch)
2018-03-22 12:48:46.066391: step 8980, loss = 0.71 (267.2 examples/sec; 0.479 sec/batch)
2018-03-22 12:48:51.111478: step 8990, loss = 0.89 (253.7 examples/sec; 0.505 sec/batch)
2018-03-22 12:48:56.596084: step 9000, loss = 0.97 (233.4 examples/sec; 0.548 sec/batch)
2018-03-22 12:49:01.575961: step 9010, loss = 1.18 (257.0 examples/sec; 0.498 sec/batch)
2018-03-22 12:49:06.337255: step 9020, loss = 0.74 (268.8 examples/sec; 0.476 sec/batch)
2018-03-22 12:49:11.687073: step 9030, loss = 0.63 (239.3 examples/sec; 0.535 sec/batch)
2018-03-22 12:49:16.591334: step 9040, loss = 0.99 (261.0 examples/sec; 0.490 sec/batch)
2018-03-22 12:49:21.707444: step 9050, loss = 0.85 (250.2 examples/sec; 0.512 sec/batch)
2018-03-22 12:49:26.735428: step 9060, loss = 0.89 (254.6 examples/sec; 0.503 sec/batch)
2018-03-22 12:49:32.348785: step 9070, loss = 0.97 (228.0 examples/sec; 0.561 sec/batch)
2018-03-22 12:49:36.900519: step 9080, loss = 1.01 (281.2 examples/sec; 0.455 sec/batch)
2018-03-22 12:49:42.282390: step 9090, loss = 0.97 (237.8 examples/sec; 0.538 sec/batch)
2018-03-22 12:49:47.906489: step 9100, loss = 0.95 (227.6 examples/sec; 0.562 sec/batch)
2018-03-22 12:49:52.920379: step 9110, loss = 0.75 (255.3 examples/sec; 0.501 sec/batch)
2018-03-22 12:49:57.457773: step 9120, loss = 1.20 (282.1 examples/sec; 0.454 sec/batch)
2018-03-22 12:50:02.100721: step 9130, loss = 0.90 (275.7 examples/sec; 0.464 sec/batch)
2018-03-22 12:50:06.841393: step 9140, loss = 0.75 (270.0 examples/sec; 0.474 sec/batch)
2018-03-22 12:50:11.945636: step 9150, loss = 0.79 (250.8 examples/sec; 0.510 sec/batch)
2018-03-22 12:50:16.521831: step 9160, loss = 0.81 (279.7 examples/sec; 0.458 sec/batch)
2018-03-22 12:50:21.782985: step 9170, loss = 0.85 (243.3 examples/sec; 0.526 sec/batch)
2018-03-22 12:50:26.691392: step 9180, loss = 0.82 (260.8 examples/sec; 0.491 sec/batch)
2018-03-22 12:50:32.332865: step 9190, loss = 0.93 (226.9 examples/sec; 0.564 sec/batch)
2018-03-22 12:50:37.282313: step 9200, loss = 1.03 (258.6 examples/sec; 0.495 sec/batch)
2018-03-22 12:50:42.414990: step 9210, loss = 0.84 (249.4 examples/sec; 0.513 sec/batch)
2018-03-22 12:50:47.398801: step 9220, loss = 1.06 (256.8 examples/sec; 0.498 sec/batch)
2018-03-22 12:50:51.948617: step 9230, loss = 0.80 (281.3 examples/sec; 0.455 sec/batch)
2018-03-22 12:50:56.059372: step 9240, loss = 0.99 (311.4 examples/sec; 0.411 sec/batch)
2018-03-22 12:51:01.364400: step 9250, loss = 0.69 (241.3 examples/sec; 0.531 sec/batch)
2018-03-22 12:51:06.698448: step 9260, loss = 0.76 (240.0 examples/sec; 0.533 sec/batch)
2018-03-22 12:51:11.869385: step 9270, loss = 0.99 (247.5 examples/sec; 0.517 sec/batch)
2018-03-22 12:51:17.042817: step 9280, loss = 0.91 (247.4 examples/sec; 0.517 sec/batch)
2018-03-22 12:51:22.092087: step 9290, loss = 0.92 (253.5 examples/sec; 0.505 sec/batch)
2018-03-22 12:51:27.346107: step 9300, loss = 0.79 (243.6 examples/sec; 0.525 sec/batch)
2018-03-22 12:51:32.496424: step 9310, loss = 0.80 (248.5 examples/sec; 0.515 sec/batch)
2018-03-22 12:51:37.737399: step 9320, loss = 0.80 (244.2 examples/sec; 0.524 sec/batch)
2018-03-22 12:51:43.405433: step 9330, loss = 0.92 (225.8 examples/sec; 0.567 sec/batch)
2018-03-22 12:51:48.157363: step 9340, loss = 0.96 (269.4 examples/sec; 0.475 sec/batch)
2018-03-22 12:51:53.284386: step 9350, loss = 0.68 (249.7 examples/sec; 0.513 sec/batch)
2018-03-22 12:51:58.330412: step 9360, loss = 0.81 (253.7 examples/sec; 0.505 sec/batch)
2018-03-22 12:52:03.358894: step 9370, loss = 0.89 (254.6 examples/sec; 0.503 sec/batch)
2018-03-22 12:52:08.575241: step 9380, loss = 0.72 (245.4 examples/sec; 0.522 sec/batch)
2018-03-22 12:52:13.625442: step 9390, loss = 0.80 (253.5 examples/sec; 0.505 sec/batch)
2018-03-22 12:52:18.646828: step 9400, loss = 0.83 (254.9 examples/sec; 0.502 sec/batch)
2018-03-22 12:52:23.872951: step 9410, loss = 0.77 (244.9 examples/sec; 0.523 sec/batch)
2018-03-22 12:52:28.651815: step 9420, loss = 0.93 (267.8 examples/sec; 0.478 sec/batch)
2018-03-22 12:52:34.224707: step 9430, loss = 1.11 (229.7 examples/sec; 0.557 sec/batch)
2018-03-22 12:52:39.854113: step 9440, loss = 1.03 (227.4 examples/sec; 0.563 sec/batch)
2018-03-22 12:52:44.614861: step 9450, loss = 0.84 (268.9 examples/sec; 0.476 sec/batch)
2018-03-22 12:52:49.249432: step 9460, loss = 1.05 (276.2 examples/sec; 0.463 sec/batch)
2018-03-22 12:52:54.809574: step 9470, loss = 0.91 (230.2 examples/sec; 0.556 sec/batch)
2018-03-22 12:53:00.115822: step 9480, loss = 0.79 (241.2 examples/sec; 0.531 sec/batch)
2018-03-22 12:53:04.644751: step 9490, loss = 0.84 (282.6 examples/sec; 0.453 sec/batch)
2018-03-22 12:53:10.109433: step 9500, loss = 0.89 (234.2 examples/sec; 0.546 sec/batch)
2018-03-22 12:53:15.095717: step 9510, loss = 0.91 (256.7 examples/sec; 0.499 sec/batch)
2018-03-22 12:53:20.784398: step 9520, loss = 0.89 (225.0 examples/sec; 0.569 sec/batch)
2018-03-22 12:53:25.590439: step 9530, loss = 0.92 (266.3 examples/sec; 0.481 sec/batch)
2018-03-22 12:53:30.801478: step 9540, loss = 0.74 (245.6 examples/sec; 0.521 sec/batch)
2018-03-22 12:53:36.173388: step 9550, loss = 0.95 (238.3 examples/sec; 0.537 sec/batch)
2018-03-22 12:53:41.028475: step 9560, loss = 0.97 (263.6 examples/sec; 0.486 sec/batch)
2018-03-22 12:53:45.606649: step 9570, loss = 0.78 (279.6 examples/sec; 0.458 sec/batch)
2018-03-22 12:53:50.609115: step 9580, loss = 1.00 (255.9 examples/sec; 0.500 sec/batch)
2018-03-22 12:53:55.625372: step 9590, loss = 0.82 (255.2 examples/sec; 0.502 sec/batch)
2018-03-22 12:54:00.619838: step 9600, loss = 0.89 (256.3 examples/sec; 0.499 sec/batch)
2018-03-22 12:54:05.331379: step 9610, loss = 0.82 (271.7 examples/sec; 0.471 sec/batch)
2018-03-22 12:54:10.800533: step 9620, loss = 0.73 (234.0 examples/sec; 0.547 sec/batch)
2018-03-22 12:54:16.247364: step 9630, loss = 0.96 (235.0 examples/sec; 0.545 sec/batch)
2018-03-22 12:54:21.540416: step 9640, loss = 0.80 (241.8 examples/sec; 0.529 sec/batch)
2018-03-22 12:54:26.164074: step 9650, loss = 0.84 (276.8 examples/sec; 0.462 sec/batch)
2018-03-22 12:54:31.003364: step 9660, loss = 0.93 (264.5 examples/sec; 0.484 sec/batch)
2018-03-22 12:54:35.761415: step 9670, loss = 0.98 (269.0 examples/sec; 0.476 sec/batch)
2018-03-22 12:54:40.257303: step 9680, loss = 1.00 (284.7 examples/sec; 0.450 sec/batch)
2018-03-22 12:54:44.590579: step 9690, loss = 0.89 (295.4 examples/sec; 0.433 sec/batch)
2018-03-22 12:54:49.419746: step 9700, loss = 0.79 (265.1 examples/sec; 0.483 sec/batch)
2018-03-22 12:54:54.498557: step 9710, loss = 1.14 (252.0 examples/sec; 0.508 sec/batch)
2018-03-22 12:54:59.065430: step 9720, loss = 0.84 (280.3 examples/sec; 0.457 sec/batch)
2018-03-22 12:55:04.008219: step 9730, loss = 0.80 (259.0 examples/sec; 0.494 sec/batch)
2018-03-22 12:55:09.510529: step 9740, loss = 0.84 (232.6 examples/sec; 0.550 sec/batch)
2018-03-22 12:55:14.844593: step 9750, loss = 1.10 (240.0 examples/sec; 0.533 sec/batch)
2018-03-22 12:55:19.772994: step 9760, loss = 0.78 (259.7 examples/sec; 0.493 sec/batch)
2018-03-22 12:55:24.533395: step 9770, loss = 0.95 (268.9 examples/sec; 0.476 sec/batch)
2018-03-22 12:55:30.416526: step 9780, loss = 0.95 (217.6 examples/sec; 0.588 sec/batch)
2018-03-22 12:55:35.678430: step 9790, loss = 0.94 (243.3 examples/sec; 0.526 sec/batch)
2018-03-22 12:55:40.863856: step 9800, loss = 0.94 (246.8 examples/sec; 0.519 sec/batch)
2018-03-22 12:55:46.225406: step 9810, loss = 0.86 (238.7 examples/sec; 0.536 sec/batch)
2018-03-22 12:55:51.027699: step 9820, loss = 0.84 (266.5 examples/sec; 0.480 sec/batch)
2018-03-22 12:55:55.776357: step 9830, loss = 1.19 (269.5 examples/sec; 0.475 sec/batch)
2018-03-22 12:56:00.583435: step 9840, loss = 0.99 (266.3 examples/sec; 0.481 sec/batch)
2018-03-22 12:56:05.975369: step 9850, loss = 0.86 (237.4 examples/sec; 0.539 sec/batch)
2018-03-22 12:56:11.361814: step 9860, loss = 0.83 (237.6 examples/sec; 0.539 sec/batch)
2018-03-22 12:56:15.870416: step 9870, loss = 0.98 (283.9 examples/sec; 0.451 sec/batch)
2018-03-22 12:56:21.051343: step 9880, loss = 1.16 (247.1 examples/sec; 0.518 sec/batch)
2018-03-22 12:56:26.064529: step 9890, loss = 0.89 (255.3 examples/sec; 0.501 sec/batch)
2018-03-22 12:56:31.420688: step 9900, loss = 0.80 (239.0 examples/sec; 0.536 sec/batch)
2018-03-22 12:56:36.319569: step 9910, loss = 1.03 (261.3 examples/sec; 0.490 sec/batch)
2018-03-22 12:56:40.933360: step 9920, loss = 1.04 (277.4 examples/sec; 0.461 sec/batch)
2018-03-22 12:56:46.169403: step 9930, loss = 0.92 (244.5 examples/sec; 0.524 sec/batch)
2018-03-22 12:56:51.418411: step 9940, loss = 0.92 (243.9 examples/sec; 0.525 sec/batch)
2018-03-22 12:56:56.659380: step 9950, loss = 0.92 (244.2 examples/sec; 0.524 sec/batch)
2018-03-22 12:57:02.008008: step 9960, loss = 0.80 (239.3 examples/sec; 0.535 sec/batch)
2018-03-22 12:57:06.738560: step 9970, loss = 0.82 (270.6 examples/sec; 0.473 sec/batch)
2018-03-22 12:57:11.572352: step 9980, loss = 0.90 (264.8 examples/sec; 0.483 sec/batch)
2018-03-22 12:57:17.192377: step 9990, loss = 0.97 (227.8 examples/sec; 0.562 sec/batch)
2018-03-22 12:57:22.411586: step 10000, loss = 0.84 (245.2 examples/sec; 0.522 sec/batch)
2018-03-22 12:57:26.888504: step 10010, loss = 0.85 (285.9 examples/sec; 0.448 sec/batch)
2018-03-22 12:57:32.764432: step 10020, loss = 0.92 (217.8 examples/sec; 0.588 sec/batch)
2018-03-22 12:57:37.678401: step 10030, loss = 0.86 (260.5 examples/sec; 0.491 sec/batch)
2018-03-22 12:57:42.517601: step 10040, loss = 1.01 (264.5 examples/sec; 0.484 sec/batch)
2018-03-22 12:57:47.496315: step 10050, loss = 0.89 (257.1 examples/sec; 0.498 sec/batch)
2018-03-22 12:57:52.545429: step 10060, loss = 0.83 (253.5 examples/sec; 0.505 sec/batch)
2018-03-22 12:57:57.487424: step 10070, loss = 0.83 (259.0 examples/sec; 0.494 sec/batch)
2018-03-22 12:58:02.573433: step 10080, loss = 0.80 (251.7 examples/sec; 0.509 sec/batch)
2018-03-22 12:58:08.009451: step 10090, loss = 0.78 (235.5 examples/sec; 0.544 sec/batch)
2018-03-22 12:58:13.683223: step 10100, loss = 0.78 (225.6 examples/sec; 0.567 sec/batch)
2018-03-22 12:58:18.797040: step 10110, loss = 0.67 (250.3 examples/sec; 0.511 sec/batch)
2018-03-22 12:58:23.569412: step 10120, loss = 0.88 (268.2 examples/sec; 0.477 sec/batch)
2018-03-22 12:58:28.507640: step 10130, loss = 0.84 (259.2 examples/sec; 0.494 sec/batch)
2018-03-22 12:58:33.208352: step 10140, loss = 0.93 (272.3 examples/sec; 0.470 sec/batch)
2018-03-22 12:58:38.137801: step 10150, loss = 0.90 (259.7 examples/sec; 0.493 sec/batch)
2018-03-22 12:58:43.078383: step 10160, loss = 0.74 (259.1 examples/sec; 0.494 sec/batch)
2018-03-22 12:58:47.844213: step 10170, loss = 0.83 (268.6 examples/sec; 0.477 sec/batch)
2018-03-22 12:58:53.122348: step 10180, loss = 0.95 (242.5 examples/sec; 0.528 sec/batch)
2018-03-22 12:58:58.339916: step 10190, loss = 0.92 (245.3 examples/sec; 0.522 sec/batch)
2018-03-22 12:59:04.104082: step 10200, loss = 0.83 (222.1 examples/sec; 0.576 sec/batch)
2018-03-22 12:59:08.866538: step 10210, loss = 1.05 (268.8 examples/sec; 0.476 sec/batch)
2018-03-22 12:59:13.728050: step 10220, loss = 0.79 (263.3 examples/sec; 0.486 sec/batch)
2018-03-22 12:59:18.764434: step 10230, loss = 0.87 (254.2 examples/sec; 0.504 sec/batch)
2018-03-22 12:59:24.208586: step 10240, loss = 0.84 (235.1 examples/sec; 0.544 sec/batch)
2018-03-22 12:59:29.485345: step 10250, loss = 0.90 (242.6 examples/sec; 0.528 sec/batch)
2018-03-22 12:59:34.593431: step 10260, loss = 0.86 (250.6 examples/sec; 0.511 sec/batch)
2018-03-22 12:59:39.820333: step 10270, loss = 0.93 (244.9 examples/sec; 0.523 sec/batch)
2018-03-22 12:59:44.629407: step 10280, loss = 0.91 (266.2 examples/sec; 0.481 sec/batch)
2018-03-22 12:59:50.436428: step 10290, loss = 0.86 (220.4 examples/sec; 0.581 sec/batch)
2018-03-22 12:59:55.813129: step 10300, loss = 1.19 (238.1 examples/sec; 0.538 sec/batch)
2018-03-22 13:00:00.787427: step 10310, loss = 0.85 (257.3 examples/sec; 0.497 sec/batch)
2018-03-22 13:00:05.808409: step 10320, loss = 0.85 (254.9 examples/sec; 0.502 sec/batch)
2018-03-22 13:00:11.179461: step 10330, loss = 1.00 (238.3 examples/sec; 0.537 sec/batch)
2018-03-22 13:00:16.610415: step 10340, loss = 0.82 (235.7 examples/sec; 0.543 sec/batch)
2018-03-22 13:00:21.831366: step 10350, loss = 0.82 (245.2 examples/sec; 0.522 sec/batch)
2018-03-22 13:00:27.083421: step 10360, loss = 0.85 (243.7 examples/sec; 0.525 sec/batch)
2018-03-22 13:00:32.104398: step 10370, loss = 0.84 (254.9 examples/sec; 0.502 sec/batch)
2018-03-22 13:00:37.396899: step 10380, loss = 0.93 (241.9 examples/sec; 0.529 sec/batch)
2018-03-22 13:00:42.688338: step 10390, loss = 1.01 (241.9 examples/sec; 0.529 sec/batch)
2018-03-22 13:00:48.030051: step 10400, loss = 0.85 (239.6 examples/sec; 0.534 sec/batch)
2018-03-22 13:00:52.739058: step 10410, loss = 0.93 (271.8 examples/sec; 0.471 sec/batch)
2018-03-22 13:00:58.018373: step 10420, loss = 0.84 (242.5 examples/sec; 0.528 sec/batch)
2018-03-22 13:01:03.053377: step 10430, loss = 1.01 (254.2 examples/sec; 0.504 sec/batch)
2018-03-22 13:01:08.247348: step 10440, loss = 1.12 (246.4 examples/sec; 0.519 sec/batch)
2018-03-22 13:01:13.456410: step 10450, loss = 1.15 (245.7 examples/sec; 0.521 sec/batch)
2018-03-22 13:01:17.967115: step 10460, loss = 0.72 (283.8 examples/sec; 0.451 sec/batch)
2018-03-22 13:01:23.066364: step 10470, loss = 0.85 (251.0 examples/sec; 0.510 sec/batch)
2018-03-22 13:01:27.842261: step 10480, loss = 0.93 (268.0 examples/sec; 0.478 sec/batch)
2018-03-22 13:01:33.207407: step 10490, loss = 0.78 (238.6 examples/sec; 0.537 sec/batch)
2018-03-22 13:01:38.330577: step 10500, loss = 0.98 (249.8 examples/sec; 0.512 sec/batch)
2018-03-22 13:01:43.310468: step 10510, loss = 0.97 (257.0 examples/sec; 0.498 sec/batch)
2018-03-22 13:01:48.321413: step 10520, loss = 0.83 (255.4 examples/sec; 0.501 sec/batch)
2018-03-22 13:01:54.006550: step 10530, loss = 0.81 (225.1 examples/sec; 0.569 sec/batch)
2018-03-22 13:01:59.180605: step 10540, loss = 0.83 (247.4 examples/sec; 0.517 sec/batch)
2018-03-22 13:02:04.175980: step 10550, loss = 0.96 (256.2 examples/sec; 0.500 sec/batch)
2018-03-22 13:02:09.003711: step 10560, loss = 0.80 (265.1 examples/sec; 0.483 sec/batch)
2018-03-22 13:02:13.995455: step 10570, loss = 0.77 (256.4 examples/sec; 0.499 sec/batch)
2018-03-22 13:02:18.495672: step 10580, loss = 0.88 (284.4 examples/sec; 0.450 sec/batch)
2018-03-22 13:02:22.860792: step 10590, loss = 0.93 (293.2 examples/sec; 0.437 sec/batch)
2018-03-22 13:02:28.088781: step 10600, loss = 0.86 (244.8 examples/sec; 0.523 sec/batch)
2018-03-22 13:02:32.510587: step 10610, loss = 0.86 (289.5 examples/sec; 0.442 sec/batch)
2018-03-22 13:02:37.384452: step 10620, loss = 1.13 (262.6 examples/sec; 0.487 sec/batch)
2018-03-22 13:02:42.896464: step 10630, loss = 0.83 (232.2 examples/sec; 0.551 sec/batch)
2018-03-22 13:02:47.752352: step 10640, loss = 0.70 (263.6 examples/sec; 0.486 sec/batch)
2018-03-22 13:02:52.888400: step 10650, loss = 1.04 (249.2 examples/sec; 0.514 sec/batch)
2018-03-22 13:02:57.952723: step 10660, loss = 0.94 (252.7 examples/sec; 0.506 sec/batch)
2018-03-22 13:03:03.482402: step 10670, loss = 0.85 (231.5 examples/sec; 0.553 sec/batch)
2018-03-22 13:03:08.187350: step 10680, loss = 0.89 (272.1 examples/sec; 0.470 sec/batch)
2018-03-22 13:03:13.210417: step 10690, loss = 0.88 (254.8 examples/sec; 0.502 sec/batch)
2018-03-22 13:03:18.926285: step 10700, loss = 1.01 (223.9 examples/sec; 0.572 sec/batch)
2018-03-22 13:03:24.449412: step 10710, loss = 0.83 (231.8 examples/sec; 0.552 sec/batch)
2018-03-22 13:03:29.762933: step 10720, loss = 0.93 (240.9 examples/sec; 0.531 sec/batch)
2018-03-22 13:03:34.940670: step 10730, loss = 0.87 (247.2 examples/sec; 0.518 sec/batch)
2018-03-22 13:03:40.236058: step 10740, loss = 0.83 (241.7 examples/sec; 0.530 sec/batch)
2018-03-22 13:03:45.402350: step 10750, loss = 0.85 (247.8 examples/sec; 0.517 sec/batch)
2018-03-22 13:03:50.149118: step 10760, loss = 0.89 (269.7 examples/sec; 0.475 sec/batch)
2018-03-22 13:03:55.095139: step 10770, loss = 1.02 (258.8 examples/sec; 0.495 sec/batch)
2018-03-22 13:04:00.220392: step 10780, loss = 0.80 (249.7 examples/sec; 0.513 sec/batch)
2018-03-22 13:04:05.097318: step 10790, loss = 0.85 (262.5 examples/sec; 0.488 sec/batch)
2018-03-22 13:04:10.694936: step 10800, loss = 0.78 (228.7 examples/sec; 0.560 sec/batch)
2018-03-22 13:04:15.614374: step 10810, loss = 0.89 (260.2 examples/sec; 0.492 sec/batch)
2018-03-22 13:04:20.326410: step 10820, loss = 0.82 (271.6 examples/sec; 0.471 sec/batch)
2018-03-22 13:04:25.076352: step 10830, loss = 0.89 (269.5 examples/sec; 0.475 sec/batch)
2018-03-22 13:04:30.663351: step 10840, loss = 1.01 (229.1 examples/sec; 0.559 sec/batch)
2018-03-22 13:04:35.782345: step 10850, loss = 0.90 (250.0 examples/sec; 0.512 sec/batch)
2018-03-22 13:04:40.819716: step 10860, loss = 0.76 (254.1 examples/sec; 0.504 sec/batch)
2018-03-22 13:04:45.587980: step 10870, loss = 0.84 (268.4 examples/sec; 0.477 sec/batch)
2018-03-22 13:04:51.151535: step 10880, loss = 0.79 (230.1 examples/sec; 0.556 sec/batch)
2018-03-22 13:04:56.584399: step 10890, loss = 0.74 (235.6 examples/sec; 0.543 sec/batch)
2018-03-22 13:05:02.001856: step 10900, loss = 0.74 (236.3 examples/sec; 0.542 sec/batch)
2018-03-22 13:05:06.875597: step 10910, loss = 0.98 (262.6 examples/sec; 0.487 sec/batch)
2018-03-22 13:05:11.651870: step 10920, loss = 0.91 (268.0 examples/sec; 0.478 sec/batch)
2018-03-22 13:05:16.330436: step 10930, loss = 0.78 (273.6 examples/sec; 0.468 sec/batch)
2018-03-22 13:05:21.592199: step 10940, loss = 0.78 (243.3 examples/sec; 0.526 sec/batch)
2018-03-22 13:05:26.472249: step 10950, loss = 1.00 (262.3 examples/sec; 0.488 sec/batch)
2018-03-22 13:05:31.787030: step 10960, loss = 0.80 (240.8 examples/sec; 0.531 sec/batch)
2018-03-22 13:05:36.842440: step 10970, loss = 0.78 (253.2 examples/sec; 0.506 sec/batch)
2018-03-22 13:05:42.069850: step 10980, loss = 0.77 (244.9 examples/sec; 0.523 sec/batch)
2018-03-22 13:05:46.986856: step 10990, loss = 0.89 (260.3 examples/sec; 0.492 sec/batch)
2018-03-22 13:05:52.758622: step 11000, loss = 0.79 (221.8 examples/sec; 0.577 sec/batch)
2018-03-22 13:05:57.434069: step 11010, loss = 0.68 (273.8 examples/sec; 0.468 sec/batch)
2018-03-22 13:06:02.530396: step 11020, loss = 1.06 (251.2 examples/sec; 0.510 sec/batch)
2018-03-22 13:06:07.738419: step 11030, loss = 0.91 (245.8 examples/sec; 0.521 sec/batch)
2018-03-22 13:06:12.548995: step 11040, loss = 1.00 (266.1 examples/sec; 0.481 sec/batch)
2018-03-22 13:06:17.087430: step 11050, loss = 0.77 (282.0 examples/sec; 0.454 sec/batch)
2018-03-22 13:06:21.739020: step 11060, loss = 0.87 (275.2 examples/sec; 0.465 sec/batch)
2018-03-22 13:06:26.455387: step 11070, loss = 0.89 (271.4 examples/sec; 0.472 sec/batch)
2018-03-22 13:06:31.694584: step 11080, loss = 0.73 (244.3 examples/sec; 0.524 sec/batch)
2018-03-22 13:06:36.455415: step 11090, loss = 0.78 (268.9 examples/sec; 0.476 sec/batch)
2018-03-22 13:06:41.993060: step 11100, loss = 0.88 (231.1 examples/sec; 0.554 sec/batch)
2018-03-22 13:06:47.125339: step 11110, loss = 0.83 (249.4 examples/sec; 0.513 sec/batch)
2018-03-22 13:06:52.009309: step 11120, loss = 0.77 (262.1 examples/sec; 0.488 sec/batch)
2018-03-22 13:06:56.764319: step 11130, loss = 1.14 (269.2 examples/sec; 0.476 sec/batch)
2018-03-22 13:07:02.351180: step 11140, loss = 0.68 (229.1 examples/sec; 0.559 sec/batch)
2018-03-22 13:07:06.991351: step 11150, loss = 1.01 (275.9 examples/sec; 0.464 sec/batch)
2018-03-22 13:07:11.942410: step 11160, loss = 0.83 (258.5 examples/sec; 0.495 sec/batch)
2018-03-22 13:07:16.401095: step 11170, loss = 0.83 (287.1 examples/sec; 0.446 sec/batch)
2018-03-22 13:07:21.584861: step 11180, loss = 0.91 (246.9 examples/sec; 0.518 sec/batch)
2018-03-22 13:07:27.087363: step 11190, loss = 0.99 (232.6 examples/sec; 0.550 sec/batch)
2018-03-22 13:07:32.282039: step 11200, loss = 0.83 (246.4 examples/sec; 0.519 sec/batch)
2018-03-22 13:07:37.311654: step 11210, loss = 0.88 (254.5 examples/sec; 0.503 sec/batch)
2018-03-22 13:07:42.914424: step 11220, loss = 0.75 (228.5 examples/sec; 0.560 sec/batch)
2018-03-22 13:07:47.782923: step 11230, loss = 1.00 (262.9 examples/sec; 0.487 sec/batch)
2018-03-22 13:07:52.771573: step 11240, loss = 0.90 (256.6 examples/sec; 0.499 sec/batch)
2018-03-22 13:07:58.111379: step 11250, loss = 1.06 (239.7 examples/sec; 0.534 sec/batch)
2018-03-22 13:08:03.449395: step 11260, loss = 0.84 (239.8 examples/sec; 0.534 sec/batch)
2018-03-22 13:08:08.310423: step 11270, loss = 0.78 (263.3 examples/sec; 0.486 sec/batch)
2018-03-22 13:08:13.170208: step 11280, loss = 0.92 (263.4 examples/sec; 0.486 sec/batch)
2018-03-22 13:08:18.124681: step 11290, loss = 0.85 (258.4 examples/sec; 0.495 sec/batch)
2018-03-22 13:08:23.758343: step 11300, loss = 0.86 (227.2 examples/sec; 0.563 sec/batch)
2018-03-22 13:08:28.772436: step 11310, loss = 0.77 (255.3 examples/sec; 0.501 sec/batch)
2018-03-22 13:08:34.085352: step 11320, loss = 1.00 (240.9 examples/sec; 0.531 sec/batch)
2018-03-22 13:08:38.862430: step 11330, loss = 0.75 (267.9 examples/sec; 0.478 sec/batch)
2018-03-22 13:08:43.949415: step 11340, loss = 0.89 (251.6 examples/sec; 0.509 sec/batch)
2018-03-22 13:08:49.691349: step 11350, loss = 0.81 (222.9 examples/sec; 0.574 sec/batch)
2018-03-22 13:08:55.212261: step 11360, loss = 0.87 (231.8 examples/sec; 0.552 sec/batch)
2018-03-22 13:09:00.276303: step 11370, loss = 0.92 (252.8 examples/sec; 0.506 sec/batch)
2018-03-22 13:09:04.857353: step 11380, loss = 0.84 (279.4 examples/sec; 0.458 sec/batch)
2018-03-22 13:09:09.809415: step 11390, loss = 0.97 (258.5 examples/sec; 0.495 sec/batch)
2018-03-22 13:09:14.983469: step 11400, loss = 0.82 (247.4 examples/sec; 0.517 sec/batch)
2018-03-22 13:09:20.149525: step 11410, loss = 0.93 (247.8 examples/sec; 0.517 sec/batch)
2018-03-22 13:09:24.645202: step 11420, loss = 0.88 (284.7 examples/sec; 0.450 sec/batch)
2018-03-22 13:09:30.276411: step 11430, loss = 0.79 (227.3 examples/sec; 0.563 sec/batch)
2018-03-22 13:09:35.328447: step 11440, loss = 0.85 (253.4 examples/sec; 0.505 sec/batch)
2018-03-22 13:09:40.077428: step 11450, loss = 0.70 (269.5 examples/sec; 0.475 sec/batch)
2018-03-22 13:09:45.104276: step 11460, loss = 0.73 (254.6 examples/sec; 0.503 sec/batch)
2018-03-22 13:09:50.015415: step 11470, loss = 0.76 (260.6 examples/sec; 0.491 sec/batch)
2018-03-22 13:09:54.350320: step 11480, loss = 0.86 (295.3 examples/sec; 0.433 sec/batch)
2018-03-22 13:09:59.424386: step 11490, loss = 0.77 (252.3 examples/sec; 0.507 sec/batch)
2018-03-22 13:10:04.881191: step 11500, loss = 0.79 (234.6 examples/sec; 0.546 sec/batch)
2018-03-22 13:10:09.398457: step 11510, loss = 1.09 (283.4 examples/sec; 0.452 sec/batch)
2018-03-22 13:10:14.217899: step 11520, loss = 0.85 (265.6 examples/sec; 0.482 sec/batch)
2018-03-22 13:10:19.459527: step 11530, loss = 0.75 (244.2 examples/sec; 0.524 sec/batch)
2018-03-22 13:10:25.251433: step 11540, loss = 0.81 (221.0 examples/sec; 0.579 sec/batch)
2018-03-22 13:10:30.707987: step 11550, loss = 0.99 (234.6 examples/sec; 0.546 sec/batch)
2018-03-22 13:10:35.338036: step 11560, loss = 0.88 (276.5 examples/sec; 0.463 sec/batch)
2018-03-22 13:10:41.171500: step 11570, loss = 0.92 (219.4 examples/sec; 0.583 sec/batch)
2018-03-22 13:10:46.501427: step 11580, loss = 0.87 (240.2 examples/sec; 0.533 sec/batch)
2018-03-22 13:10:51.709267: step 11590, loss = 0.92 (245.8 examples/sec; 0.521 sec/batch)
2018-03-22 13:10:57.319577: step 11600, loss = 0.77 (228.2 examples/sec; 0.561 sec/batch)
2018-03-22 13:11:02.040084: step 11610, loss = 0.78 (271.2 examples/sec; 0.472 sec/batch)
2018-03-22 13:11:07.152366: step 11620, loss = 0.81 (250.4 examples/sec; 0.511 sec/batch)
2018-03-22 13:11:12.009396: step 11630, loss = 0.91 (263.5 examples/sec; 0.486 sec/batch)
2018-03-22 13:11:17.353793: step 11640, loss = 1.02 (239.5 examples/sec; 0.534 sec/batch)
2018-03-22 13:11:22.664399: step 11650, loss = 0.88 (241.0 examples/sec; 0.531 sec/batch)
2018-03-22 13:11:27.361410: step 11660, loss = 0.69 (272.5 examples/sec; 0.470 sec/batch)
2018-03-22 13:11:32.407499: step 11670, loss = 0.89 (253.7 examples/sec; 0.505 sec/batch)
2018-03-22 13:11:37.429461: step 11680, loss = 0.99 (254.9 examples/sec; 0.502 sec/batch)
2018-03-22 13:11:43.246501: step 11690, loss = 0.89 (220.0 examples/sec; 0.582 sec/batch)
2018-03-22 13:11:48.600328: step 11700, loss = 0.87 (239.1 examples/sec; 0.535 sec/batch)
2018-03-22 13:11:53.465450: step 11710, loss = 0.87 (263.1 examples/sec; 0.487 sec/batch)
2018-03-22 13:11:58.421315: step 11720, loss = 0.89 (258.3 examples/sec; 0.496 sec/batch)
2018-03-22 13:12:03.551342: step 11730, loss = 0.97 (249.5 examples/sec; 0.513 sec/batch)
2018-03-22 13:12:08.543400: step 11740, loss = 0.86 (256.4 examples/sec; 0.499 sec/batch)
2018-03-22 13:12:14.080657: step 11750, loss = 1.02 (231.2 examples/sec; 0.554 sec/batch)
2018-03-22 13:12:18.784462: step 11760, loss = 1.03 (272.1 examples/sec; 0.470 sec/batch)
2018-03-22 13:12:23.603120: step 11770, loss = 0.87 (265.6 examples/sec; 0.482 sec/batch)
2018-03-22 13:12:28.346454: step 11780, loss = 0.84 (269.9 examples/sec; 0.474 sec/batch)
2018-03-22 13:12:33.959353: step 11790, loss = 0.91 (228.0 examples/sec; 0.561 sec/batch)
2018-03-22 13:12:38.937336: step 11800, loss = 0.89 (257.1 examples/sec; 0.498 sec/batch)
2018-03-22 13:12:44.009440: step 11810, loss = 0.87 (252.4 examples/sec; 0.507 sec/batch)
2018-03-22 13:12:48.880104: step 11820, loss = 0.85 (262.8 examples/sec; 0.487 sec/batch)
2018-03-22 13:12:54.257740: step 11830, loss = 0.91 (238.0 examples/sec; 0.538 sec/batch)
2018-03-22 13:12:58.898469: step 11840, loss = 0.99 (275.8 examples/sec; 0.464 sec/batch)
2018-03-22 13:13:03.187389: step 11850, loss = 0.67 (298.4 examples/sec; 0.429 sec/batch)
2018-03-22 13:13:08.370345: step 11860, loss = 0.75 (247.0 examples/sec; 0.518 sec/batch)
2018-03-22 13:13:13.940371: step 11870, loss = 0.77 (229.8 examples/sec; 0.557 sec/batch)
2018-03-22 13:13:18.558795: step 11880, loss = 1.05 (277.2 examples/sec; 0.462 sec/batch)
2018-03-22 13:13:23.477305: step 11890, loss = 0.79 (260.2 examples/sec; 0.492 sec/batch)
2018-03-22 13:13:28.743103: step 11900, loss = 0.87 (243.1 examples/sec; 0.527 sec/batch)
2018-03-22 13:13:33.998048: step 11910, loss = 0.81 (243.6 examples/sec; 0.525 sec/batch)
2018-03-22 13:13:38.599140: step 11920, loss = 0.82 (278.2 examples/sec; 0.460 sec/batch)
2018-03-22 13:13:43.374414: step 11930, loss = 0.94 (268.0 examples/sec; 0.478 sec/batch)
2018-03-22 13:13:48.088163: step 11940, loss = 0.75 (271.5 examples/sec; 0.471 sec/batch)
2018-03-22 13:13:53.540354: step 11950, loss = 0.84 (234.8 examples/sec; 0.545 sec/batch)
2018-03-22 13:13:58.087694: step 11960, loss = 0.80 (281.5 examples/sec; 0.455 sec/batch)
2018-03-22 13:14:02.763346: step 11970, loss = 1.09 (273.8 examples/sec; 0.468 sec/batch)
2018-03-22 13:14:07.654484: step 11980, loss = 0.96 (261.7 examples/sec; 0.489 sec/batch)
2018-03-22 13:14:13.331396: step 11990, loss = 0.90 (225.5 examples/sec; 0.568 sec/batch)
2018-03-22 13:14:18.678104: step 12000, loss = 0.93 (239.4 examples/sec; 0.535 sec/batch)
2018-03-22 13:14:24.365414: step 12010, loss = 0.88 (225.1 examples/sec; 0.569 sec/batch)
2018-03-22 13:14:29.011772: step 12020, loss = 0.81 (275.5 examples/sec; 0.465 sec/batch)
2018-03-22 13:14:33.740384: step 12030, loss = 1.05 (270.7 examples/sec; 0.473 sec/batch)
2018-03-22 13:14:40.072790: step 12040, loss = 0.91 (202.1 examples/sec; 0.633 sec/batch)
2018-03-22 13:14:45.136353: step 12050, loss = 0.84 (252.8 examples/sec; 0.506 sec/batch)
2018-03-22 13:14:50.295399: step 12060, loss = 0.80 (248.1 examples/sec; 0.516 sec/batch)
2018-03-22 13:14:54.599038: step 12070, loss = 0.78 (297.4 examples/sec; 0.430 sec/batch)
2018-03-22 13:14:59.384331: step 12080, loss = 0.76 (267.5 examples/sec; 0.479 sec/batch)
2018-03-22 13:15:04.997844: step 12090, loss = 0.91 (228.0 examples/sec; 0.561 sec/batch)
2018-03-22 13:15:10.325399: step 12100, loss = 0.74 (240.3 examples/sec; 0.533 sec/batch)
2018-03-22 13:15:15.032338: step 12110, loss = 0.77 (271.9 examples/sec; 0.471 sec/batch)
2018-03-22 13:15:20.201403: step 12120, loss = 0.90 (247.6 examples/sec; 0.517 sec/batch)
2018-03-22 13:15:25.021925: step 12130, loss = 0.93 (265.5 examples/sec; 0.482 sec/batch)
2018-03-22 13:15:29.907986: step 12140, loss = 0.73 (262.0 examples/sec; 0.489 sec/batch)
2018-03-22 13:15:34.414377: step 12150, loss = 0.83 (284.0 examples/sec; 0.451 sec/batch)
2018-03-22 13:15:40.122975: step 12160, loss = 0.77 (224.2 examples/sec; 0.571 sec/batch)
2018-03-22 13:15:45.081404: step 12170, loss = 0.94 (258.1 examples/sec; 0.496 sec/batch)
2018-03-22 13:15:50.009451: step 12180, loss = 0.88 (259.7 examples/sec; 0.493 sec/batch)
2018-03-22 13:15:54.873401: step 12190, loss = 0.93 (263.2 examples/sec; 0.486 sec/batch)
2018-03-22 13:16:00.428813: step 12200, loss = 0.85 (230.4 examples/sec; 0.556 sec/batch)
2018-03-22 13:16:05.439932: step 12210, loss = 0.82 (255.4 examples/sec; 0.501 sec/batch)
2018-03-22 13:16:10.051551: step 12220, loss = 0.92 (277.6 examples/sec; 0.461 sec/batch)
2018-03-22 13:16:14.884625: step 12230, loss = 0.81 (264.8 examples/sec; 0.483 sec/batch)
2018-03-22 13:16:19.798071: step 12240, loss = 0.72 (260.5 examples/sec; 0.491 sec/batch)
2018-03-22 13:16:25.077357: step 12250, loss = 1.04 (242.5 examples/sec; 0.528 sec/batch)
2018-03-22 13:16:30.259376: step 12260, loss = 1.11 (247.0 examples/sec; 0.518 sec/batch)
2018-03-22 13:16:34.871479: step 12270, loss = 0.99 (277.5 examples/sec; 0.461 sec/batch)
2018-03-22 13:16:39.974451: step 12280, loss = 0.95 (250.8 examples/sec; 0.510 sec/batch)
2018-03-22 13:16:44.776936: step 12290, loss = 0.91 (266.5 examples/sec; 0.480 sec/batch)
2018-03-22 13:16:49.863354: step 12300, loss = 0.88 (251.7 examples/sec; 0.509 sec/batch)
2018-03-22 13:16:54.184843: step 12310, loss = 0.87 (296.2 examples/sec; 0.432 sec/batch)
2018-03-22 13:16:58.953481: step 12320, loss = 0.93 (268.4 examples/sec; 0.477 sec/batch)
2018-03-22 13:17:04.098417: step 12330, loss = 0.86 (248.8 examples/sec; 0.515 sec/batch)
2018-03-22 13:17:09.017339: step 12340, loss = 0.83 (260.2 examples/sec; 0.492 sec/batch)
2018-03-22 13:17:14.857383: step 12350, loss = 0.86 (219.2 examples/sec; 0.584 sec/batch)
2018-03-22 13:17:19.915141: step 12360, loss = 0.84 (253.1 examples/sec; 0.506 sec/batch)
2018-03-22 13:17:24.137405: step 12370, loss = 0.86 (303.2 examples/sec; 0.422 sec/batch)
2018-03-22 13:17:29.789322: step 12380, loss = 0.65 (226.5 examples/sec; 0.565 sec/batch)
2018-03-22 13:17:35.178408: step 12390, loss = 0.75 (237.5 examples/sec; 0.539 sec/batch)
2018-03-22 13:17:40.453912: step 12400, loss = 0.83 (242.6 examples/sec; 0.528 sec/batch)
2018-03-22 13:17:44.668400: step 12410, loss = 0.87 (303.7 examples/sec; 0.421 sec/batch)
2018-03-22 13:17:49.666048: step 12420, loss = 0.81 (256.1 examples/sec; 0.500 sec/batch)
2018-03-22 13:17:54.661420: step 12430, loss = 0.89 (256.2 examples/sec; 0.500 sec/batch)
2018-03-22 13:17:59.610389: step 12440, loss = 0.94 (258.6 examples/sec; 0.495 sec/batch)
2018-03-22 13:18:04.237481: step 12450, loss = 0.92 (276.6 examples/sec; 0.463 sec/batch)
2018-03-22 13:18:09.858407: step 12460, loss = 0.87 (227.7 examples/sec; 0.562 sec/batch)
2018-03-22 13:18:14.657331: step 12470, loss = 0.89 (266.7 examples/sec; 0.480 sec/batch)
2018-03-22 13:18:19.524427: step 12480, loss = 0.72 (263.0 examples/sec; 0.487 sec/batch)
2018-03-22 13:18:24.508343: step 12490, loss = 0.88 (256.8 examples/sec; 0.498 sec/batch)
2018-03-22 13:18:29.848895: step 12500, loss = 0.86 (239.7 examples/sec; 0.534 sec/batch)
2018-03-22 13:18:34.659379: step 12510, loss = 0.85 (266.1 examples/sec; 0.481 sec/batch)
2018-03-22 13:18:40.218362: step 12520, loss = 0.78 (230.3 examples/sec; 0.556 sec/batch)
2018-03-22 13:18:45.193134: step 12530, loss = 0.87 (257.3 examples/sec; 0.497 sec/batch)
2018-03-22 13:18:50.171428: step 12540, loss = 0.93 (257.1 examples/sec; 0.498 sec/batch)
2018-03-22 13:18:54.642449: step 12550, loss = 0.85 (286.3 examples/sec; 0.447 sec/batch)
2018-03-22 13:19:00.395321: step 12560, loss = 0.83 (222.5 examples/sec; 0.575 sec/batch)
2018-03-22 13:19:05.191367: step 12570, loss = 0.78 (266.9 examples/sec; 0.480 sec/batch)
2018-03-22 13:19:09.962322: step 12580, loss = 0.81 (268.3 examples/sec; 0.477 sec/batch)
2018-03-22 13:19:14.763440: step 12590, loss = 0.95 (266.6 examples/sec; 0.480 sec/batch)
2018-03-22 13:19:20.171053: step 12600, loss = 0.79 (236.7 examples/sec; 0.541 sec/batch)
2018-03-22 13:19:25.455743: step 12610, loss = 0.63 (242.2 examples/sec; 0.528 sec/batch)
2018-03-22 13:19:30.130495: step 12620, loss = 0.77 (273.8 examples/sec; 0.467 sec/batch)
2018-03-22 13:19:34.782809: step 12630, loss = 0.77 (275.1 examples/sec; 0.465 sec/batch)
2018-03-22 13:19:39.900421: step 12640, loss = 1.01 (250.1 examples/sec; 0.512 sec/batch)
2018-03-22 13:19:45.229250: step 12650, loss = 0.82 (240.2 examples/sec; 0.533 sec/batch)
2018-03-22 13:19:50.479847: step 12660, loss = 0.87 (243.8 examples/sec; 0.525 sec/batch)
2018-03-22 13:19:55.787230: step 12670, loss = 1.16 (241.2 examples/sec; 0.531 sec/batch)
2018-03-22 13:20:00.651087: step 12680, loss = 1.02 (263.2 examples/sec; 0.486 sec/batch)
2018-03-22 13:20:05.407683: step 12690, loss = 0.70 (269.1 examples/sec; 0.476 sec/batch)
2018-03-22 13:20:10.887885: step 12700, loss = 0.81 (233.6 examples/sec; 0.548 sec/batch)
2018-03-22 13:20:16.074486: step 12710, loss = 0.86 (246.8 examples/sec; 0.519 sec/batch)
2018-03-22 13:20:21.378129: step 12720, loss = 0.94 (241.3 examples/sec; 0.530 sec/batch)
2018-03-22 13:20:26.609955: step 12730, loss = 0.88 (244.7 examples/sec; 0.523 sec/batch)
2018-03-22 13:20:31.140324: step 12740, loss = 0.90 (282.5 examples/sec; 0.453 sec/batch)
2018-03-22 13:20:35.790367: step 12750, loss = 1.02 (275.3 examples/sec; 0.465 sec/batch)
2018-03-22 13:20:40.677352: step 12760, loss = 1.00 (261.9 examples/sec; 0.489 sec/batch)
2018-03-22 13:20:45.742131: step 12770, loss = 0.74 (252.7 examples/sec; 0.506 sec/batch)
2018-03-22 13:20:50.665354: step 12780, loss = 0.94 (260.0 examples/sec; 0.492 sec/batch)
2018-03-22 13:20:56.103378: step 12790, loss = 0.71 (235.4 examples/sec; 0.544 sec/batch)
2018-03-22 13:21:01.703385: step 12800, loss = 0.80 (228.6 examples/sec; 0.560 sec/batch)
2018-03-22 13:21:06.976349: step 12810, loss = 0.98 (242.7 examples/sec; 0.527 sec/batch)
2018-03-22 13:21:12.295313: step 12820, loss = 0.88 (240.6 examples/sec; 0.532 sec/batch)
2018-03-22 13:21:17.099376: step 12830, loss = 0.92 (266.4 examples/sec; 0.480 sec/batch)
2018-03-22 13:21:22.250352: step 12840, loss = 0.89 (248.5 examples/sec; 0.515 sec/batch)
2018-03-22 13:21:27.367594: step 12850, loss = 0.72 (250.1 examples/sec; 0.512 sec/batch)
2018-03-22 13:21:31.672961: step 12860, loss = 0.78 (297.3 examples/sec; 0.431 sec/batch)
2018-03-22 13:21:36.271462: step 12870, loss = 0.77 (278.4 examples/sec; 0.460 sec/batch)
2018-03-22 13:21:41.438097: step 12880, loss = 0.64 (247.7 examples/sec; 0.517 sec/batch)
2018-03-22 13:21:46.655185: step 12890, loss = 0.91 (245.3 examples/sec; 0.522 sec/batch)
2018-03-22 13:21:51.837910: step 12900, loss = 0.80 (247.0 examples/sec; 0.518 sec/batch)
2018-03-22 13:21:56.394536: step 12910, loss = 0.77 (280.9 examples/sec; 0.456 sec/batch)
2018-03-22 13:22:01.838384: step 12920, loss = 0.82 (235.1 examples/sec; 0.544 sec/batch)
2018-03-22 13:22:06.842324: step 12930, loss = 0.77 (255.8 examples/sec; 0.500 sec/batch)
2018-03-22 13:22:12.351330: step 12940, loss = 0.91 (232.3 examples/sec; 0.551 sec/batch)
2018-03-22 13:22:17.097399: step 12950, loss = 0.92 (269.7 examples/sec; 0.475 sec/batch)
2018-03-22 13:22:22.322902: step 12960, loss = 0.80 (245.0 examples/sec; 0.523 sec/batch)
2018-03-22 13:22:27.022523: step 12970, loss = 0.82 (272.4 examples/sec; 0.470 sec/batch)
2018-03-22 13:22:32.351300: step 12980, loss = 0.84 (240.2 examples/sec; 0.533 sec/batch)
2018-03-22 13:22:37.218901: step 12990, loss = 0.65 (263.0 examples/sec; 0.487 sec/batch)
2018-03-22 13:22:42.936066: step 13000, loss = 0.65 (223.9 examples/sec; 0.572 sec/batch)
2018-03-22 13:22:48.101397: step 13010, loss = 0.85 (247.8 examples/sec; 0.517 sec/batch)
2018-03-22 13:22:53.654332: step 13020, loss = 0.80 (230.5 examples/sec; 0.555 sec/batch)
2018-03-22 13:22:58.387402: step 13030, loss = 0.85 (270.4 examples/sec; 0.473 sec/batch)
2018-03-22 13:23:03.022119: step 13040, loss = 0.96 (276.2 examples/sec; 0.463 sec/batch)
2018-03-22 13:23:07.909547: step 13050, loss = 0.80 (261.9 examples/sec; 0.489 sec/batch)
2018-03-22 13:23:13.349449: step 13060, loss = 0.74 (235.3 examples/sec; 0.544 sec/batch)
2018-03-22 13:23:18.420365: step 13070, loss = 0.92 (252.4 examples/sec; 0.507 sec/batch)
2018-03-22 13:23:22.942737: step 13080, loss = 0.90 (283.0 examples/sec; 0.452 sec/batch)
2018-03-22 13:23:28.238031: step 13090, loss = 0.77 (241.7 examples/sec; 0.530 sec/batch)
2018-03-22 13:23:33.665375: step 13100, loss = 0.81 (235.8 examples/sec; 0.543 sec/batch)
2018-03-22 13:23:38.562338: step 13110, loss = 0.79 (261.4 examples/sec; 0.490 sec/batch)
2018-03-22 13:23:44.103296: step 13120, loss = 0.81 (231.0 examples/sec; 0.554 sec/batch)
2018-03-22 13:23:48.504998: step 13130, loss = 0.86 (290.8 examples/sec; 0.440 sec/batch)
2018-03-22 13:23:53.540160: step 13140, loss = 0.88 (254.2 examples/sec; 0.504 sec/batch)
2018-03-22 13:23:58.928371: step 13150, loss = 0.79 (237.6 examples/sec; 0.539 sec/batch)
2018-03-22 13:24:04.089232: step 13160, loss = 0.93 (248.0 examples/sec; 0.516 sec/batch)
2018-03-22 13:24:08.728367: step 13170, loss = 0.85 (275.9 examples/sec; 0.464 sec/batch)
2018-03-22 13:24:13.579353: step 13180, loss = 0.85 (263.9 examples/sec; 0.485 sec/batch)
2018-03-22 13:24:18.335386: step 13190, loss = 1.07 (269.1 examples/sec; 0.476 sec/batch)
2018-03-22 13:24:24.103488: step 13200, loss = 0.70 (221.9 examples/sec; 0.577 sec/batch)
2018-03-22 13:24:28.807485: step 13210, loss = 0.97 (272.1 examples/sec; 0.470 sec/batch)
2018-03-22 13:24:33.461329: step 13220, loss = 0.88 (275.0 examples/sec; 0.465 sec/batch)
2018-03-22 13:24:38.803821: step 13230, loss = 0.73 (239.6 examples/sec; 0.534 sec/batch)
2018-03-22 13:24:43.633360: step 13240, loss = 0.82 (265.0 examples/sec; 0.483 sec/batch)
2018-03-22 13:24:49.450430: step 13250, loss = 0.89 (220.0 examples/sec; 0.582 sec/batch)
2018-03-22 13:24:54.341294: step 13260, loss = 0.89 (261.7 examples/sec; 0.489 sec/batch)
2018-03-22 13:24:59.788682: step 13270, loss = 0.87 (235.0 examples/sec; 0.545 sec/batch)
2018-03-22 13:25:04.798381: step 13280, loss = 0.90 (255.5 examples/sec; 0.501 sec/batch)
2018-03-22 13:25:09.545411: step 13290, loss = 0.78 (269.6 examples/sec; 0.475 sec/batch)
2018-03-22 13:25:14.664747: step 13300, loss = 0.71 (250.0 examples/sec; 0.512 sec/batch)
2018-03-22 13:25:19.557451: step 13310, loss = 0.77 (261.6 examples/sec; 0.489 sec/batch)
2018-03-22 13:25:24.919415: step 13320, loss = 0.66 (238.7 examples/sec; 0.536 sec/batch)
2018-03-22 13:25:30.030388: step 13330, loss = 0.80 (250.4 examples/sec; 0.511 sec/batch)
2018-03-22 13:25:34.876990: step 13340, loss = 0.87 (264.1 examples/sec; 0.485 sec/batch)
2018-03-22 13:25:40.785423: step 13350, loss = 0.95 (216.6 examples/sec; 0.591 sec/batch)
2018-03-22 13:25:45.907338: step 13360, loss = 0.86 (249.9 examples/sec; 0.512 sec/batch)
2018-03-22 13:25:50.416450: step 13370, loss = 0.94 (283.9 examples/sec; 0.451 sec/batch)
2018-03-22 13:25:55.348456: step 13380, loss = 0.97 (259.5 examples/sec; 0.493 sec/batch)
2018-03-22 13:26:00.875449: step 13390, loss = 0.87 (231.6 examples/sec; 0.553 sec/batch)
2018-03-22 13:26:06.022397: step 13400, loss = 0.89 (248.7 examples/sec; 0.515 sec/batch)
2018-03-22 13:26:11.331174: step 13410, loss = 0.90 (241.1 examples/sec; 0.531 sec/batch)
2018-03-22 13:26:16.336545: step 13420, loss = 0.70 (255.7 examples/sec; 0.501 sec/batch)
2018-03-22 13:26:21.550538: step 13430, loss = 0.87 (245.5 examples/sec; 0.521 sec/batch)
2018-03-22 13:26:26.625365: step 13440, loss = 0.82 (252.2 examples/sec; 0.507 sec/batch)
2018-03-22 13:26:31.814403: step 13450, loss = 0.82 (246.7 examples/sec; 0.519 sec/batch)
2018-03-22 13:26:37.099993: step 13460, loss = 0.67 (242.2 examples/sec; 0.529 sec/batch)
2018-03-22 13:26:41.884354: step 13470, loss = 1.00 (267.5 examples/sec; 0.478 sec/batch)
2018-03-22 13:26:46.883445: step 13480, loss = 0.84 (256.0 examples/sec; 0.500 sec/batch)
2018-03-22 13:26:52.047855: step 13490, loss = 0.77 (247.9 examples/sec; 0.516 sec/batch)
2018-03-22 13:26:57.167471: step 13500, loss = 0.90 (250.0 examples/sec; 0.512 sec/batch)
2018-03-22 13:27:02.383652: step 13510, loss = 0.75 (245.4 examples/sec; 0.522 sec/batch)
2018-03-22 13:27:07.673381: step 13520, loss = 0.85 (242.0 examples/sec; 0.529 sec/batch)
2018-03-22 13:27:12.589438: step 13530, loss = 0.91 (260.4 examples/sec; 0.492 sec/batch)
2018-03-22 13:27:17.847481: step 13540, loss = 0.83 (243.4 examples/sec; 0.526 sec/batch)
2018-03-22 13:27:23.315407: step 13550, loss = 0.76 (234.1 examples/sec; 0.547 sec/batch)
2018-03-22 13:27:28.030651: step 13560, loss = 0.74 (271.5 examples/sec; 0.472 sec/batch)
2018-03-22 13:27:33.370318: step 13570, loss = 0.78 (239.7 examples/sec; 0.534 sec/batch)
2018-03-22 13:27:38.507274: step 13580, loss = 1.01 (249.2 examples/sec; 0.514 sec/batch)
2018-03-22 13:27:43.237745: step 13590, loss = 0.82 (270.6 examples/sec; 0.473 sec/batch)
2018-03-22 13:27:48.619533: step 13600, loss = 0.74 (237.8 examples/sec; 0.538 sec/batch)
2018-03-22 13:27:53.944777: step 13610, loss = 0.67 (240.4 examples/sec; 0.533 sec/batch)
2018-03-22 13:27:59.105450: step 13620, loss = 0.92 (248.0 examples/sec; 0.516 sec/batch)
2018-03-22 13:28:04.258381: step 13630, loss = 0.82 (248.4 examples/sec; 0.515 sec/batch)
2018-03-22 13:28:08.920035: step 13640, loss = 0.80 (274.6 examples/sec; 0.466 sec/batch)
2018-03-22 13:28:13.825366: step 13650, loss = 0.97 (260.9 examples/sec; 0.491 sec/batch)
2018-03-22 13:28:19.654031: step 13660, loss = 0.72 (219.6 examples/sec; 0.583 sec/batch)
2018-03-22 13:28:25.118937: step 13670, loss = 0.90 (234.2 examples/sec; 0.546 sec/batch)
2018-03-22 13:28:30.202181: step 13680, loss = 0.98 (251.8 examples/sec; 0.508 sec/batch)
2018-03-22 13:28:34.951468: step 13690, loss = 0.88 (269.5 examples/sec; 0.475 sec/batch)
2018-03-22 13:28:40.392622: step 13700, loss = 0.88 (235.2 examples/sec; 0.544 sec/batch)
2018-03-22 13:28:45.460312: step 13710, loss = 0.82 (252.6 examples/sec; 0.507 sec/batch)
2018-03-22 13:28:50.165387: step 13720, loss = 0.75 (272.0 examples/sec; 0.471 sec/batch)
2018-03-22 13:28:54.648390: step 13730, loss = 0.82 (285.5 examples/sec; 0.448 sec/batch)
2018-03-22 13:28:58.641486: step 13740, loss = 0.79 (320.6 examples/sec; 0.399 sec/batch)
2018-03-22 13:29:03.585440: step 13750, loss = 0.85 (258.9 examples/sec; 0.494 sec/batch)
2018-03-22 13:29:08.486410: step 13760, loss = 0.97 (261.2 examples/sec; 0.490 sec/batch)
2018-03-22 13:29:13.538301: step 13770, loss = 0.87 (253.4 examples/sec; 0.505 sec/batch)
2018-03-22 13:29:18.248494: step 13780, loss = 0.75 (271.8 examples/sec; 0.471 sec/batch)
2018-03-22 13:29:22.913375: step 13790, loss = 0.82 (274.4 examples/sec; 0.466 sec/batch)
2018-03-22 13:29:27.979254: step 13800, loss = 0.91 (252.7 examples/sec; 0.507 sec/batch)
2018-03-22 13:29:33.039863: step 13810, loss = 0.89 (252.9 examples/sec; 0.506 sec/batch)
2018-03-22 13:29:37.877585: step 13820, loss = 0.72 (264.6 examples/sec; 0.484 sec/batch)
2018-03-22 13:29:43.317425: step 13830, loss = 1.03 (235.3 examples/sec; 0.544 sec/batch)
2018-03-22 13:29:48.497331: step 13840, loss = 0.76 (247.1 examples/sec; 0.518 sec/batch)
2018-03-22 13:29:53.830433: step 13850, loss = 0.79 (240.0 examples/sec; 0.533 sec/batch)
2018-03-22 13:29:58.281129: step 13860, loss = 0.83 (287.6 examples/sec; 0.445 sec/batch)
2018-03-22 13:30:03.573660: step 13870, loss = 0.91 (241.8 examples/sec; 0.529 sec/batch)
2018-03-22 13:30:08.439338: step 13880, loss = 0.72 (263.1 examples/sec; 0.487 sec/batch)
2018-03-22 13:30:13.820377: step 13890, loss = 0.89 (237.9 examples/sec; 0.538 sec/batch)
2018-03-22 13:30:18.792940: step 13900, loss = 0.84 (257.4 examples/sec; 0.497 sec/batch)
2018-03-22 13:30:23.827280: step 13910, loss = 0.80 (254.3 examples/sec; 0.503 sec/batch)
2018-03-22 13:30:29.987359: step 13920, loss = 0.89 (207.8 examples/sec; 0.616 sec/batch)
2018-03-22 13:30:35.511366: step 13930, loss = 0.89 (231.7 examples/sec; 0.552 sec/batch)
2018-03-22 13:30:40.348317: step 13940, loss = 0.88 (264.6 examples/sec; 0.484 sec/batch)
2018-03-22 13:30:45.106055: step 13950, loss = 0.91 (269.0 examples/sec; 0.476 sec/batch)
2018-03-22 13:30:50.128695: step 13960, loss = 0.81 (254.8 examples/sec; 0.502 sec/batch)
2018-03-22 13:30:55.199931: step 13970, loss = 0.83 (252.4 examples/sec; 0.507 sec/batch)
2018-03-22 13:31:00.631465: step 13980, loss = 0.89 (235.7 examples/sec; 0.543 sec/batch)
2018-03-22 13:31:05.350338: step 13990, loss = 0.94 (271.3 examples/sec; 0.472 sec/batch)
2018-03-22 13:31:11.039443: step 14000, loss = 0.93 (225.0 examples/sec; 0.569 sec/batch)
2018-03-22 13:31:16.176298: step 14010, loss = 0.88 (249.2 examples/sec; 0.514 sec/batch)
2018-03-22 13:31:20.909138: step 14020, loss = 0.87 (270.5 examples/sec; 0.473 sec/batch)
2018-03-22 13:31:25.562162: step 14030, loss = 0.88 (275.1 examples/sec; 0.465 sec/batch)
2018-03-22 13:31:30.425695: step 14040, loss = 0.84 (263.2 examples/sec; 0.486 sec/batch)
2018-03-22 13:31:35.451379: step 14050, loss = 0.80 (254.7 examples/sec; 0.503 sec/batch)
2018-03-22 13:31:40.464496: step 14060, loss = 0.76 (255.3 examples/sec; 0.501 sec/batch)
2018-03-22 13:31:45.180126: step 14070, loss = 0.78 (271.4 examples/sec; 0.472 sec/batch)
2018-03-22 13:31:49.773792: step 14080, loss = 0.80 (278.6 examples/sec; 0.459 sec/batch)
2018-03-22 13:31:54.831179: step 14090, loss = 0.78 (253.1 examples/sec; 0.506 sec/batch)
2018-03-22 13:32:00.009455: step 14100, loss = 0.79 (247.2 examples/sec; 0.518 sec/batch)
2018-03-22 13:32:04.939447: step 14110, loss = 0.90 (259.6 examples/sec; 0.493 sec/batch)
2018-03-22 13:32:10.153398: step 14120, loss = 0.86 (245.5 examples/sec; 0.521 sec/batch)
2018-03-22 13:32:14.904777: step 14130, loss = 0.72 (269.4 examples/sec; 0.475 sec/batch)
2018-03-22 13:32:20.649408: step 14140, loss = 0.76 (222.8 examples/sec; 0.574 sec/batch)
2018-03-22 13:32:25.809492: step 14150, loss = 0.76 (248.1 examples/sec; 0.516 sec/batch)
2018-03-22 13:32:30.963284: step 14160, loss = 1.01 (248.4 examples/sec; 0.515 sec/batch)
2018-03-22 13:32:35.320375: step 14170, loss = 0.73 (293.8 examples/sec; 0.436 sec/batch)
2018-03-22 13:32:40.728320: step 14180, loss = 0.86 (236.7 examples/sec; 0.541 sec/batch)
2018-03-22 13:32:45.453112: step 14190, loss = 0.87 (270.9 examples/sec; 0.472 sec/batch)
2018-03-22 13:32:50.358366: step 14200, loss = 0.80 (260.9 examples/sec; 0.491 sec/batch)
2018-03-22 13:32:55.147837: step 14210, loss = 0.86 (267.3 examples/sec; 0.479 sec/batch)
2018-03-22 13:33:00.411488: step 14220, loss = 0.83 (243.2 examples/sec; 0.526 sec/batch)
2018-03-22 13:33:05.499417: step 14230, loss = 0.86 (251.6 examples/sec; 0.509 sec/batch)
2018-03-22 13:33:10.446620: step 14240, loss = 0.84 (258.7 examples/sec; 0.495 sec/batch)
2018-03-22 13:33:15.140556: step 14250, loss = 0.98 (272.7 examples/sec; 0.469 sec/batch)
2018-03-22 13:33:19.887425: step 14260, loss = 0.78 (269.7 examples/sec; 0.475 sec/batch)
2018-03-22 13:33:25.108254: step 14270, loss = 0.78 (245.2 examples/sec; 0.522 sec/batch)
2018-03-22 13:33:30.261347: step 14280, loss = 0.91 (248.4 examples/sec; 0.515 sec/batch)
2018-03-22 13:33:35.120521: step 14290, loss = 0.75 (263.4 examples/sec; 0.486 sec/batch)
2018-03-22 13:33:40.366331: step 14300, loss = 0.76 (244.0 examples/sec; 0.525 sec/batch)
2018-03-22 13:33:45.666333: step 14310, loss = 0.84 (241.5 examples/sec; 0.530 sec/batch)
2018-03-22 13:33:51.168307: step 14320, loss = 0.92 (232.6 examples/sec; 0.550 sec/batch)
2018-03-22 13:33:55.988472: step 14330, loss = 0.85 (265.6 examples/sec; 0.482 sec/batch)
2018-03-22 13:34:00.910391: step 14340, loss = 0.87 (260.1 examples/sec; 0.492 sec/batch)
2018-03-22 13:34:06.037867: step 14350, loss = 0.90 (249.6 examples/sec; 0.513 sec/batch)
2018-03-22 13:34:10.547705: step 14360, loss = 0.71 (283.8 examples/sec; 0.451 sec/batch)
2018-03-22 13:34:15.267889: step 14370, loss = 0.83 (271.2 examples/sec; 0.472 sec/batch)
2018-03-22 13:34:20.188812: step 14380, loss = 0.79 (260.1 examples/sec; 0.492 sec/batch)
2018-03-22 13:34:25.855138: step 14390, loss = 0.65 (225.9 examples/sec; 0.567 sec/batch)
2018-03-22 13:34:30.897815: step 14400, loss = 0.92 (253.8 examples/sec; 0.504 sec/batch)
2018-03-22 13:34:35.595275: step 14410, loss = 0.90 (272.5 examples/sec; 0.470 sec/batch)
2018-03-22 13:34:40.982235: step 14420, loss = 0.89 (237.6 examples/sec; 0.539 sec/batch)
2018-03-22 13:34:45.838916: step 14430, loss = 1.01 (263.6 examples/sec; 0.486 sec/batch)
2018-03-22 13:34:50.877300: step 14440, loss = 1.02 (254.0 examples/sec; 0.504 sec/batch)
2018-03-22 13:34:55.452250: step 14450, loss = 0.80 (279.8 examples/sec; 0.457 sec/batch)
2018-03-22 13:35:00.705415: step 14460, loss = 0.84 (243.7 examples/sec; 0.525 sec/batch)
2018-03-22 13:35:05.711432: step 14470, loss = 0.89 (255.7 examples/sec; 0.501 sec/batch)
2018-03-22 13:35:11.017268: step 14480, loss = 0.69 (241.2 examples/sec; 0.531 sec/batch)
2018-03-22 13:35:15.531563: step 14490, loss = 0.88 (283.5 examples/sec; 0.451 sec/batch)
2018-03-22 13:35:20.700888: step 14500, loss = 0.88 (247.6 examples/sec; 0.517 sec/batch)
2018-03-22 13:35:25.845394: step 14510, loss = 0.91 (248.8 examples/sec; 0.514 sec/batch)
2018-03-22 13:35:30.922405: step 14520, loss = 0.73 (252.1 examples/sec; 0.508 sec/batch)
2018-03-22 13:35:35.560181: step 14530, loss = 0.88 (276.0 examples/sec; 0.464 sec/batch)
2018-03-22 13:35:40.800182: step 14540, loss = 0.88 (244.3 examples/sec; 0.524 sec/batch)
2018-03-22 13:35:45.874339: step 14550, loss = 0.87 (252.3 examples/sec; 0.507 sec/batch)
2018-03-22 13:35:50.995281: step 14560, loss = 1.02 (250.0 examples/sec; 0.512 sec/batch)
2018-03-22 13:35:56.164359: step 14570, loss = 0.81 (247.6 examples/sec; 0.517 sec/batch)
2018-03-22 13:36:00.890116: step 14580, loss = 0.96 (270.9 examples/sec; 0.473 sec/batch)
2018-03-22 13:36:06.018523: step 14590, loss = 0.76 (249.6 examples/sec; 0.513 sec/batch)
2018-03-22 13:36:11.292753: step 14600, loss = 0.77 (242.7 examples/sec; 0.527 sec/batch)
2018-03-22 13:36:16.544768: step 14610, loss = 0.77 (243.7 examples/sec; 0.525 sec/batch)
2018-03-22 13:36:21.644447: step 14620, loss = 0.87 (251.0 examples/sec; 0.510 sec/batch)
2018-03-22 13:36:25.902820: step 14630, loss = 0.78 (300.6 examples/sec; 0.426 sec/batch)
2018-03-22 13:36:30.711367: step 14640, loss = 0.81 (266.2 examples/sec; 0.481 sec/batch)
2018-03-22 13:36:34.676328: step 14650, loss = 0.94 (322.8 examples/sec; 0.396 sec/batch)
2018-03-22 13:36:39.842447: step 14660, loss = 0.90 (247.8 examples/sec; 0.517 sec/batch)
2018-03-22 13:36:45.195431: step 14670, loss = 0.91 (239.1 examples/sec; 0.535 sec/batch)
2018-03-22 13:36:50.675197: step 14680, loss = 0.95 (233.6 examples/sec; 0.548 sec/batch)
2018-03-22 13:36:56.161022: step 14690, loss = 0.83 (233.3 examples/sec; 0.549 sec/batch)
2018-03-22 13:37:01.242939: step 14700, loss = 0.85 (251.9 examples/sec; 0.508 sec/batch)
2018-03-22 13:37:06.148872: step 14710, loss = 0.89 (260.9 examples/sec; 0.491 sec/batch)
2018-03-22 13:37:11.331933: step 14720, loss = 0.71 (247.0 examples/sec; 0.518 sec/batch)
2018-03-22 13:37:16.317324: step 14730, loss = 0.88 (256.8 examples/sec; 0.499 sec/batch)
2018-03-22 13:37:21.661387: step 14740, loss = 0.88 (239.5 examples/sec; 0.534 sec/batch)
2018-03-22 13:37:26.801464: step 14750, loss = 0.83 (249.0 examples/sec; 0.514 sec/batch)
2018-03-22 13:37:31.910953: step 14760, loss = 0.96 (250.5 examples/sec; 0.511 sec/batch)
2018-03-22 13:37:36.858403: step 14770, loss = 0.91 (258.7 examples/sec; 0.495 sec/batch)
2018-03-22 13:37:41.692463: step 14780, loss = 1.08 (264.8 examples/sec; 0.483 sec/batch)
2018-03-22 13:37:47.042329: step 14790, loss = 0.72 (239.3 examples/sec; 0.535 sec/batch)
2018-03-22 13:37:52.296363: step 14800, loss = 0.70 (243.6 examples/sec; 0.525 sec/batch)
2018-03-22 13:37:57.003239: step 14810, loss = 0.98 (271.9 examples/sec; 0.471 sec/batch)
2018-03-22 13:38:01.940143: step 14820, loss = 0.85 (259.3 examples/sec; 0.494 sec/batch)
2018-03-22 13:38:07.191053: step 14830, loss = 0.80 (243.8 examples/sec; 0.525 sec/batch)
2018-03-22 13:38:12.854005: step 14840, loss = 0.85 (226.0 examples/sec; 0.566 sec/batch)
2018-03-22 13:38:17.444457: step 14850, loss = 0.99 (278.8 examples/sec; 0.459 sec/batch)
2018-03-22 13:38:22.373330: step 14860, loss = 0.76 (259.7 examples/sec; 0.493 sec/batch)
2018-03-22 13:38:27.347354: step 14870, loss = 0.83 (257.3 examples/sec; 0.497 sec/batch)
2018-03-22 13:38:32.654429: step 14880, loss = 0.93 (241.2 examples/sec; 0.531 sec/batch)
2018-03-22 13:38:38.055009: step 14890, loss = 0.80 (237.0 examples/sec; 0.540 sec/batch)
2018-03-22 13:38:43.504275: step 14900, loss = 0.72 (234.9 examples/sec; 0.545 sec/batch)
2018-03-22 13:38:48.412473: step 14910, loss = 0.93 (260.8 examples/sec; 0.491 sec/batch)
2018-03-22 13:38:53.373301: step 14920, loss = 0.84 (258.0 examples/sec; 0.496 sec/batch)
2018-03-22 13:38:58.306380: step 14930, loss = 0.87 (259.5 examples/sec; 0.493 sec/batch)
2018-03-22 13:39:03.554939: step 14940, loss = 0.84 (243.9 examples/sec; 0.525 sec/batch)
2018-03-22 13:39:08.034392: step 14950, loss = 0.92 (285.7 examples/sec; 0.448 sec/batch)
2018-03-22 13:39:12.926692: step 14960, loss = 0.98 (261.6 examples/sec; 0.489 sec/batch)
2018-03-22 13:39:18.178507: step 14970, loss = 0.89 (243.7 examples/sec; 0.525 sec/batch)
2018-03-22 13:39:23.577486: step 14980, loss = 0.64 (237.1 examples/sec; 0.540 sec/batch)
2018-03-22 13:39:28.425835: step 14990, loss = 0.69 (264.0 examples/sec; 0.485 sec/batch)
2018-03-22 13:39:33.748653: step 15000, loss = 0.84 (240.5 examples/sec; 0.532 sec/batch)
2018-03-22 13:39:39.256355: step 15010, loss = 0.84 (232.4 examples/sec; 0.551 sec/batch)
2018-03-22 13:39:44.917190: step 15020, loss = 0.76 (226.1 examples/sec; 0.566 sec/batch)
2018-03-22 13:39:50.270236: step 15030, loss = 0.84 (239.1 examples/sec; 0.535 sec/batch)
2018-03-22 13:39:55.504446: step 15040, loss = 0.87 (244.5 examples/sec; 0.523 sec/batch)
2018-03-22 13:40:00.575371: step 15050, loss = 0.76 (252.4 examples/sec; 0.507 sec/batch)
2018-03-22 13:40:05.677383: step 15060, loss = 0.90 (250.9 examples/sec; 0.510 sec/batch)
2018-03-22 13:40:10.431462: step 15070, loss = 0.89 (269.2 examples/sec; 0.475 sec/batch)
2018-03-22 13:40:14.452485: step 15080, loss = 0.83 (318.3 examples/sec; 0.402 sec/batch)
2018-03-22 13:40:19.031926: step 15090, loss = 0.70 (279.5 examples/sec; 0.458 sec/batch)
2018-03-22 13:40:24.094675: step 15100, loss = 0.77 (252.8 examples/sec; 0.506 sec/batch)
2018-03-22 13:40:28.465090: step 15110, loss = 0.84 (292.9 examples/sec; 0.437 sec/batch)
2018-03-22 13:40:33.464384: step 15120, loss = 1.04 (256.0 examples/sec; 0.500 sec/batch)
2018-03-22 13:40:38.609878: step 15130, loss = 0.93 (248.8 examples/sec; 0.515 sec/batch)
2018-03-22 13:40:43.745349: step 15140, loss = 0.94 (249.2 examples/sec; 0.514 sec/batch)
2018-03-22 13:40:48.232442: step 15150, loss = 0.82 (285.3 examples/sec; 0.449 sec/batch)
2018-03-22 13:40:53.080188: step 15160, loss = 0.85 (264.0 examples/sec; 0.485 sec/batch)
2018-03-22 13:40:57.811342: step 15170, loss = 1.09 (270.5 examples/sec; 0.473 sec/batch)
2018-03-22 13:41:03.160380: step 15180, loss = 0.91 (239.3 examples/sec; 0.535 sec/batch)
2018-03-22 13:41:08.274467: step 15190, loss = 1.01 (250.3 examples/sec; 0.511 sec/batch)
2018-03-22 13:41:13.471280: step 15200, loss = 0.80 (246.3 examples/sec; 0.520 sec/batch)
2018-03-22 13:41:18.942421: step 15210, loss = 0.84 (234.0 examples/sec; 0.547 sec/batch)
2018-03-22 13:41:24.039857: step 15220, loss = 0.80 (251.1 examples/sec; 0.510 sec/batch)
2018-03-22 13:41:29.464290: step 15230, loss = 0.90 (236.0 examples/sec; 0.542 sec/batch)
2018-03-22 13:41:34.325541: step 15240, loss = 0.92 (263.3 examples/sec; 0.486 sec/batch)
2018-03-22 13:41:39.704330: step 15250, loss = 0.61 (238.0 examples/sec; 0.538 sec/batch)
2018-03-22 13:41:45.160416: step 15260, loss = 0.92 (234.6 examples/sec; 0.546 sec/batch)
2018-03-22 13:41:49.801510: step 15270, loss = 0.80 (275.8 examples/sec; 0.464 sec/batch)
2018-03-22 13:41:54.475175: step 15280, loss = 0.86 (273.9 examples/sec; 0.467 sec/batch)
2018-03-22 13:42:00.243304: step 15290, loss = 0.93 (221.9 examples/sec; 0.577 sec/batch)
2018-03-22 13:42:05.831072: step 15300, loss = 0.98 (229.1 examples/sec; 0.559 sec/batch)
2018-03-22 13:42:11.308964: step 15310, loss = 0.92 (233.7 examples/sec; 0.548 sec/batch)
2018-03-22 13:42:16.393407: step 15320, loss = 0.79 (251.7 examples/sec; 0.508 sec/batch)
2018-03-22 13:42:21.014427: step 15330, loss = 0.97 (277.0 examples/sec; 0.462 sec/batch)
2018-03-22 13:42:26.206605: step 15340, loss = 0.78 (246.5 examples/sec; 0.519 sec/batch)
2018-03-22 13:42:31.052405: step 15350, loss = 0.80 (264.1 examples/sec; 0.485 sec/batch)
2018-03-22 13:42:36.032859: step 15360, loss = 0.83 (257.0 examples/sec; 0.498 sec/batch)
2018-03-22 13:42:41.062332: step 15370, loss = 0.91 (254.5 examples/sec; 0.503 sec/batch)
2018-03-22 13:42:45.958422: step 15380, loss = 0.92 (261.4 examples/sec; 0.490 sec/batch)
2018-03-22 13:42:50.758500: step 15390, loss = 0.71 (266.7 examples/sec; 0.480 sec/batch)
2018-03-22 13:42:55.828929: step 15400, loss = 0.68 (252.4 examples/sec; 0.507 sec/batch)
2018-03-22 13:43:00.753230: step 15410, loss = 0.78 (259.9 examples/sec; 0.492 sec/batch)
2018-03-22 13:43:05.148448: step 15420, loss = 0.91 (291.2 examples/sec; 0.440 sec/batch)
2018-03-22 13:43:10.292598: step 15430, loss = 0.79 (248.8 examples/sec; 0.514 sec/batch)
2018-03-22 13:43:15.008474: step 15440, loss = 0.88 (271.4 examples/sec; 0.472 sec/batch)
2018-03-22 13:43:19.866426: step 15450, loss = 0.79 (263.5 examples/sec; 0.486 sec/batch)
2018-03-22 13:43:24.973504: step 15460, loss = 0.89 (250.6 examples/sec; 0.511 sec/batch)
2018-03-22 13:43:30.259028: step 15470, loss = 0.85 (242.2 examples/sec; 0.529 sec/batch)
2018-03-22 13:43:35.214392: step 15480, loss = 0.85 (258.3 examples/sec; 0.496 sec/batch)
2018-03-22 13:43:40.198467: step 15490, loss = 0.91 (256.8 examples/sec; 0.498 sec/batch)
2018-03-22 13:43:45.498380: step 15500, loss = 0.81 (241.5 examples/sec; 0.530 sec/batch)
2018-03-22 13:43:50.935367: step 15510, loss = 0.83 (235.4 examples/sec; 0.544 sec/batch)
2018-03-22 13:43:56.081441: step 15520, loss = 0.95 (248.7 examples/sec; 0.515 sec/batch)
2018-03-22 13:44:00.948375: step 15530, loss = 0.72 (263.0 examples/sec; 0.487 sec/batch)
2018-03-22 13:44:05.903480: step 15540, loss = 0.92 (258.3 examples/sec; 0.496 sec/batch)
2018-03-22 13:44:10.685361: step 15550, loss = 0.74 (267.7 examples/sec; 0.478 sec/batch)
2018-03-22 13:44:15.216301: step 15560, loss = 0.78 (282.5 examples/sec; 0.453 sec/batch)
2018-03-22 13:44:19.633795: step 15570, loss = 0.70 (289.8 examples/sec; 0.442 sec/batch)
2018-03-22 13:44:24.850825: step 15580, loss = 0.74 (245.4 examples/sec; 0.522 sec/batch)
2018-03-22 13:44:29.625388: step 15590, loss = 0.69 (268.1 examples/sec; 0.477 sec/batch)
2018-03-22 13:44:34.903346: step 15600, loss = 0.88 (242.5 examples/sec; 0.528 sec/batch)
2018-03-22 13:44:40.451304: step 15610, loss = 0.97 (230.7 examples/sec; 0.555 sec/batch)
2018-03-22 13:44:44.993483: step 15620, loss = 0.88 (281.8 examples/sec; 0.454 sec/batch)
2018-03-22 13:44:49.835421: step 15630, loss = 0.78 (264.4 examples/sec; 0.484 sec/batch)
2018-03-22 13:44:55.053418: step 15640, loss = 0.92 (245.3 examples/sec; 0.522 sec/batch)
2018-03-22 13:44:59.820398: step 15650, loss = 0.74 (268.5 examples/sec; 0.477 sec/batch)
2018-03-22 13:45:05.164378: step 15660, loss = 0.76 (239.5 examples/sec; 0.534 sec/batch)
2018-03-22 13:45:10.373310: step 15670, loss = 0.85 (245.7 examples/sec; 0.521 sec/batch)
2018-03-22 13:45:15.356326: step 15680, loss = 0.95 (256.9 examples/sec; 0.498 sec/batch)
2018-03-22 13:45:20.397722: step 15690, loss = 0.79 (253.9 examples/sec; 0.504 sec/batch)
2018-03-22 13:45:25.401662: step 15700, loss = 0.78 (255.8 examples/sec; 0.500 sec/batch)
2018-03-22 13:45:31.047431: step 15710, loss = 0.82 (226.7 examples/sec; 0.565 sec/batch)
2018-03-22 13:45:36.222330: step 15720, loss = 0.86 (247.3 examples/sec; 0.517 sec/batch)
2018-03-22 13:45:41.268967: step 15730, loss = 0.98 (253.6 examples/sec; 0.505 sec/batch)
2018-03-22 13:45:45.656401: step 15740, loss = 0.87 (291.7 examples/sec; 0.439 sec/batch)
2018-03-22 13:45:51.598367: step 15750, loss = 0.82 (215.4 examples/sec; 0.594 sec/batch)
2018-03-22 13:45:56.858376: step 15760, loss = 0.86 (243.3 examples/sec; 0.526 sec/batch)
2018-03-22 13:46:02.099406: step 15770, loss = 0.90 (244.2 examples/sec; 0.524 sec/batch)
2018-03-22 13:46:07.009577: step 15780, loss = 0.76 (260.7 examples/sec; 0.491 sec/batch)
2018-03-22 13:46:12.344451: step 15790, loss = 0.90 (239.9 examples/sec; 0.533 sec/batch)
2018-03-22 13:46:17.587754: step 15800, loss = 0.79 (244.1 examples/sec; 0.524 sec/batch)
2018-03-22 13:46:22.883835: step 15810, loss = 0.88 (241.7 examples/sec; 0.530 sec/batch)
2018-03-22 13:46:28.130213: step 15820, loss = 0.99 (244.0 examples/sec; 0.525 sec/batch)
2018-03-22 13:46:33.680427: step 15830, loss = 0.88 (230.6 examples/sec; 0.555 sec/batch)
2018-03-22 13:46:38.552425: step 15840, loss = 0.75 (262.7 examples/sec; 0.487 sec/batch)
2018-03-22 13:46:43.031968: step 15850, loss = 0.88 (285.7 examples/sec; 0.448 sec/batch)
2018-03-22 13:46:48.037910: step 15860, loss = 0.71 (255.7 examples/sec; 0.501 sec/batch)
2018-03-22 13:46:52.459851: step 15870, loss = 0.82 (289.5 examples/sec; 0.442 sec/batch)
2018-03-22 13:46:57.588011: step 15880, loss = 0.79 (249.6 examples/sec; 0.513 sec/batch)
2018-03-22 13:47:02.995278: step 15890, loss = 0.83 (236.7 examples/sec; 0.541 sec/batch)
2018-03-22 13:47:08.340856: step 15900, loss = 0.85 (239.5 examples/sec; 0.535 sec/batch)
2018-03-22 13:47:13.857395: step 15910, loss = 0.86 (232.0 examples/sec; 0.552 sec/batch)
2018-03-22 13:47:18.854166: step 15920, loss = 0.78 (256.2 examples/sec; 0.500 sec/batch)
2018-03-22 13:47:23.502405: step 15930, loss = 0.83 (275.4 examples/sec; 0.465 sec/batch)
2018-03-22 13:47:28.793438: step 15940, loss = 0.83 (241.9 examples/sec; 0.529 sec/batch)
2018-03-22 13:47:33.900791: step 15950, loss = 1.00 (250.6 examples/sec; 0.511 sec/batch)
2018-03-22 13:47:39.011759: step 15960, loss = 0.79 (250.4 examples/sec; 0.511 sec/batch)
2018-03-22 13:47:44.435943: step 15970, loss = 0.82 (236.0 examples/sec; 0.542 sec/batch)
2018-03-22 13:47:49.311334: step 15980, loss = 0.74 (262.5 examples/sec; 0.488 sec/batch)
2018-03-22 13:47:53.611822: step 15990, loss = 0.87 (297.6 examples/sec; 0.430 sec/batch)
2018-03-22 13:47:59.170223: step 16000, loss = 0.75 (230.3 examples/sec; 0.556 sec/batch)
2018-03-22 13:48:03.910269: step 16010, loss = 0.94 (270.0 examples/sec; 0.474 sec/batch)
2018-03-22 13:48:08.830405: step 16020, loss = 0.78 (260.2 examples/sec; 0.492 sec/batch)
2018-03-22 13:48:13.699424: step 16030, loss = 0.78 (262.9 examples/sec; 0.487 sec/batch)
2018-03-22 13:48:18.635655: step 16040, loss = 0.86 (259.3 examples/sec; 0.494 sec/batch)
2018-03-22 13:48:23.654343: step 16050, loss = 0.90 (255.0 examples/sec; 0.502 sec/batch)
2018-03-22 13:48:28.816347: step 16060, loss = 0.72 (248.0 examples/sec; 0.516 sec/batch)
2018-03-22 13:48:34.090079: step 16070, loss = 0.75 (242.7 examples/sec; 0.527 sec/batch)
2018-03-22 13:48:39.201452: step 16080, loss = 0.82 (250.4 examples/sec; 0.511 sec/batch)
2018-03-22 13:48:44.324380: step 16090, loss = 0.82 (249.9 examples/sec; 0.512 sec/batch)
2018-03-22 13:48:49.628225: step 16100, loss = 0.79 (241.3 examples/sec; 0.530 sec/batch)
2018-03-22 13:48:54.638305: step 16110, loss = 0.79 (255.5 examples/sec; 0.501 sec/batch)
2018-03-22 13:49:00.080429: step 16120, loss = 0.92 (235.2 examples/sec; 0.544 sec/batch)
2018-03-22 13:49:05.166400: step 16130, loss = 0.83 (251.7 examples/sec; 0.509 sec/batch)
2018-03-22 13:49:10.232677: step 16140, loss = 0.93 (252.7 examples/sec; 0.507 sec/batch)
2018-03-22 13:49:15.440439: step 16150, loss = 0.86 (245.8 examples/sec; 0.521 sec/batch)
2018-03-22 13:49:20.258659: step 16160, loss = 0.75 (265.7 examples/sec; 0.482 sec/batch)
2018-03-22 13:49:24.787419: step 16170, loss = 0.80 (282.6 examples/sec; 0.453 sec/batch)
2018-03-22 13:49:30.183620: step 16180, loss = 0.69 (237.2 examples/sec; 0.540 sec/batch)
2018-03-22 13:49:35.067315: step 16190, loss = 0.82 (262.1 examples/sec; 0.488 sec/batch)
2018-03-22 13:49:40.499172: step 16200, loss = 0.81 (235.6 examples/sec; 0.543 sec/batch)
2018-03-22 13:49:45.467416: step 16210, loss = 0.88 (257.6 examples/sec; 0.497 sec/batch)
2018-03-22 13:49:50.421437: step 16220, loss = 0.81 (258.4 examples/sec; 0.495 sec/batch)
2018-03-22 13:49:55.726440: step 16230, loss = 0.89 (241.3 examples/sec; 0.530 sec/batch)
2018-03-22 13:50:00.990393: step 16240, loss = 0.62 (243.2 examples/sec; 0.526 sec/batch)
2018-03-22 13:50:05.946279: step 16250, loss = 0.88 (258.3 examples/sec; 0.496 sec/batch)
2018-03-22 13:50:12.104443: step 16260, loss = 0.89 (207.9 examples/sec; 0.616 sec/batch)
2018-03-22 13:50:16.915013: step 16270, loss = 0.98 (266.1 examples/sec; 0.481 sec/batch)
2018-03-22 13:50:21.777371: step 16280, loss = 0.87 (263.2 examples/sec; 0.486 sec/batch)
2018-03-22 13:50:26.533928: step 16290, loss = 0.78 (269.1 examples/sec; 0.476 sec/batch)
2018-03-22 13:50:31.866156: step 16300, loss = 0.66 (240.0 examples/sec; 0.533 sec/batch)
2018-03-22 13:50:36.914413: step 16310, loss = 0.82 (253.6 examples/sec; 0.505 sec/batch)
2018-03-22 13:50:41.527868: step 16320, loss = 0.86 (277.4 examples/sec; 0.461 sec/batch)
2018-03-22 13:50:46.212389: step 16330, loss = 0.74 (273.2 examples/sec; 0.468 sec/batch)
2018-03-22 13:50:51.303620: step 16340, loss = 0.80 (251.4 examples/sec; 0.509 sec/batch)
2018-03-22 13:50:56.619320: step 16350, loss = 1.08 (240.8 examples/sec; 0.532 sec/batch)
2018-03-22 13:51:01.720520: step 16360, loss = 0.87 (250.9 examples/sec; 0.510 sec/batch)
2018-03-22 13:51:06.641361: step 16370, loss = 0.78 (260.1 examples/sec; 0.492 sec/batch)
2018-03-22 13:51:12.072400: step 16380, loss = 0.80 (235.7 examples/sec; 0.543 sec/batch)
2018-03-22 13:51:17.481231: step 16390, loss = 0.96 (236.7 examples/sec; 0.541 sec/batch)
2018-03-22 13:51:22.941144: step 16400, loss = 0.82 (234.4 examples/sec; 0.546 sec/batch)
2018-03-22 13:51:28.068011: step 16410, loss = 0.93 (249.7 examples/sec; 0.513 sec/batch)
2018-03-22 13:51:32.769392: step 16420, loss = 0.65 (272.3 examples/sec; 0.470 sec/batch)
2018-03-22 13:51:37.587376: step 16430, loss = 0.83 (265.7 examples/sec; 0.482 sec/batch)
2018-03-22 13:51:42.818388: step 16440, loss = 0.80 (244.7 examples/sec; 0.523 sec/batch)
2018-03-22 13:51:47.667022: step 16450, loss = 0.89 (264.0 examples/sec; 0.485 sec/batch)
2018-03-22 13:51:52.318452: step 16460, loss = 0.83 (275.2 examples/sec; 0.465 sec/batch)
2018-03-22 13:51:56.695395: step 16470, loss = 0.89 (292.4 examples/sec; 0.438 sec/batch)
2018-03-22 13:52:01.825986: step 16480, loss = 0.68 (249.5 examples/sec; 0.513 sec/batch)
2018-03-22 13:52:06.993229: step 16490, loss = 0.81 (247.7 examples/sec; 0.517 sec/batch)
2018-03-22 13:52:12.155763: step 16500, loss = 0.82 (247.9 examples/sec; 0.516 sec/batch)
2018-03-22 13:52:17.344784: step 16510, loss = 0.81 (246.7 examples/sec; 0.519 sec/batch)
2018-03-22 13:52:22.218483: step 16520, loss = 0.83 (262.6 examples/sec; 0.487 sec/batch)
2018-03-22 13:52:27.165413: step 16530, loss = 0.85 (258.7 examples/sec; 0.495 sec/batch)
2018-03-22 13:52:32.378410: step 16540, loss = 1.03 (245.5 examples/sec; 0.521 sec/batch)
2018-03-22 13:52:38.116375: step 16550, loss = 0.80 (223.1 examples/sec; 0.574 sec/batch)
2018-03-22 13:52:43.511437: step 16560, loss = 1.09 (237.3 examples/sec; 0.540 sec/batch)
2018-03-22 13:52:47.676439: step 16570, loss = 0.90 (307.3 examples/sec; 0.417 sec/batch)
2018-03-22 13:52:52.668890: step 16580, loss = 0.88 (256.4 examples/sec; 0.499 sec/batch)
2018-03-22 13:52:58.031436: step 16590, loss = 1.07 (238.7 examples/sec; 0.536 sec/batch)
2018-03-22 13:53:04.079456: step 16600, loss = 0.78 (211.6 examples/sec; 0.605 sec/batch)
2018-03-22 13:53:09.038156: step 16610, loss = 0.69 (258.1 examples/sec; 0.496 sec/batch)
2018-03-22 13:53:13.727491: step 16620, loss = 0.83 (273.0 examples/sec; 0.469 sec/batch)
2018-03-22 13:53:18.417406: step 16630, loss = 0.97 (272.9 examples/sec; 0.469 sec/batch)
2018-03-22 13:53:23.466426: step 16640, loss = 0.94 (253.5 examples/sec; 0.505 sec/batch)
2018-03-22 13:53:28.431394: step 16650, loss = 0.74 (257.8 examples/sec; 0.496 sec/batch)
2018-03-22 13:53:33.304325: step 16660, loss = 0.78 (262.7 examples/sec; 0.487 sec/batch)
2018-03-22 13:53:38.492353: step 16670, loss = 0.90 (246.7 examples/sec; 0.519 sec/batch)
2018-03-22 13:53:43.791424: step 16680, loss = 0.88 (241.6 examples/sec; 0.530 sec/batch)
2018-03-22 13:53:48.943418: step 16690, loss = 0.98 (248.4 examples/sec; 0.515 sec/batch)
2018-03-22 13:53:54.368837: step 16700, loss = 0.81 (235.9 examples/sec; 0.543 sec/batch)
2018-03-22 13:53:59.959074: step 16710, loss = 0.74 (229.0 examples/sec; 0.559 sec/batch)
2018-03-22 13:54:04.851431: step 16720, loss = 0.84 (261.6 examples/sec; 0.489 sec/batch)
2018-03-22 13:54:09.648444: step 16730, loss = 0.89 (266.8 examples/sec; 0.480 sec/batch)
2018-03-22 13:54:14.334385: step 16740, loss = 0.97 (273.2 examples/sec; 0.469 sec/batch)
2018-03-22 13:54:18.634394: step 16750, loss = 0.85 (297.7 examples/sec; 0.430 sec/batch)
2018-03-22 13:54:23.969793: step 16760, loss = 0.89 (239.9 examples/sec; 0.534 sec/batch)
2018-03-22 13:54:28.699816: step 16770, loss = 0.72 (270.6 examples/sec; 0.473 sec/batch)
2018-03-22 13:54:33.485373: step 16780, loss = 0.73 (267.5 examples/sec; 0.479 sec/batch)
2018-03-22 13:54:38.198235: step 16790, loss = 0.74 (271.6 examples/sec; 0.471 sec/batch)
2018-03-22 13:54:43.533437: step 16800, loss = 0.86 (239.9 examples/sec; 0.534 sec/batch)
2018-03-22 13:54:48.650309: step 16810, loss = 0.83 (250.2 examples/sec; 0.512 sec/batch)
2018-03-22 13:54:53.834411: step 16820, loss = 0.86 (246.9 examples/sec; 0.518 sec/batch)
2018-03-22 13:54:58.787000: step 16830, loss = 0.74 (258.5 examples/sec; 0.495 sec/batch)
2018-03-22 13:55:03.512410: step 16840, loss = 0.93 (270.9 examples/sec; 0.473 sec/batch)
2018-03-22 13:55:08.625125: step 16850, loss = 0.69 (250.4 examples/sec; 0.511 sec/batch)
2018-03-22 13:55:14.224356: step 16860, loss = 0.70 (228.6 examples/sec; 0.560 sec/batch)
2018-03-22 13:55:18.740409: step 16870, loss = 0.84 (283.4 examples/sec; 0.452 sec/batch)
2018-03-22 13:55:23.696324: step 16880, loss = 0.78 (258.3 examples/sec; 0.496 sec/batch)
2018-03-22 13:55:28.871415: step 16890, loss = 0.93 (247.3 examples/sec; 0.518 sec/batch)
2018-03-22 13:55:34.199128: step 16900, loss = 0.93 (240.3 examples/sec; 0.533 sec/batch)
2018-03-22 13:55:38.798803: step 16910, loss = 0.77 (278.3 examples/sec; 0.460 sec/batch)
2018-03-22 13:55:43.593401: step 16920, loss = 0.79 (267.0 examples/sec; 0.479 sec/batch)
2018-03-22 13:55:48.524548: step 16930, loss = 0.80 (259.6 examples/sec; 0.493 sec/batch)
2018-03-22 13:55:53.939486: step 16940, loss = 0.87 (236.4 examples/sec; 0.542 sec/batch)
2018-03-22 13:55:58.533762: step 16950, loss = 0.64 (278.6 examples/sec; 0.459 sec/batch)
2018-03-22 13:56:03.329355: step 16960, loss = 0.76 (266.9 examples/sec; 0.480 sec/batch)
2018-03-22 13:56:08.320546: step 16970, loss = 0.96 (256.5 examples/sec; 0.499 sec/batch)
2018-03-22 13:56:13.122863: step 16980, loss = 0.76 (266.5 examples/sec; 0.480 sec/batch)
2018-03-22 13:56:18.207640: step 16990, loss = 0.84 (251.7 examples/sec; 0.508 sec/batch)
2018-03-22 13:56:23.601582: step 17000, loss = 0.80 (237.3 examples/sec; 0.539 sec/batch)
2018-03-22 13:56:28.625709: step 17010, loss = 0.92 (254.8 examples/sec; 0.502 sec/batch)
2018-03-22 13:56:33.972425: step 17020, loss = 0.77 (239.4 examples/sec; 0.535 sec/batch)
2018-03-22 13:56:38.473358: step 17030, loss = 0.89 (284.4 examples/sec; 0.450 sec/batch)
2018-03-22 13:56:43.278508: step 17040, loss = 0.92 (266.4 examples/sec; 0.481 sec/batch)
2018-03-22 13:56:48.084389: step 17050, loss = 0.86 (266.3 examples/sec; 0.481 sec/batch)
2018-03-22 13:56:53.712068: step 17060, loss = 0.82 (227.4 examples/sec; 0.563 sec/batch)
2018-03-22 13:56:58.331300: step 17070, loss = 0.83 (277.1 examples/sec; 0.462 sec/batch)
2018-03-22 13:57:03.233140: step 17080, loss = 0.77 (261.1 examples/sec; 0.490 sec/batch)
2018-03-22 13:57:08.137908: step 17090, loss = 0.91 (261.0 examples/sec; 0.490 sec/batch)
2018-03-22 13:57:13.920246: step 17100, loss = 0.77 (221.4 examples/sec; 0.578 sec/batch)
2018-03-22 13:57:18.726477: step 17110, loss = 0.74 (266.3 examples/sec; 0.481 sec/batch)
2018-03-22 13:57:23.992434: step 17120, loss = 0.82 (243.1 examples/sec; 0.527 sec/batch)
2018-03-22 13:57:28.909367: step 17130, loss = 1.06 (260.3 examples/sec; 0.492 sec/batch)
2018-03-22 13:57:34.520678: step 17140, loss = 0.77 (228.1 examples/sec; 0.561 sec/batch)
2018-03-22 13:57:39.713427: step 17150, loss = 0.73 (246.5 examples/sec; 0.519 sec/batch)
2018-03-22 13:57:44.933372: step 17160, loss = 0.85 (245.2 examples/sec; 0.522 sec/batch)
2018-03-22 13:57:50.502407: step 17170, loss = 0.82 (229.8 examples/sec; 0.557 sec/batch)
2018-03-22 13:57:55.346375: step 17180, loss = 0.75 (264.2 examples/sec; 0.484 sec/batch)
2018-03-22 13:58:00.168420: step 17190, loss = 0.84 (265.4 examples/sec; 0.482 sec/batch)
2018-03-22 13:58:04.948622: step 17200, loss = 0.89 (267.8 examples/sec; 0.478 sec/batch)
2018-03-22 13:58:09.769405: step 17210, loss = 0.90 (265.5 examples/sec; 0.482 sec/batch)
2018-03-22 13:58:14.532401: step 17220, loss = 0.78 (268.7 examples/sec; 0.476 sec/batch)
2018-03-22 13:58:19.266403: step 17230, loss = 0.86 (270.4 examples/sec; 0.473 sec/batch)
2018-03-22 13:58:24.125049: step 17240, loss = 0.87 (263.4 examples/sec; 0.486 sec/batch)
2018-03-22 13:58:29.883358: step 17250, loss = 0.69 (222.3 examples/sec; 0.576 sec/batch)
2018-03-22 13:58:35.051409: step 17260, loss = 0.89 (247.7 examples/sec; 0.517 sec/batch)
2018-03-22 13:58:40.262467: step 17270, loss = 0.86 (245.6 examples/sec; 0.521 sec/batch)
2018-03-22 13:58:44.919447: step 17280, loss = 0.87 (274.9 examples/sec; 0.466 sec/batch)
2018-03-22 13:58:50.082568: step 17290, loss = 0.94 (247.9 examples/sec; 0.516 sec/batch)
2018-03-22 13:58:54.684843: step 17300, loss = 0.84 (278.1 examples/sec; 0.460 sec/batch)
2018-03-22 13:58:59.860887: step 17310, loss = 0.97 (247.3 examples/sec; 0.518 sec/batch)
2018-03-22 13:59:05.054497: step 17320, loss = 0.91 (246.5 examples/sec; 0.519 sec/batch)
2018-03-22 13:59:09.789312: step 17330, loss = 0.77 (270.3 examples/sec; 0.473 sec/batch)
2018-03-22 13:59:14.574426: step 17340, loss = 0.79 (267.5 examples/sec; 0.479 sec/batch)
2018-03-22 13:59:20.052361: step 17350, loss = 0.75 (233.7 examples/sec; 0.548 sec/batch)
2018-03-22 13:59:25.375328: step 17360, loss = 0.86 (240.5 examples/sec; 0.532 sec/batch)
2018-03-22 13:59:30.612962: step 17370, loss = 1.07 (244.4 examples/sec; 0.524 sec/batch)
2018-03-22 13:59:35.228904: step 17380, loss = 0.87 (277.3 examples/sec; 0.462 sec/batch)
2018-03-22 13:59:40.051366: step 17390, loss = 0.90 (265.4 examples/sec; 0.482 sec/batch)
2018-03-22 13:59:45.532147: step 17400, loss = 0.81 (233.5 examples/sec; 0.548 sec/batch)
2018-03-22 13:59:50.443385: step 17410, loss = 1.02 (260.6 examples/sec; 0.491 sec/batch)
2018-03-22 13:59:55.338836: step 17420, loss = 0.90 (261.5 examples/sec; 0.490 sec/batch)
2018-03-22 14:00:00.586444: step 17430, loss = 1.05 (243.9 examples/sec; 0.525 sec/batch)
2018-03-22 14:00:05.209259: step 17440, loss = 0.91 (276.9 examples/sec; 0.462 sec/batch)
2018-03-22 14:00:10.622735: step 17450, loss = 0.91 (236.4 examples/sec; 0.541 sec/batch)
2018-03-22 14:00:16.048385: step 17460, loss = 0.90 (235.9 examples/sec; 0.543 sec/batch)
2018-03-22 14:00:21.410365: step 17470, loss = 0.74 (238.7 examples/sec; 0.536 sec/batch)
2018-03-22 14:00:26.208671: step 17480, loss = 0.86 (266.8 examples/sec; 0.480 sec/batch)
2018-03-22 14:00:31.266688: step 17490, loss = 0.76 (253.1 examples/sec; 0.506 sec/batch)
2018-03-22 14:00:36.277757: step 17500, loss = 1.08 (255.4 examples/sec; 0.501 sec/batch)
2018-03-22 14:00:41.390198: step 17510, loss = 1.10 (250.4 examples/sec; 0.511 sec/batch)
2018-03-22 14:00:46.625406: step 17520, loss = 0.89 (244.5 examples/sec; 0.524 sec/batch)
2018-03-22 14:00:51.464386: step 17530, loss = 0.84 (264.5 examples/sec; 0.484 sec/batch)
2018-03-22 14:00:56.353434: step 17540, loss = 0.85 (261.8 examples/sec; 0.489 sec/batch)
2018-03-22 14:01:00.909816: step 17550, loss = 0.86 (280.9 examples/sec; 0.456 sec/batch)
2018-03-22 14:01:05.921395: step 17560, loss = 1.02 (255.4 examples/sec; 0.501 sec/batch)
2018-03-22 14:01:11.288642: step 17570, loss = 0.86 (238.5 examples/sec; 0.537 sec/batch)
2018-03-22 14:01:16.739334: step 17580, loss = 0.82 (234.8 examples/sec; 0.545 sec/batch)
2018-03-22 14:01:21.475409: step 17590, loss = 0.80 (270.3 examples/sec; 0.474 sec/batch)
2018-03-22 14:01:27.104661: step 17600, loss = 0.88 (227.4 examples/sec; 0.563 sec/batch)
2018-03-22 14:01:32.464415: step 17610, loss = 0.77 (238.8 examples/sec; 0.536 sec/batch)
2018-03-22 14:01:37.758387: step 17620, loss = 0.71 (241.8 examples/sec; 0.529 sec/batch)
2018-03-22 14:01:42.824493: step 17630, loss = 0.69 (252.7 examples/sec; 0.507 sec/batch)
2018-03-22 14:01:48.031435: step 17640, loss = 0.88 (245.8 examples/sec; 0.521 sec/batch)
2018-03-22 14:01:53.414715: step 17650, loss = 0.90 (237.8 examples/sec; 0.538 sec/batch)
2018-03-22 14:01:57.886335: step 17660, loss = 0.73 (286.3 examples/sec; 0.447 sec/batch)
2018-03-22 14:02:02.382455: step 17670, loss = 0.73 (284.7 examples/sec; 0.450 sec/batch)
2018-03-22 14:02:07.256866: step 17680, loss = 0.81 (262.6 examples/sec; 0.487 sec/batch)
2018-03-22 14:02:12.186664: step 17690, loss = 0.84 (259.6 examples/sec; 0.493 sec/batch)
2018-03-22 14:02:17.202403: step 17700, loss = 0.80 (255.2 examples/sec; 0.502 sec/batch)
2018-03-22 14:02:22.187418: step 17710, loss = 0.85 (256.8 examples/sec; 0.499 sec/batch)
2018-03-22 14:02:27.454326: step 17720, loss = 0.86 (243.0 examples/sec; 0.527 sec/batch)
2018-03-22 14:02:32.448669: step 17730, loss = 1.09 (256.3 examples/sec; 0.499 sec/batch)
2018-03-22 14:02:37.168382: step 17740, loss = 0.79 (271.2 examples/sec; 0.472 sec/batch)
2018-03-22 14:02:42.181421: step 17750, loss = 0.93 (255.3 examples/sec; 0.501 sec/batch)
2018-03-22 14:02:47.314392: step 17760, loss = 0.76 (249.4 examples/sec; 0.513 sec/batch)
2018-03-22 14:02:52.506247: step 17770, loss = 0.83 (246.5 examples/sec; 0.519 sec/batch)
2018-03-22 14:02:57.115237: step 17780, loss = 0.75 (277.7 examples/sec; 0.461 sec/batch)
2018-03-22 14:03:01.811656: step 17790, loss = 0.92 (272.5 examples/sec; 0.470 sec/batch)
2018-03-22 14:03:06.661396: step 17800, loss = 0.88 (263.9 examples/sec; 0.485 sec/batch)
2018-03-22 14:03:11.454378: step 17810, loss = 0.82 (267.1 examples/sec; 0.479 sec/batch)
2018-03-22 14:03:16.464537: step 17820, loss = 0.75 (255.5 examples/sec; 0.501 sec/batch)
2018-03-22 14:03:21.328653: step 17830, loss = 0.97 (263.2 examples/sec; 0.486 sec/batch)
2018-03-22 14:03:25.617558: step 17840, loss = 0.72 (298.4 examples/sec; 0.429 sec/batch)
2018-03-22 14:03:31.049166: step 17850, loss = 0.65 (235.7 examples/sec; 0.543 sec/batch)
2018-03-22 14:03:36.428508: step 17860, loss = 0.92 (237.9 examples/sec; 0.538 sec/batch)
2018-03-22 14:03:41.332454: step 17870, loss = 0.88 (261.0 examples/sec; 0.490 sec/batch)
2018-03-22 14:03:46.128706: step 17880, loss = 0.80 (266.9 examples/sec; 0.480 sec/batch)
2018-03-22 14:03:51.122962: step 17890, loss = 0.72 (256.3 examples/sec; 0.499 sec/batch)
2018-03-22 14:03:56.141695: step 17900, loss = 0.86 (255.0 examples/sec; 0.502 sec/batch)
2018-03-22 14:04:01.183393: step 17910, loss = 0.91 (253.9 examples/sec; 0.504 sec/batch)
2018-03-22 14:04:06.004935: step 17920, loss = 0.84 (265.5 examples/sec; 0.482 sec/batch)
2018-03-22 14:04:11.450394: step 17930, loss = 0.80 (235.1 examples/sec; 0.545 sec/batch)
2018-03-22 14:04:16.440272: step 17940, loss = 0.91 (256.5 examples/sec; 0.499 sec/batch)
2018-03-22 14:04:21.268484: step 17950, loss = 0.83 (265.1 examples/sec; 0.483 sec/batch)
2018-03-22 14:04:26.602791: step 17960, loss = 0.74 (240.0 examples/sec; 0.533 sec/batch)
2018-03-22 14:04:32.374900: step 17970, loss = 0.73 (221.8 examples/sec; 0.577 sec/batch)
2018-03-22 14:04:37.039407: step 17980, loss = 0.81 (274.4 examples/sec; 0.466 sec/batch)
2018-03-22 14:04:41.729754: step 17990, loss = 1.00 (272.9 examples/sec; 0.469 sec/batch)
2018-03-22 14:04:46.636856: step 18000, loss = 0.79 (260.8 examples/sec; 0.491 sec/batch)
2018-03-22 14:04:51.186538: step 18010, loss = 0.87 (281.3 examples/sec; 0.455 sec/batch)
2018-03-22 14:04:55.890380: step 18020, loss = 0.69 (272.1 examples/sec; 0.470 sec/batch)
2018-03-22 14:05:01.052461: step 18030, loss = 0.92 (248.0 examples/sec; 0.516 sec/batch)
2018-03-22 14:05:06.265101: step 18040, loss = 0.73 (245.6 examples/sec; 0.521 sec/batch)
2018-03-22 14:05:11.331743: step 18050, loss = 0.86 (252.6 examples/sec; 0.507 sec/batch)
2018-03-22 14:05:16.284436: step 18060, loss = 0.89 (258.4 examples/sec; 0.495 sec/batch)
2018-03-22 14:05:21.439360: step 18070, loss = 0.82 (248.3 examples/sec; 0.515 sec/batch)
2018-03-22 14:05:26.517486: step 18080, loss = 0.87 (252.1 examples/sec; 0.508 sec/batch)
2018-03-22 14:05:31.665409: step 18090, loss = 0.89 (248.6 examples/sec; 0.515 sec/batch)
2018-03-22 14:05:36.932849: step 18100, loss = 0.90 (243.0 examples/sec; 0.527 sec/batch)
2018-03-22 14:05:41.976032: step 18110, loss = 0.85 (253.8 examples/sec; 0.504 sec/batch)
2018-03-22 14:05:46.296428: step 18120, loss = 0.81 (296.3 examples/sec; 0.432 sec/batch)
2018-03-22 14:05:50.870419: step 18130, loss = 0.92 (279.8 examples/sec; 0.457 sec/batch)
2018-03-22 14:05:55.975392: step 18140, loss = 0.73 (250.7 examples/sec; 0.510 sec/batch)
2018-03-22 14:06:00.822331: step 18150, loss = 0.84 (264.1 examples/sec; 0.485 sec/batch)
2018-03-22 14:06:05.603420: step 18160, loss = 0.96 (267.7 examples/sec; 0.478 sec/batch)
2018-03-22 14:06:10.560395: step 18170, loss = 0.75 (258.2 examples/sec; 0.496 sec/batch)
2018-03-22 14:06:15.839352: step 18180, loss = 0.74 (242.5 examples/sec; 0.528 sec/batch)
2018-03-22 14:06:20.863355: step 18190, loss = 0.58 (254.8 examples/sec; 0.502 sec/batch)
2018-03-22 14:06:25.459064: step 18200, loss = 0.74 (278.5 examples/sec; 0.460 sec/batch)
2018-03-22 14:06:30.609280: step 18210, loss = 0.89 (248.5 examples/sec; 0.515 sec/batch)
2018-03-22 14:06:36.074414: step 18220, loss = 0.55 (234.2 examples/sec; 0.547 sec/batch)
2018-03-22 14:06:40.954010: step 18230, loss = 0.78 (262.3 examples/sec; 0.488 sec/batch)
2018-03-22 14:06:45.855212: step 18240, loss = 0.72 (261.2 examples/sec; 0.490 sec/batch)
2018-03-22 14:06:51.097398: step 18250, loss = 0.68 (244.2 examples/sec; 0.524 sec/batch)
2018-03-22 14:06:55.434452: step 18260, loss = 0.67 (295.1 examples/sec; 0.434 sec/batch)
2018-03-22 14:07:00.297368: step 18270, loss = 0.92 (263.2 examples/sec; 0.486 sec/batch)
2018-03-22 14:07:05.258074: step 18280, loss = 0.91 (258.0 examples/sec; 0.496 sec/batch)
2018-03-22 14:07:10.409403: step 18290, loss = 0.80 (248.5 examples/sec; 0.515 sec/batch)
2018-03-22 14:07:15.285870: step 18300, loss = 1.04 (262.5 examples/sec; 0.488 sec/batch)
2018-03-22 14:07:20.185056: step 18310, loss = 0.90 (261.3 examples/sec; 0.490 sec/batch)
2018-03-22 14:07:24.968405: step 18320, loss = 0.79 (267.6 examples/sec; 0.478 sec/batch)
2018-03-22 14:07:30.242265: step 18330, loss = 0.90 (242.7 examples/sec; 0.527 sec/batch)
2018-03-22 14:07:35.771365: step 18340, loss = 0.86 (231.5 examples/sec; 0.553 sec/batch)
2018-03-22 14:07:40.914133: step 18350, loss = 0.71 (248.9 examples/sec; 0.514 sec/batch)
2018-03-22 14:07:45.873222: step 18360, loss = 0.78 (258.1 examples/sec; 0.496 sec/batch)
2018-03-22 14:07:50.844385: step 18370, loss = 0.88 (257.5 examples/sec; 0.497 sec/batch)
2018-03-22 14:07:55.580402: step 18380, loss = 0.76 (270.3 examples/sec; 0.474 sec/batch)
2018-03-22 14:08:01.220408: step 18390, loss = 1.00 (227.0 examples/sec; 0.564 sec/batch)
2018-03-22 14:08:06.597476: step 18400, loss = 0.95 (238.0 examples/sec; 0.538 sec/batch)
2018-03-22 14:08:11.487329: step 18410, loss = 0.75 (261.8 examples/sec; 0.489 sec/batch)
2018-03-22 14:08:15.981767: step 18420, loss = 0.67 (284.8 examples/sec; 0.449 sec/batch)
2018-03-22 14:08:21.332398: step 18430, loss = 0.87 (239.2 examples/sec; 0.535 sec/batch)
2018-03-22 14:08:26.401339: step 18440, loss = 0.92 (252.5 examples/sec; 0.507 sec/batch)
2018-03-22 14:08:31.759458: step 18450, loss = 0.65 (238.9 examples/sec; 0.536 sec/batch)
2018-03-22 14:08:36.464445: step 18460, loss = 1.03 (272.1 examples/sec; 0.470 sec/batch)
2018-03-22 14:08:41.185342: step 18470, loss = 0.85 (271.1 examples/sec; 0.472 sec/batch)
2018-03-22 14:08:46.175500: step 18480, loss = 1.03 (256.5 examples/sec; 0.499 sec/batch)
2018-03-22 14:08:51.157389: step 18490, loss = 0.93 (256.9 examples/sec; 0.498 sec/batch)
2018-03-22 14:08:56.241228: step 18500, loss = 0.80 (251.8 examples/sec; 0.508 sec/batch)
2018-03-22 14:09:01.354791: step 18510, loss = 0.73 (250.3 examples/sec; 0.511 sec/batch)
2018-03-22 14:09:06.139402: step 18520, loss = 0.75 (267.5 examples/sec; 0.478 sec/batch)
2018-03-22 14:09:11.056555: step 18530, loss = 1.04 (260.3 examples/sec; 0.492 sec/batch)
2018-03-22 14:09:15.806419: step 18540, loss = 0.74 (269.5 examples/sec; 0.475 sec/batch)
2018-03-22 14:09:21.552904: step 18550, loss = 0.81 (222.7 examples/sec; 0.575 sec/batch)
2018-03-22 14:09:26.715411: step 18560, loss = 0.84 (247.9 examples/sec; 0.516 sec/batch)
2018-03-22 14:09:31.704417: step 18570, loss = 0.83 (256.6 examples/sec; 0.499 sec/batch)
2018-03-22 14:09:36.322844: step 18580, loss = 0.88 (277.2 examples/sec; 0.462 sec/batch)
2018-03-22 14:09:41.026265: step 18590, loss = 0.73 (272.1 examples/sec; 0.470 sec/batch)
2018-03-22 14:09:45.634631: step 18600, loss = 0.92 (277.8 examples/sec; 0.461 sec/batch)
2018-03-22 14:09:50.566916: step 18610, loss = 0.80 (259.5 examples/sec; 0.493 sec/batch)
2018-03-22 14:09:55.100071: step 18620, loss = 0.74 (282.4 examples/sec; 0.453 sec/batch)
2018-03-22 14:10:00.767355: step 18630, loss = 0.92 (225.9 examples/sec; 0.567 sec/batch)
2018-03-22 14:10:05.478322: step 18640, loss = 0.95 (271.7 examples/sec; 0.471 sec/batch)
2018-03-22 14:10:10.501625: step 18650, loss = 0.94 (254.8 examples/sec; 0.502 sec/batch)
2018-03-22 14:10:15.542322: step 18660, loss = 0.78 (253.9 examples/sec; 0.504 sec/batch)
2018-03-22 14:10:20.428933: step 18670, loss = 0.83 (261.9 examples/sec; 0.489 sec/batch)
2018-03-22 14:10:25.282266: step 18680, loss = 0.87 (263.7 examples/sec; 0.485 sec/batch)
2018-03-22 14:10:30.360426: step 18690, loss = 0.82 (252.1 examples/sec; 0.508 sec/batch)
2018-03-22 14:10:35.926683: step 18700, loss = 0.87 (230.0 examples/sec; 0.557 sec/batch)
2018-03-22 14:10:41.570430: step 18710, loss = 0.69 (226.8 examples/sec; 0.564 sec/batch)
2018-03-22 14:10:46.391148: step 18720, loss = 0.93 (265.5 examples/sec; 0.482 sec/batch)
2018-03-22 14:10:51.823410: step 18730, loss = 0.80 (235.6 examples/sec; 0.543 sec/batch)
2018-03-22 14:10:56.687485: step 18740, loss = 0.80 (263.2 examples/sec; 0.486 sec/batch)
2018-03-22 14:11:01.621601: step 18750, loss = 0.70 (259.4 examples/sec; 0.493 sec/batch)
2018-03-22 14:11:06.088803: step 18760, loss = 0.83 (286.5 examples/sec; 0.447 sec/batch)
2018-03-22 14:11:11.109327: step 18770, loss = 0.74 (255.0 examples/sec; 0.502 sec/batch)
2018-03-22 14:11:16.049049: step 18780, loss = 0.88 (259.1 examples/sec; 0.494 sec/batch)
2018-03-22 14:11:21.479565: step 18790, loss = 0.90 (235.7 examples/sec; 0.543 sec/batch)
2018-03-22 14:11:26.686588: step 18800, loss = 0.81 (245.8 examples/sec; 0.521 sec/batch)
2018-03-22 14:11:31.823423: step 18810, loss = 0.88 (249.2 examples/sec; 0.514 sec/batch)
2018-03-22 14:11:36.343337: step 18820, loss = 0.79 (283.2 examples/sec; 0.452 sec/batch)
2018-03-22 14:11:41.545472: step 18830, loss = 0.94 (246.1 examples/sec; 0.520 sec/batch)
2018-03-22 14:11:46.622343: step 18840, loss = 0.76 (252.1 examples/sec; 0.508 sec/batch)
2018-03-22 14:11:52.150379: step 18850, loss = 1.02 (231.5 examples/sec; 0.553 sec/batch)
2018-03-22 14:11:57.303300: step 18860, loss = 0.87 (248.4 examples/sec; 0.515 sec/batch)
2018-03-22 14:12:02.580068: step 18870, loss = 0.76 (242.6 examples/sec; 0.528 sec/batch)
2018-03-22 14:12:07.116391: step 18880, loss = 0.84 (282.2 examples/sec; 0.454 sec/batch)
2018-03-22 14:12:12.412472: step 18890, loss = 0.92 (241.7 examples/sec; 0.530 sec/batch)
2018-03-22 14:12:17.634648: step 18900, loss = 0.79 (245.1 examples/sec; 0.522 sec/batch)
2018-03-22 14:12:23.162464: step 18910, loss = 0.83 (231.6 examples/sec; 0.553 sec/batch)
2018-03-22 14:12:28.032024: step 18920, loss = 1.10 (262.9 examples/sec; 0.487 sec/batch)
2018-03-22 14:12:33.130395: step 18930, loss = 0.63 (251.1 examples/sec; 0.510 sec/batch)
2018-03-22 14:12:38.354523: step 18940, loss = 0.91 (245.0 examples/sec; 0.522 sec/batch)
2018-03-22 14:12:44.186412: step 18950, loss = 0.85 (219.5 examples/sec; 0.583 sec/batch)
2018-03-22 14:12:49.487250: step 18960, loss = 0.79 (241.5 examples/sec; 0.530 sec/batch)
2018-03-22 14:12:54.272014: step 18970, loss = 0.73 (267.5 examples/sec; 0.478 sec/batch)
2018-03-22 14:12:59.218306: step 18980, loss = 0.79 (258.8 examples/sec; 0.495 sec/batch)
2018-03-22 14:13:04.551527: step 18990, loss = 0.89 (240.0 examples/sec; 0.533 sec/batch)
2018-03-22 14:13:10.057625: step 19000, loss = 1.06 (232.5 examples/sec; 0.551 sec/batch)
2018-03-22 14:13:15.327364: step 19010, loss = 0.79 (242.9 examples/sec; 0.527 sec/batch)
2018-03-22 14:13:20.880405: step 19020, loss = 0.67 (230.5 examples/sec; 0.555 sec/batch)
2018-03-22 14:13:25.582558: step 19030, loss = 0.74 (272.2 examples/sec; 0.470 sec/batch)
2018-03-22 14:13:31.320409: step 19040, loss = 0.85 (223.1 examples/sec; 0.574 sec/batch)
2018-03-22 14:13:36.874438: step 19050, loss = 0.77 (230.5 examples/sec; 0.555 sec/batch)
2018-03-22 14:13:41.915341: step 19060, loss = 0.64 (253.9 examples/sec; 0.504 sec/batch)
2018-03-22 14:13:46.685399: step 19070, loss = 0.63 (268.3 examples/sec; 0.477 sec/batch)
2018-03-22 14:13:51.736385: step 19080, loss = 1.02 (253.4 examples/sec; 0.505 sec/batch)
2018-03-22 14:13:57.080386: step 19090, loss = 0.82 (239.5 examples/sec; 0.534 sec/batch)
2018-03-22 14:14:02.629101: step 19100, loss = 0.80 (230.7 examples/sec; 0.555 sec/batch)
2018-03-22 14:14:07.707561: step 19110, loss = 0.77 (252.0 examples/sec; 0.508 sec/batch)
2018-03-22 14:14:13.123333: step 19120, loss = 0.80 (236.3 examples/sec; 0.542 sec/batch)
2018-03-22 14:14:17.832312: step 19130, loss = 0.80 (271.8 examples/sec; 0.471 sec/batch)
2018-03-22 14:14:23.354466: step 19140, loss = 0.87 (231.8 examples/sec; 0.552 sec/batch)
2018-03-22 14:14:28.308362: step 19150, loss = 0.76 (258.4 examples/sec; 0.495 sec/batch)
2018-03-22 14:14:33.889414: step 19160, loss = 0.74 (229.3 examples/sec; 0.558 sec/batch)
2018-03-22 14:14:38.454440: step 19170, loss = 0.77 (280.4 examples/sec; 0.457 sec/batch)
2018-03-22 14:14:43.117325: step 19180, loss = 0.76 (274.5 examples/sec; 0.466 sec/batch)
2018-03-22 14:14:47.543047: step 19190, loss = 0.65 (289.2 examples/sec; 0.443 sec/batch)
2018-03-22 14:14:53.290316: step 19200, loss = 0.80 (222.7 examples/sec; 0.575 sec/batch)
2018-03-22 14:14:58.277110: step 19210, loss = 0.83 (256.7 examples/sec; 0.499 sec/batch)
2018-03-22 14:15:03.079897: step 19220, loss = 0.91 (266.5 examples/sec; 0.480 sec/batch)
2018-03-22 14:15:08.374980: step 19230, loss = 0.73 (241.7 examples/sec; 0.530 sec/batch)
2018-03-22 14:15:13.062783: step 19240, loss = 0.61 (273.0 examples/sec; 0.469 sec/batch)
2018-03-22 14:15:17.910413: step 19250, loss = 0.92 (264.0 examples/sec; 0.485 sec/batch)
2018-03-22 14:15:23.035377: step 19260, loss = 0.66 (249.8 examples/sec; 0.512 sec/batch)
2018-03-22 14:15:28.073395: step 19270, loss = 0.99 (254.1 examples/sec; 0.504 sec/batch)
2018-03-22 14:15:33.247005: step 19280, loss = 0.70 (247.4 examples/sec; 0.517 sec/batch)
2018-03-22 14:15:37.784344: step 19290, loss = 0.94 (282.1 examples/sec; 0.454 sec/batch)
2018-03-22 14:15:43.530766: step 19300, loss = 0.85 (222.7 examples/sec; 0.575 sec/batch)
2018-03-22 14:15:48.325332: step 19310, loss = 0.90 (267.0 examples/sec; 0.479 sec/batch)
2018-03-22 14:15:52.978458: step 19320, loss = 0.88 (275.1 examples/sec; 0.465 sec/batch)
2018-03-22 14:15:58.278462: step 19330, loss = 0.71 (241.5 examples/sec; 0.530 sec/batch)
2018-03-22 14:16:03.853409: step 19340, loss = 1.13 (229.6 examples/sec; 0.557 sec/batch)
2018-03-22 14:16:08.885473: step 19350, loss = 0.90 (254.4 examples/sec; 0.503 sec/batch)
2018-03-22 14:16:13.725437: step 19360, loss = 0.90 (264.5 examples/sec; 0.484 sec/batch)
2018-03-22 14:16:19.060441: step 19370, loss = 0.84 (239.9 examples/sec; 0.534 sec/batch)
2018-03-22 14:16:24.461445: step 19380, loss = 0.87 (237.0 examples/sec; 0.540 sec/batch)
2018-03-22 14:16:29.388385: step 19390, loss = 0.87 (259.8 examples/sec; 0.493 sec/batch)
2018-03-22 14:16:34.649246: step 19400, loss = 0.88 (243.3 examples/sec; 0.526 sec/batch)
2018-03-22 14:16:39.517264: step 19410, loss = 0.77 (262.9 examples/sec; 0.487 sec/batch)
2018-03-22 14:16:44.041485: step 19420, loss = 0.98 (282.9 examples/sec; 0.452 sec/batch)
2018-03-22 14:16:49.062976: step 19430, loss = 0.85 (254.9 examples/sec; 0.502 sec/batch)
2018-03-22 14:16:54.297394: step 19440, loss = 0.74 (244.5 examples/sec; 0.523 sec/batch)
2018-03-22 14:16:59.334396: step 19450, loss = 0.83 (254.1 examples/sec; 0.504 sec/batch)
2018-03-22 14:17:04.265363: step 19460, loss = 0.74 (259.6 examples/sec; 0.493 sec/batch)
2018-03-22 14:17:09.933361: step 19470, loss = 0.98 (225.8 examples/sec; 0.567 sec/batch)
2018-03-22 14:17:14.571468: step 19480, loss = 0.83 (276.0 examples/sec; 0.464 sec/batch)
2018-03-22 14:17:19.985440: step 19490, loss = 0.85 (236.4 examples/sec; 0.541 sec/batch)
2018-03-22 14:17:24.981533: step 19500, loss = 0.86 (256.2 examples/sec; 0.500 sec/batch)
2018-03-22 14:17:30.071618: step 19510, loss = 0.73 (251.5 examples/sec; 0.509 sec/batch)
2018-03-22 14:17:34.795416: step 19520, loss = 0.86 (271.0 examples/sec; 0.472 sec/batch)
2018-03-22 14:17:39.918859: step 19530, loss = 0.83 (249.8 examples/sec; 0.512 sec/batch)
2018-03-22 14:17:45.059300: step 19540, loss = 0.75 (249.0 examples/sec; 0.514 sec/batch)
2018-03-22 14:17:50.581446: step 19550, loss = 0.78 (231.8 examples/sec; 0.552 sec/batch)
2018-03-22 14:17:55.717410: step 19560, loss = 0.84 (249.2 examples/sec; 0.514 sec/batch)
2018-03-22 14:18:00.564389: step 19570, loss = 0.76 (264.1 examples/sec; 0.485 sec/batch)
2018-03-22 14:18:05.821286: step 19580, loss = 0.85 (243.5 examples/sec; 0.526 sec/batch)
2018-03-22 14:18:11.350410: step 19590, loss = 0.83 (231.5 examples/sec; 0.553 sec/batch)
2018-03-22 14:18:15.998600: step 19600, loss = 0.90 (275.4 examples/sec; 0.465 sec/batch)
2018-03-22 14:18:21.060529: step 19610, loss = 0.88 (252.9 examples/sec; 0.506 sec/batch)
2018-03-22 14:18:25.341389: step 19620, loss = 0.88 (299.0 examples/sec; 0.428 sec/batch)
2018-03-22 14:18:29.834688: step 19630, loss = 0.95 (284.9 examples/sec; 0.449 sec/batch)
2018-03-22 14:18:34.104218: step 19640, loss = 0.85 (299.8 examples/sec; 0.427 sec/batch)
2018-03-22 14:18:39.234406: step 19650, loss = 0.90 (249.5 examples/sec; 0.513 sec/batch)
2018-03-22 14:18:44.625107: step 19660, loss = 0.87 (237.4 examples/sec; 0.539 sec/batch)
2018-03-22 14:18:49.617418: step 19670, loss = 0.97 (256.4 examples/sec; 0.499 sec/batch)
2018-03-22 14:18:54.672357: step 19680, loss = 0.83 (253.2 examples/sec; 0.505 sec/batch)
2018-03-22 14:18:59.949225: step 19690, loss = 0.86 (242.6 examples/sec; 0.528 sec/batch)
2018-03-22 14:19:04.694646: step 19700, loss = 0.90 (269.7 examples/sec; 0.475 sec/batch)
2018-03-22 14:19:09.489167: step 19710, loss = 0.89 (267.0 examples/sec; 0.479 sec/batch)
2018-03-22 14:19:14.320855: step 19720, loss = 0.80 (264.9 examples/sec; 0.483 sec/batch)
2018-03-22 14:19:19.806045: step 19730, loss = 0.87 (233.4 examples/sec; 0.549 sec/batch)
2018-03-22 14:19:25.552342: step 19740, loss = 0.89 (222.8 examples/sec; 0.575 sec/batch)
2018-03-22 14:19:30.319400: step 19750, loss = 0.74 (268.5 examples/sec; 0.477 sec/batch)
2018-03-22 14:19:34.792205: step 19760, loss = 0.83 (286.2 examples/sec; 0.447 sec/batch)
2018-03-22 14:19:40.794428: step 19770, loss = 0.97 (213.3 examples/sec; 0.600 sec/batch)
2018-03-22 14:19:45.883415: step 19780, loss = 0.77 (251.5 examples/sec; 0.509 sec/batch)
2018-03-22 14:19:50.819321: step 19790, loss = 0.92 (259.3 examples/sec; 0.494 sec/batch)
2018-03-22 14:19:55.328940: step 19800, loss = 0.79 (283.8 examples/sec; 0.451 sec/batch)
2018-03-22 14:20:00.475322: step 19810, loss = 0.74 (248.7 examples/sec; 0.515 sec/batch)
2018-03-22 14:20:05.622843: step 19820, loss = 0.83 (248.7 examples/sec; 0.515 sec/batch)
2018-03-22 14:20:10.669378: step 19830, loss = 0.82 (253.6 examples/sec; 0.505 sec/batch)
2018-03-22 14:20:15.590386: step 19840, loss = 0.77 (260.1 examples/sec; 0.492 sec/batch)
2018-03-22 14:20:20.527880: step 19850, loss = 0.89 (259.2 examples/sec; 0.494 sec/batch)
2018-03-22 14:20:25.325766: step 19860, loss = 0.85 (266.8 examples/sec; 0.480 sec/batch)
2018-03-22 14:20:30.929858: step 19870, loss = 0.74 (228.4 examples/sec; 0.560 sec/batch)
2018-03-22 14:20:35.441648: step 19880, loss = 0.79 (283.7 examples/sec; 0.451 sec/batch)
2018-03-22 14:20:40.114313: step 19890, loss = 0.70 (273.9 examples/sec; 0.467 sec/batch)
2018-03-22 14:20:44.962154: step 19900, loss = 0.96 (264.0 examples/sec; 0.485 sec/batch)
2018-03-22 14:20:50.031475: step 19910, loss = 0.82 (252.5 examples/sec; 0.507 sec/batch)
2018-03-22 14:20:54.655964: step 19920, loss = 0.78 (276.8 examples/sec; 0.462 sec/batch)
2018-03-22 14:20:59.749984: step 19930, loss = 0.68 (251.3 examples/sec; 0.509 sec/batch)
2018-03-22 14:21:05.142675: step 19940, loss = 0.85 (237.4 examples/sec; 0.539 sec/batch)
2018-03-22 14:21:10.255101: step 19950, loss = 0.81 (250.4 examples/sec; 0.511 sec/batch)
2018-03-22 14:21:15.368611: step 19960, loss = 0.78 (250.3 examples/sec; 0.511 sec/batch)
2018-03-22 14:21:20.384773: step 19970, loss = 0.75 (255.2 examples/sec; 0.502 sec/batch)
2018-03-22 14:21:25.282393: step 19980, loss = 0.80 (261.4 examples/sec; 0.490 sec/batch)
2018-03-22 14:21:30.215067: step 19990, loss = 1.01 (259.5 examples/sec; 0.493 sec/batch)
2018-03-22 14:21:35.578280: step 20000, loss = 0.79 (238.7 examples/sec; 0.536 sec/batch)
2018-03-22 14:21:40.590403: step 20010, loss = 0.84 (255.4 examples/sec; 0.501 sec/batch)
2018-03-22 14:21:45.302856: step 20020, loss = 0.75 (271.6 examples/sec; 0.471 sec/batch)
2018-03-22 14:21:50.745430: step 20030, loss = 0.78 (235.2 examples/sec; 0.544 sec/batch)
2018-03-22 14:21:55.804360: step 20040, loss = 0.90 (253.0 examples/sec; 0.506 sec/batch)
2018-03-22 14:22:00.926382: step 20050, loss = 0.92 (249.9 examples/sec; 0.512 sec/batch)
2018-03-22 14:22:05.065315: step 20060, loss = 0.69 (309.3 examples/sec; 0.414 sec/batch)
2018-03-22 14:22:09.955988: step 20070, loss = 0.77 (261.7 examples/sec; 0.489 sec/batch)
2018-03-22 14:22:14.625375: step 20080, loss = 0.76 (274.1 examples/sec; 0.467 sec/batch)
2018-03-22 14:22:18.867830: step 20090, loss = 1.17 (301.7 examples/sec; 0.424 sec/batch)
2018-03-22 14:22:23.682608: step 20100, loss = 0.85 (265.8 examples/sec; 0.481 sec/batch)
2018-03-22 14:22:28.845438: step 20110, loss = 0.80 (247.9 examples/sec; 0.516 sec/batch)
2018-03-22 14:22:34.090382: step 20120, loss = 0.82 (244.0 examples/sec; 0.524 sec/batch)
2018-03-22 14:22:39.299344: step 20130, loss = 0.69 (245.7 examples/sec; 0.521 sec/batch)
2018-03-22 14:22:44.601355: step 20140, loss = 0.84 (241.4 examples/sec; 0.530 sec/batch)
2018-03-22 14:22:49.458299: step 20150, loss = 0.74 (263.5 examples/sec; 0.486 sec/batch)
2018-03-22 14:22:54.491425: step 20160, loss = 0.99 (254.3 examples/sec; 0.503 sec/batch)
2018-03-22 14:22:59.017248: step 20170, loss = 0.81 (282.8 examples/sec; 0.453 sec/batch)
2018-03-22 14:23:04.504390: step 20180, loss = 0.93 (233.3 examples/sec; 0.549 sec/batch)
2018-03-22 14:23:09.709026: step 20190, loss = 0.84 (245.9 examples/sec; 0.520 sec/batch)
2018-03-22 14:23:14.864628: step 20200, loss = 0.81 (248.3 examples/sec; 0.516 sec/batch)
2018-03-22 14:23:19.743336: step 20210, loss = 0.82 (262.4 examples/sec; 0.488 sec/batch)
2018-03-22 14:23:24.945345: step 20220, loss = 0.79 (246.1 examples/sec; 0.520 sec/batch)
2018-03-22 14:23:29.775440: step 20230, loss = 0.67 (265.0 examples/sec; 0.483 sec/batch)
2018-03-22 14:23:35.273330: step 20240, loss = 0.78 (232.8 examples/sec; 0.550 sec/batch)
2018-03-22 14:23:40.664284: step 20250, loss = 0.75 (237.4 examples/sec; 0.539 sec/batch)
2018-03-22 14:23:45.641006: step 20260, loss = 0.76 (257.2 examples/sec; 0.498 sec/batch)
2018-03-22 14:23:50.450403: step 20270, loss = 0.86 (266.1 examples/sec; 0.481 sec/batch)
2018-03-22 14:23:55.394386: step 20280, loss = 0.75 (258.9 examples/sec; 0.494 sec/batch)
2018-03-22 14:24:00.673776: step 20290, loss = 0.88 (242.5 examples/sec; 0.528 sec/batch)
2018-03-22 14:24:05.911287: step 20300, loss = 0.62 (244.4 examples/sec; 0.524 sec/batch)
2018-03-22 14:24:11.113351: step 20310, loss = 0.70 (246.1 examples/sec; 0.520 sec/batch)
2018-03-22 14:24:16.153490: step 20320, loss = 0.83 (254.0 examples/sec; 0.504 sec/batch)
2018-03-22 14:24:20.900466: step 20330, loss = 0.79 (269.6 examples/sec; 0.475 sec/batch)
2018-03-22 14:24:25.636394: step 20340, loss = 0.85 (270.3 examples/sec; 0.474 sec/batch)
2018-03-22 14:24:30.846515: step 20350, loss = 0.75 (245.7 examples/sec; 0.521 sec/batch)
2018-03-22 14:24:35.885267: step 20360, loss = 0.85 (254.0 examples/sec; 0.504 sec/batch)
2018-03-22 14:24:40.792918: step 20370, loss = 0.98 (260.8 examples/sec; 0.491 sec/batch)
2018-03-22 14:24:45.732356: step 20380, loss = 0.88 (259.1 examples/sec; 0.494 sec/batch)
2018-03-22 14:24:51.067373: step 20390, loss = 0.86 (239.9 examples/sec; 0.534 sec/batch)
2018-03-22 14:24:56.746331: step 20400, loss = 0.82 (225.4 examples/sec; 0.568 sec/batch)
2018-03-22 14:25:01.384391: step 20410, loss = 0.85 (276.0 examples/sec; 0.464 sec/batch)
2018-03-22 14:25:06.075429: step 20420, loss = 0.86 (272.9 examples/sec; 0.469 sec/batch)
2018-03-22 14:25:11.392411: step 20430, loss = 0.84 (240.7 examples/sec; 0.532 sec/batch)
2018-03-22 14:25:16.251221: step 20440, loss = 0.83 (263.4 examples/sec; 0.486 sec/batch)
2018-03-22 14:25:21.043984: step 20450, loss = 0.90 (267.1 examples/sec; 0.479 sec/batch)
2018-03-22 14:25:26.162569: step 20460, loss = 0.87 (250.1 examples/sec; 0.512 sec/batch)
2018-03-22 14:25:31.372471: step 20470, loss = 0.71 (245.7 examples/sec; 0.521 sec/batch)
2018-03-22 14:25:36.213571: step 20480, loss = 0.70 (264.4 examples/sec; 0.484 sec/batch)
2018-03-22 14:25:41.469376: step 20490, loss = 0.84 (243.5 examples/sec; 0.526 sec/batch)
2018-03-22 14:25:47.040150: step 20500, loss = 0.73 (229.8 examples/sec; 0.557 sec/batch)
2018-03-22 14:25:52.363381: step 20510, loss = 0.73 (240.5 examples/sec; 0.532 sec/batch)
2018-03-22 14:25:57.225313: step 20520, loss = 0.88 (263.3 examples/sec; 0.486 sec/batch)
2018-03-22 14:26:02.400368: step 20530, loss = 0.98 (247.3 examples/sec; 0.518 sec/batch)
2018-03-22 14:26:07.153245: step 20540, loss = 0.70 (269.3 examples/sec; 0.475 sec/batch)
2018-03-22 14:26:11.606252: step 20550, loss = 1.00 (287.4 examples/sec; 0.445 sec/batch)
2018-03-22 14:26:15.944416: step 20560, loss = 0.74 (295.1 examples/sec; 0.434 sec/batch)
2018-03-22 14:26:21.584212: step 20570, loss = 0.79 (227.0 examples/sec; 0.564 sec/batch)
2018-03-22 14:26:26.551119: step 20580, loss = 0.86 (257.7 examples/sec; 0.497 sec/batch)
2018-03-22 14:26:31.509316: step 20590, loss = 0.88 (258.2 examples/sec; 0.496 sec/batch)
2018-03-22 14:26:36.588254: step 20600, loss = 0.74 (252.0 examples/sec; 0.508 sec/batch)
2018-03-22 14:26:41.376074: step 20610, loss = 0.93 (267.3 examples/sec; 0.479 sec/batch)
2018-03-22 14:26:45.960416: step 20620, loss = 0.86 (279.2 examples/sec; 0.458 sec/batch)
2018-03-22 14:26:51.235734: step 20630, loss = 0.77 (242.6 examples/sec; 0.528 sec/batch)
2018-03-22 14:26:56.536430: step 20640, loss = 0.81 (241.5 examples/sec; 0.530 sec/batch)
2018-03-22 14:27:02.008145: step 20650, loss = 0.78 (233.9 examples/sec; 0.547 sec/batch)
2018-03-22 14:27:06.649152: step 20660, loss = 0.91 (275.8 examples/sec; 0.464 sec/batch)
2018-03-22 14:27:11.842155: step 20670, loss = 0.66 (246.5 examples/sec; 0.519 sec/batch)
2018-03-22 14:27:17.036771: step 20680, loss = 0.83 (246.4 examples/sec; 0.519 sec/batch)
2018-03-22 14:27:22.023438: step 20690, loss = 0.71 (256.7 examples/sec; 0.499 sec/batch)
2018-03-22 14:27:27.009671: step 20700, loss = 0.70 (256.7 examples/sec; 0.499 sec/batch)
2018-03-22 14:27:32.274460: step 20710, loss = 0.87 (243.1 examples/sec; 0.526 sec/batch)
2018-03-22 14:27:36.886463: step 20720, loss = 0.65 (277.5 examples/sec; 0.461 sec/batch)
2018-03-22 14:27:41.858463: step 20730, loss = 0.83 (257.4 examples/sec; 0.497 sec/batch)
2018-03-22 14:27:46.993434: step 20740, loss = 0.93 (249.3 examples/sec; 0.513 sec/batch)
2018-03-22 14:27:52.232342: step 20750, loss = 0.75 (244.3 examples/sec; 0.524 sec/batch)
2018-03-22 14:27:56.897396: step 20760, loss = 0.87 (274.4 examples/sec; 0.467 sec/batch)
2018-03-22 14:28:01.915461: step 20770, loss = 0.70 (255.1 examples/sec; 0.502 sec/batch)
2018-03-22 14:28:06.998766: step 20780, loss = 0.67 (251.8 examples/sec; 0.508 sec/batch)
2018-03-22 14:28:11.984389: step 20790, loss = 0.86 (256.7 examples/sec; 0.499 sec/batch)
2018-03-22 14:28:16.682026: step 20800, loss = 0.80 (272.5 examples/sec; 0.470 sec/batch)
2018-03-22 14:28:21.382433: step 20810, loss = 0.88 (272.3 examples/sec; 0.470 sec/batch)
2018-03-22 14:28:26.509490: step 20820, loss = 0.70 (249.7 examples/sec; 0.513 sec/batch)
2018-03-22 14:28:31.349423: step 20830, loss = 0.86 (264.5 examples/sec; 0.484 sec/batch)
2018-03-22 14:28:36.154471: step 20840, loss = 0.90 (266.4 examples/sec; 0.481 sec/batch)
2018-03-22 14:28:40.836683: step 20850, loss = 0.71 (273.4 examples/sec; 0.468 sec/batch)
2018-03-22 14:28:46.219018: step 20860, loss = 0.88 (237.8 examples/sec; 0.538 sec/batch)
2018-03-22 14:28:51.593185: step 20870, loss = 0.65 (238.2 examples/sec; 0.537 sec/batch)
2018-03-22 14:28:56.813367: step 20880, loss = 0.83 (245.2 examples/sec; 0.522 sec/batch)
2018-03-22 14:29:01.602977: step 20890, loss = 0.78 (267.2 examples/sec; 0.479 sec/batch)
2018-03-22 14:29:06.949230: step 20900, loss = 0.85 (239.4 examples/sec; 0.535 sec/batch)
2018-03-22 14:29:12.380957: step 20910, loss = 0.71 (235.7 examples/sec; 0.543 sec/batch)
2018-03-22 14:29:17.229440: step 20920, loss = 0.94 (264.0 examples/sec; 0.485 sec/batch)
2018-03-22 14:29:22.317399: step 20930, loss = 0.88 (251.6 examples/sec; 0.509 sec/batch)
2018-03-22 14:29:26.637003: step 20940, loss = 0.87 (296.3 examples/sec; 0.432 sec/batch)
2018-03-22 14:29:31.828886: step 20950, loss = 0.75 (246.5 examples/sec; 0.519 sec/batch)
2018-03-22 14:29:36.968962: step 20960, loss = 0.78 (249.0 examples/sec; 0.514 sec/batch)
2018-03-22 14:29:42.065041: step 20970, loss = 0.96 (251.2 examples/sec; 0.510 sec/batch)
2018-03-22 14:29:46.908541: step 20980, loss = 0.90 (264.3 examples/sec; 0.484 sec/batch)
2018-03-22 14:29:51.577746: step 20990, loss = 0.73 (274.1 examples/sec; 0.467 sec/batch)
2018-03-22 14:29:56.602932: step 21000, loss = 0.85 (254.7 examples/sec; 0.503 sec/batch)
2018-03-22 14:30:01.739164: step 21010, loss = 0.81 (249.2 examples/sec; 0.514 sec/batch)
2018-03-22 14:30:06.371488: step 21020, loss = 0.85 (276.3 examples/sec; 0.463 sec/batch)
2018-03-22 14:30:11.613284: step 21030, loss = 0.83 (244.2 examples/sec; 0.524 sec/batch)
2018-03-22 14:30:17.153581: step 21040, loss = 0.80 (231.0 examples/sec; 0.554 sec/batch)
2018-03-22 14:30:22.571112: step 21050, loss = 0.80 (236.3 examples/sec; 0.542 sec/batch)
2018-03-22 14:30:27.538322: step 21060, loss = 0.82 (257.7 examples/sec; 0.497 sec/batch)
2018-03-22 14:30:32.417813: step 21070, loss = 1.00 (262.3 examples/sec; 0.488 sec/batch)
2018-03-22 14:30:37.712354: step 21080, loss = 0.83 (241.8 examples/sec; 0.529 sec/batch)
2018-03-22 14:30:42.664392: step 21090, loss = 0.80 (258.5 examples/sec; 0.495 sec/batch)
2018-03-22 14:30:48.204599: step 21100, loss = 0.80 (231.0 examples/sec; 0.554 sec/batch)
2018-03-22 14:30:53.518286: step 21110, loss = 0.68 (240.9 examples/sec; 0.531 sec/batch)
2018-03-22 14:30:57.849643: step 21120, loss = 0.75 (295.5 examples/sec; 0.433 sec/batch)
2018-03-22 14:31:02.944299: step 21130, loss = 0.68 (251.2 examples/sec; 0.509 sec/batch)
2018-03-22 14:31:07.575927: step 21140, loss = 0.85 (276.4 examples/sec; 0.463 sec/batch)
2018-03-22 14:31:12.370399: step 21150, loss = 0.78 (267.0 examples/sec; 0.479 sec/batch)
2018-03-22 14:31:17.558319: step 21160, loss = 0.82 (246.7 examples/sec; 0.519 sec/batch)
2018-03-22 14:31:22.686615: step 21170, loss = 0.92 (249.6 examples/sec; 0.513 sec/batch)
2018-03-22 14:31:27.609832: step 21180, loss = 0.68 (260.0 examples/sec; 0.492 sec/batch)
2018-03-22 14:31:32.457471: step 21190, loss = 0.67 (264.0 examples/sec; 0.485 sec/batch)
2018-03-22 14:31:37.605723: step 21200, loss = 0.78 (248.6 examples/sec; 0.515 sec/batch)
2018-03-22 14:31:42.800388: step 21210, loss = 0.82 (246.4 examples/sec; 0.519 sec/batch)
2018-03-22 14:31:48.151466: step 21220, loss = 0.63 (239.2 examples/sec; 0.535 sec/batch)
2018-03-22 14:31:53.562698: step 21230, loss = 0.82 (236.5 examples/sec; 0.541 sec/batch)
2018-03-22 14:31:58.045383: step 21240, loss = 0.85 (285.5 examples/sec; 0.448 sec/batch)
2018-03-22 14:32:03.100098: step 21250, loss = 0.77 (253.2 examples/sec; 0.505 sec/batch)
2018-03-22 14:32:07.946362: step 21260, loss = 0.94 (264.1 examples/sec; 0.485 sec/batch)
2018-03-22 14:32:13.881415: step 21270, loss = 0.89 (215.7 examples/sec; 0.594 sec/batch)
2018-03-22 14:32:18.259191: step 21280, loss = 0.69 (292.4 examples/sec; 0.438 sec/batch)
2018-03-22 14:32:23.065329: step 21290, loss = 0.62 (266.3 examples/sec; 0.481 sec/batch)
2018-03-22 14:32:28.407291: step 21300, loss = 0.75 (239.6 examples/sec; 0.534 sec/batch)
2018-03-22 14:32:34.375386: step 21310, loss = 0.81 (214.5 examples/sec; 0.597 sec/batch)
2018-03-22 14:32:39.069446: step 21320, loss = 0.67 (272.7 examples/sec; 0.469 sec/batch)
2018-03-22 14:32:44.170420: step 21330, loss = 0.74 (250.9 examples/sec; 0.510 sec/batch)
2018-03-22 14:32:49.236380: step 21340, loss = 0.79 (252.7 examples/sec; 0.507 sec/batch)
2018-03-22 14:32:54.731739: step 21350, loss = 0.77 (232.9 examples/sec; 0.550 sec/batch)
2018-03-22 14:32:59.859042: step 21360, loss = 0.68 (249.6 examples/sec; 0.513 sec/batch)
2018-03-22 14:33:04.674415: step 21370, loss = 0.70 (265.8 examples/sec; 0.482 sec/batch)
2018-03-22 14:33:10.023487: step 21380, loss = 0.84 (239.3 examples/sec; 0.535 sec/batch)
2018-03-22 14:33:15.048453: step 21390, loss = 0.82 (254.7 examples/sec; 0.502 sec/batch)
2018-03-22 14:33:20.156765: step 21400, loss = 0.72 (250.6 examples/sec; 0.511 sec/batch)
2018-03-22 14:33:24.720498: step 21410, loss = 0.93 (280.5 examples/sec; 0.456 sec/batch)
2018-03-22 14:33:30.359049: step 21420, loss = 0.92 (227.0 examples/sec; 0.564 sec/batch)
2018-03-22 14:33:35.211350: step 21430, loss = 0.68 (263.8 examples/sec; 0.485 sec/batch)
2018-03-22 14:33:39.957693: step 21440, loss = 0.83 (269.7 examples/sec; 0.475 sec/batch)
2018-03-22 14:33:44.475874: step 21450, loss = 0.74 (283.3 examples/sec; 0.452 sec/batch)
2018-03-22 14:33:49.322702: step 21460, loss = 0.80 (264.1 examples/sec; 0.485 sec/batch)
2018-03-22 14:33:54.145123: step 21470, loss = 0.73 (265.4 examples/sec; 0.482 sec/batch)
2018-03-22 14:33:58.760362: step 21480, loss = 1.11 (277.3 examples/sec; 0.462 sec/batch)
2018-03-22 14:34:03.726581: step 21490, loss = 0.80 (257.7 examples/sec; 0.497 sec/batch)
2018-03-22 14:34:09.149646: step 21500, loss = 0.97 (236.0 examples/sec; 0.542 sec/batch)
2018-03-22 14:34:14.445059: step 21510, loss = 0.84 (241.7 examples/sec; 0.530 sec/batch)
2018-03-22 14:34:19.868406: step 21520, loss = 0.76 (236.0 examples/sec; 0.542 sec/batch)
2018-03-22 14:34:24.210582: step 21530, loss = 0.85 (294.8 examples/sec; 0.434 sec/batch)
2018-03-22 14:34:29.395345: step 21540, loss = 0.90 (246.9 examples/sec; 0.518 sec/batch)
2018-03-22 14:34:34.184944: step 21550, loss = 0.89 (267.2 examples/sec; 0.479 sec/batch)
2018-03-22 14:34:39.413291: step 21560, loss = 0.74 (244.8 examples/sec; 0.523 sec/batch)
2018-03-22 14:34:44.219225: step 21570, loss = 0.58 (266.3 examples/sec; 0.481 sec/batch)
2018-03-22 14:34:49.153964: step 21580, loss = 0.73 (259.4 examples/sec; 0.493 sec/batch)
2018-03-22 14:34:53.920476: step 21590, loss = 0.96 (268.5 examples/sec; 0.477 sec/batch)
2018-03-22 14:34:59.097488: step 21600, loss = 0.79 (247.2 examples/sec; 0.518 sec/batch)
2018-03-22 14:35:04.187397: step 21610, loss = 0.91 (251.5 examples/sec; 0.509 sec/batch)
2018-03-22 14:35:09.192451: step 21620, loss = 0.75 (255.7 examples/sec; 0.501 sec/batch)
2018-03-22 14:35:14.250419: step 21630, loss = 0.79 (253.1 examples/sec; 0.506 sec/batch)
2018-03-22 14:35:19.914408: step 21640, loss = 0.64 (226.0 examples/sec; 0.566 sec/batch)
2018-03-22 14:35:25.065386: step 21650, loss = 0.83 (248.5 examples/sec; 0.515 sec/batch)
2018-03-22 14:35:30.023436: step 21660, loss = 1.00 (258.2 examples/sec; 0.496 sec/batch)
2018-03-22 14:35:34.921234: step 21670, loss = 0.91 (261.3 examples/sec; 0.490 sec/batch)
2018-03-22 14:35:40.411420: step 21680, loss = 0.75 (233.1 examples/sec; 0.549 sec/batch)
2018-03-22 14:35:45.561623: step 21690, loss = 0.81 (248.5 examples/sec; 0.515 sec/batch)
2018-03-22 14:35:50.536692: step 21700, loss = 0.92 (257.3 examples/sec; 0.498 sec/batch)
2018-03-22 14:35:55.388466: step 21710, loss = 0.96 (263.8 examples/sec; 0.485 sec/batch)
2018-03-22 14:36:00.672417: step 21720, loss = 0.71 (242.2 examples/sec; 0.528 sec/batch)
2018-03-22 14:36:05.584322: step 21730, loss = 0.72 (260.6 examples/sec; 0.491 sec/batch)
2018-03-22 14:36:10.666386: step 21740, loss = 0.95 (251.9 examples/sec; 0.508 sec/batch)
2018-03-22 14:36:16.028496: step 21750, loss = 0.92 (238.7 examples/sec; 0.536 sec/batch)
2018-03-22 14:36:21.097421: step 21760, loss = 0.87 (252.5 examples/sec; 0.507 sec/batch)
2018-03-22 14:36:26.463409: step 21770, loss = 0.73 (238.5 examples/sec; 0.537 sec/batch)
2018-03-22 14:36:31.440365: step 21780, loss = 0.79 (257.2 examples/sec; 0.498 sec/batch)
2018-03-22 14:36:36.684406: step 21790, loss = 0.87 (244.1 examples/sec; 0.524 sec/batch)
2018-03-22 14:36:42.133221: step 21800, loss = 0.77 (234.9 examples/sec; 0.545 sec/batch)
2018-03-22 14:36:47.297348: step 21810, loss = 0.74 (247.9 examples/sec; 0.516 sec/batch)
2018-03-22 14:36:52.307073: step 21820, loss = 1.01 (255.5 examples/sec; 0.501 sec/batch)
2018-03-22 14:36:57.334365: step 21830, loss = 0.68 (254.6 examples/sec; 0.503 sec/batch)
2018-03-22 14:37:02.940590: step 21840, loss = 0.70 (228.3 examples/sec; 0.561 sec/batch)
2018-03-22 14:37:07.805422: step 21850, loss = 0.82 (263.1 examples/sec; 0.486 sec/batch)
2018-03-22 14:37:12.978460: step 21860, loss = 0.94 (247.4 examples/sec; 0.517 sec/batch)
2018-03-22 14:37:18.147435: step 21870, loss = 0.69 (247.6 examples/sec; 0.517 sec/batch)
2018-03-22 14:37:23.390100: step 21880, loss = 0.72 (244.2 examples/sec; 0.524 sec/batch)
2018-03-22 14:37:28.220409: step 21890, loss = 0.78 (265.0 examples/sec; 0.483 sec/batch)
2018-03-22 14:37:33.322885: step 21900, loss = 0.68 (250.9 examples/sec; 0.510 sec/batch)
2018-03-22 14:37:38.282422: step 21910, loss = 0.81 (258.1 examples/sec; 0.496 sec/batch)
2018-03-22 14:37:43.226754: step 21920, loss = 0.96 (258.9 examples/sec; 0.494 sec/batch)
2018-03-22 14:37:47.733849: step 21930, loss = 0.71 (284.0 examples/sec; 0.451 sec/batch)
2018-03-22 14:37:52.898338: step 21940, loss = 0.73 (247.8 examples/sec; 0.516 sec/batch)
2018-03-22 14:37:58.205368: step 21950, loss = 0.82 (241.2 examples/sec; 0.531 sec/batch)
2018-03-22 14:38:03.586671: step 21960, loss = 0.80 (237.9 examples/sec; 0.538 sec/batch)
2018-03-22 14:38:08.312382: step 21970, loss = 0.92 (270.9 examples/sec; 0.473 sec/batch)
2018-03-22 14:38:13.008469: step 21980, loss = 0.71 (272.6 examples/sec; 0.470 sec/batch)
2018-03-22 14:38:18.090395: step 21990, loss = 0.80 (251.9 examples/sec; 0.508 sec/batch)
2018-03-22 14:38:23.326818: step 22000, loss = 0.70 (244.4 examples/sec; 0.524 sec/batch)
2018-03-22 14:38:28.755372: step 22010, loss = 0.81 (235.8 examples/sec; 0.543 sec/batch)
2018-03-22 14:38:34.101441: step 22020, loss = 0.72 (239.4 examples/sec; 0.535 sec/batch)
2018-03-22 14:38:38.902280: step 22030, loss = 0.73 (266.6 examples/sec; 0.480 sec/batch)
2018-03-22 14:38:43.911099: step 22040, loss = 0.82 (255.5 examples/sec; 0.501 sec/batch)
2018-03-22 14:38:49.113413: step 22050, loss = 0.74 (246.0 examples/sec; 0.520 sec/batch)
2018-03-22 14:38:54.551617: step 22060, loss = 0.74 (235.4 examples/sec; 0.544 sec/batch)
2018-03-22 14:38:59.162961: step 22070, loss = 0.76 (277.6 examples/sec; 0.461 sec/batch)
2018-03-22 14:39:03.775456: step 22080, loss = 0.93 (277.5 examples/sec; 0.461 sec/batch)
2018-03-22 14:39:08.825997: step 22090, loss = 0.78 (253.4 examples/sec; 0.505 sec/batch)
2018-03-22 14:39:14.127395: step 22100, loss = 0.77 (241.4 examples/sec; 0.530 sec/batch)
2018-03-22 14:39:19.344375: step 22110, loss = 0.78 (245.4 examples/sec; 0.522 sec/batch)
2018-03-22 14:39:24.198347: step 22120, loss = 0.75 (263.7 examples/sec; 0.485 sec/batch)
2018-03-22 14:39:29.667469: step 22130, loss = 0.87 (234.0 examples/sec; 0.547 sec/batch)
2018-03-22 14:39:34.922600: step 22140, loss = 0.72 (243.6 examples/sec; 0.526 sec/batch)
2018-03-22 14:39:39.307167: step 22150, loss = 0.72 (291.9 examples/sec; 0.438 sec/batch)
2018-03-22 14:39:43.889380: step 22160, loss = 0.74 (279.3 examples/sec; 0.458 sec/batch)
2018-03-22 14:39:48.807939: step 22170, loss = 0.75 (260.2 examples/sec; 0.492 sec/batch)
2018-03-22 14:39:54.232447: step 22180, loss = 1.06 (236.0 examples/sec; 0.542 sec/batch)
2018-03-22 14:39:58.990433: step 22190, loss = 0.74 (269.0 examples/sec; 0.476 sec/batch)
2018-03-22 14:40:04.090015: step 22200, loss = 0.72 (251.0 examples/sec; 0.510 sec/batch)
2018-03-22 14:40:09.111520: step 22210, loss = 0.73 (254.9 examples/sec; 0.502 sec/batch)
2018-03-22 14:40:14.151407: step 22220, loss = 0.84 (254.0 examples/sec; 0.504 sec/batch)
2018-03-22 14:40:19.367324: step 22230, loss = 1.01 (245.4 examples/sec; 0.522 sec/batch)
2018-03-22 14:40:23.951522: step 22240, loss = 0.81 (279.2 examples/sec; 0.458 sec/batch)
2018-03-22 14:40:29.411429: step 22250, loss = 0.82 (234.4 examples/sec; 0.546 sec/batch)
2018-03-22 14:40:34.747386: step 22260, loss = 0.78 (239.9 examples/sec; 0.534 sec/batch)
2018-03-22 14:40:39.844235: step 22270, loss = 0.71 (251.1 examples/sec; 0.510 sec/batch)
2018-03-22 14:40:44.834992: step 22280, loss = 0.81 (256.5 examples/sec; 0.499 sec/batch)
2018-03-22 14:40:49.933460: step 22290, loss = 0.82 (251.1 examples/sec; 0.510 sec/batch)
2018-03-22 14:40:55.130561: step 22300, loss = 0.96 (246.3 examples/sec; 0.520 sec/batch)
2018-03-22 14:41:00.000440: step 22310, loss = 0.91 (262.8 examples/sec; 0.487 sec/batch)
2018-03-22 14:41:04.714183: step 22320, loss = 0.74 (271.5 examples/sec; 0.471 sec/batch)
2018-03-22 14:41:10.297398: step 22330, loss = 0.86 (229.3 examples/sec; 0.558 sec/batch)
2018-03-22 14:41:15.225473: step 22340, loss = 0.84 (259.7 examples/sec; 0.493 sec/batch)
2018-03-22 14:41:20.432462: step 22350, loss = 0.64 (245.8 examples/sec; 0.521 sec/batch)
2018-03-22 14:41:24.970293: step 22360, loss = 0.80 (282.1 examples/sec; 0.454 sec/batch)
2018-03-22 14:41:30.195406: step 22370, loss = 0.88 (245.0 examples/sec; 0.523 sec/batch)
2018-03-22 14:41:34.976429: step 22380, loss = 0.85 (267.7 examples/sec; 0.478 sec/batch)
2018-03-22 14:41:39.678890: step 22390, loss = 0.64 (272.2 examples/sec; 0.470 sec/batch)
2018-03-22 14:41:44.603171: step 22400, loss = 0.79 (259.9 examples/sec; 0.492 sec/batch)
2018-03-22 14:41:49.744495: step 22410, loss = 0.71 (249.0 examples/sec; 0.514 sec/batch)
2018-03-22 14:41:55.157451: step 22420, loss = 0.77 (236.5 examples/sec; 0.541 sec/batch)
2018-03-22 14:42:00.172358: step 22430, loss = 0.76 (255.2 examples/sec; 0.501 sec/batch)
2018-03-22 14:42:05.094606: step 22440, loss = 0.78 (260.0 examples/sec; 0.492 sec/batch)
2018-03-22 14:42:10.268367: step 22450, loss = 0.92 (247.4 examples/sec; 0.517 sec/batch)
2018-03-22 14:42:15.380366: step 22460, loss = 0.81 (250.4 examples/sec; 0.511 sec/batch)
2018-03-22 14:42:20.186403: step 22470, loss = 0.89 (266.3 examples/sec; 0.481 sec/batch)
2018-03-22 14:42:25.198380: step 22480, loss = 0.83 (255.4 examples/sec; 0.501 sec/batch)
2018-03-22 14:42:30.782234: step 22490, loss = 0.80 (229.2 examples/sec; 0.558 sec/batch)
2018-03-22 14:42:35.718697: step 22500, loss = 0.78 (259.3 examples/sec; 0.494 sec/batch)
2018-03-22 14:42:40.555394: step 22510, loss = 0.82 (264.6 examples/sec; 0.484 sec/batch)
2018-03-22 14:42:45.733454: step 22520, loss = 0.94 (247.2 examples/sec; 0.518 sec/batch)
2018-03-22 14:42:50.829452: step 22530, loss = 0.85 (251.2 examples/sec; 0.510 sec/batch)
2018-03-22 14:42:55.805571: step 22540, loss = 1.02 (257.2 examples/sec; 0.498 sec/batch)
2018-03-22 14:43:00.710421: step 22550, loss = 0.87 (261.0 examples/sec; 0.490 sec/batch)
2018-03-22 14:43:05.590719: step 22560, loss = 0.77 (262.3 examples/sec; 0.488 sec/batch)
2018-03-22 14:43:10.983911: step 22570, loss = 0.70 (237.3 examples/sec; 0.539 sec/batch)
2018-03-22 14:43:15.754412: step 22580, loss = 0.92 (268.3 examples/sec; 0.477 sec/batch)
2018-03-22 14:43:21.476369: step 22590, loss = 0.97 (223.7 examples/sec; 0.572 sec/batch)
2018-03-22 14:43:26.767856: step 22600, loss = 0.75 (241.9 examples/sec; 0.529 sec/batch)
2018-03-22 14:43:31.800776: step 22610, loss = 0.77 (254.3 examples/sec; 0.503 sec/batch)
2018-03-22 14:43:36.509167: step 22620, loss = 0.84 (271.9 examples/sec; 0.471 sec/batch)
2018-03-22 14:43:41.026339: step 22630, loss = 0.61 (283.4 examples/sec; 0.452 sec/batch)
2018-03-22 14:43:45.785407: step 22640, loss = 0.77 (269.0 examples/sec; 0.476 sec/batch)
2018-03-22 14:43:51.237642: step 22650, loss = 0.90 (234.8 examples/sec; 0.545 sec/batch)
2018-03-22 14:43:55.950378: step 22660, loss = 0.83 (271.6 examples/sec; 0.471 sec/batch)
2018-03-22 14:44:01.081657: step 22670, loss = 0.94 (249.5 examples/sec; 0.513 sec/batch)
2018-03-22 14:44:06.307275: step 22680, loss = 0.77 (244.9 examples/sec; 0.523 sec/batch)
2018-03-22 14:44:11.656829: step 22690, loss = 0.78 (239.3 examples/sec; 0.535 sec/batch)
2018-03-22 14:44:16.899195: step 22700, loss = 0.74 (244.2 examples/sec; 0.524 sec/batch)
2018-03-22 14:44:22.172391: step 22710, loss = 0.94 (242.7 examples/sec; 0.527 sec/batch)
2018-03-22 14:44:27.659420: step 22720, loss = 0.95 (233.3 examples/sec; 0.549 sec/batch)
2018-03-22 14:44:33.145442: step 22730, loss = 0.84 (233.3 examples/sec; 0.549 sec/batch)
2018-03-22 14:44:37.828302: step 22740, loss = 0.91 (273.3 examples/sec; 0.468 sec/batch)
2018-03-22 14:44:42.708443: step 22750, loss = 0.81 (262.3 examples/sec; 0.488 sec/batch)
2018-03-22 14:44:47.365177: step 22760, loss = 0.93 (274.9 examples/sec; 0.466 sec/batch)
2018-03-22 14:44:52.696503: step 22770, loss = 0.74 (240.1 examples/sec; 0.533 sec/batch)
2018-03-22 14:44:57.780411: step 22780, loss = 0.84 (251.8 examples/sec; 0.508 sec/batch)
2018-03-22 14:45:02.769365: step 22790, loss = 0.74 (256.6 examples/sec; 0.499 sec/batch)
2018-03-22 14:45:07.834100: step 22800, loss = 0.64 (252.7 examples/sec; 0.506 sec/batch)
2018-03-22 14:45:12.829582: step 22810, loss = 0.88 (256.2 examples/sec; 0.500 sec/batch)
2018-03-22 14:45:18.010700: step 22820, loss = 0.89 (247.1 examples/sec; 0.518 sec/batch)
2018-03-22 14:45:23.012412: step 22830, loss = 0.97 (255.9 examples/sec; 0.500 sec/batch)
2018-03-22 14:45:27.895394: step 22840, loss = 0.77 (262.1 examples/sec; 0.488 sec/batch)
2018-03-22 14:45:32.438918: step 22850, loss = 0.81 (281.7 examples/sec; 0.454 sec/batch)
2018-03-22 14:45:37.082450: step 22860, loss = 0.69 (275.7 examples/sec; 0.464 sec/batch)
2018-03-22 14:45:42.485424: step 22870, loss = 0.80 (236.9 examples/sec; 0.540 sec/batch)
2018-03-22 14:45:47.584745: step 22880, loss = 0.81 (251.0 examples/sec; 0.510 sec/batch)
2018-03-22 14:45:52.855324: step 22890, loss = 0.91 (242.9 examples/sec; 0.527 sec/batch)
2018-03-22 14:45:57.857363: step 22900, loss = 0.82 (255.9 examples/sec; 0.500 sec/batch)
2018-03-22 14:46:02.433477: step 22910, loss = 0.86 (279.7 examples/sec; 0.458 sec/batch)
2018-03-22 14:46:07.154591: step 22920, loss = 0.74 (271.1 examples/sec; 0.472 sec/batch)
2018-03-22 14:46:12.320349: step 22930, loss = 0.77 (247.8 examples/sec; 0.517 sec/batch)
2018-03-22 14:46:17.550563: step 22940, loss = 0.92 (244.7 examples/sec; 0.523 sec/batch)
2018-03-22 14:46:22.589317: step 22950, loss = 0.87 (254.0 examples/sec; 0.504 sec/batch)
2018-03-22 14:46:27.500827: step 22960, loss = 0.84 (260.6 examples/sec; 0.491 sec/batch)
2018-03-22 14:46:32.037751: step 22970, loss = 0.69 (282.1 examples/sec; 0.454 sec/batch)
2018-03-22 14:46:37.378434: step 22980, loss = 0.77 (239.7 examples/sec; 0.534 sec/batch)
2018-03-22 14:46:42.431414: step 22990, loss = 0.77 (253.3 examples/sec; 0.505 sec/batch)
2018-03-22 14:46:47.707791: step 23000, loss = 0.65 (242.6 examples/sec; 0.528 sec/batch)
2018-03-22 14:46:52.940373: step 23010, loss = 0.83 (244.6 examples/sec; 0.523 sec/batch)
2018-03-22 14:46:58.196319: step 23020, loss = 0.86 (243.5 examples/sec; 0.526 sec/batch)
2018-03-22 14:47:03.384256: step 23030, loss = 0.67 (246.7 examples/sec; 0.519 sec/batch)
2018-03-22 14:47:08.474460: step 23040, loss = 0.80 (251.5 examples/sec; 0.509 sec/batch)
2018-03-22 14:47:13.224407: step 23050, loss = 0.65 (269.5 examples/sec; 0.475 sec/batch)
2018-03-22 14:47:17.818438: step 23060, loss = 0.80 (278.6 examples/sec; 0.459 sec/batch)
2018-03-22 14:47:22.888440: step 23070, loss = 0.64 (252.5 examples/sec; 0.507 sec/batch)
2018-03-22 14:47:27.491404: step 23080, loss = 0.72 (278.1 examples/sec; 0.460 sec/batch)
2018-03-22 14:47:32.218192: step 23090, loss = 0.81 (270.8 examples/sec; 0.473 sec/batch)
2018-03-22 14:47:37.260491: step 23100, loss = 0.81 (253.9 examples/sec; 0.504 sec/batch)
2018-03-22 14:47:42.217045: step 23110, loss = 0.77 (258.2 examples/sec; 0.496 sec/batch)
2018-03-22 14:47:47.361458: step 23120, loss = 0.94 (248.8 examples/sec; 0.514 sec/batch)
2018-03-22 14:47:52.538184: step 23130, loss = 0.65 (247.3 examples/sec; 0.518 sec/batch)
2018-03-22 14:47:57.517519: step 23140, loss = 0.85 (257.1 examples/sec; 0.498 sec/batch)
2018-03-22 14:48:02.944483: step 23150, loss = 0.85 (235.9 examples/sec; 0.543 sec/batch)
2018-03-22 14:48:07.385654: step 23160, loss = 0.84 (288.2 examples/sec; 0.444 sec/batch)
2018-03-22 14:48:12.379411: step 23170, loss = 0.68 (256.3 examples/sec; 0.499 sec/batch)
2018-03-22 14:48:17.548202: step 23180, loss = 0.93 (247.6 examples/sec; 0.517 sec/batch)
2018-03-22 14:48:23.532301: step 23190, loss = 0.76 (213.9 examples/sec; 0.598 sec/batch)
2018-03-22 14:48:28.909180: step 23200, loss = 0.64 (238.1 examples/sec; 0.538 sec/batch)
2018-03-22 14:48:33.246596: step 23210, loss = 0.78 (295.1 examples/sec; 0.434 sec/batch)
2018-03-22 14:48:38.151447: step 23220, loss = 0.72 (261.0 examples/sec; 0.490 sec/batch)
2018-03-22 14:48:43.493402: step 23230, loss = 0.75 (239.6 examples/sec; 0.534 sec/batch)
2018-03-22 14:48:48.239366: step 23240, loss = 0.89 (269.7 examples/sec; 0.475 sec/batch)
2018-03-22 14:48:53.336340: step 23250, loss = 0.97 (251.1 examples/sec; 0.510 sec/batch)
2018-03-22 14:48:58.646679: step 23260, loss = 0.82 (241.0 examples/sec; 0.531 sec/batch)
2018-03-22 14:49:03.890911: step 23270, loss = 0.76 (244.1 examples/sec; 0.524 sec/batch)
2018-03-22 14:49:08.406417: step 23280, loss = 0.83 (283.5 examples/sec; 0.452 sec/batch)
2018-03-22 14:49:13.295137: step 23290, loss = 0.70 (261.8 examples/sec; 0.489 sec/batch)
2018-03-22 14:49:18.226815: step 23300, loss = 0.82 (259.5 examples/sec; 0.493 sec/batch)
2018-03-22 14:49:23.491658: step 23310, loss = 0.84 (243.1 examples/sec; 0.526 sec/batch)
2018-03-22 14:49:27.895895: step 23320, loss = 0.88 (290.6 examples/sec; 0.440 sec/batch)
2018-03-22 14:49:33.081660: step 23330, loss = 0.73 (246.8 examples/sec; 0.519 sec/batch)
2018-03-22 14:49:38.243532: step 23340, loss = 0.64 (248.0 examples/sec; 0.516 sec/batch)
2018-03-22 14:49:43.873446: step 23350, loss = 1.20 (227.4 examples/sec; 0.563 sec/batch)
2018-03-22 14:49:48.532775: step 23360, loss = 0.73 (274.7 examples/sec; 0.466 sec/batch)
2018-03-22 14:49:53.561986: step 23370, loss = 0.72 (254.5 examples/sec; 0.503 sec/batch)
2018-03-22 14:49:58.347446: step 23380, loss = 0.83 (267.5 examples/sec; 0.479 sec/batch)
2018-03-22 14:50:03.834335: step 23390, loss = 0.78 (233.3 examples/sec; 0.549 sec/batch)
2018-03-22 14:50:09.041335: step 23400, loss = 0.97 (245.8 examples/sec; 0.521 sec/batch)
2018-03-22 14:50:13.922190: step 23410, loss = 0.83 (262.2 examples/sec; 0.488 sec/batch)
2018-03-22 14:50:18.889436: step 23420, loss = 0.84 (257.7 examples/sec; 0.497 sec/batch)
2018-03-22 14:50:23.730370: step 23430, loss = 0.89 (264.4 examples/sec; 0.484 sec/batch)
2018-03-22 14:50:28.643387: step 23440, loss = 0.76 (260.5 examples/sec; 0.491 sec/batch)
2018-03-22 14:50:33.459170: step 23450, loss = 0.81 (265.8 examples/sec; 0.482 sec/batch)
2018-03-22 14:50:38.374506: step 23460, loss = 0.84 (260.4 examples/sec; 0.492 sec/batch)
2018-03-22 14:50:43.357438: step 23470, loss = 0.82 (256.9 examples/sec; 0.498 sec/batch)
2018-03-22 14:50:48.495369: step 23480, loss = 0.65 (249.1 examples/sec; 0.514 sec/batch)
2018-03-22 14:50:53.808937: step 23490, loss = 0.85 (240.9 examples/sec; 0.531 sec/batch)
2018-03-22 14:50:58.690484: step 23500, loss = 0.82 (262.2 examples/sec; 0.488 sec/batch)
2018-03-22 14:51:03.715445: step 23510, loss = 0.65 (254.7 examples/sec; 0.502 sec/batch)
2018-03-22 14:51:08.442378: step 23520, loss = 0.92 (270.8 examples/sec; 0.473 sec/batch)
2018-03-22 14:51:14.359271: step 23530, loss = 0.91 (216.3 examples/sec; 0.592 sec/batch)
2018-03-22 14:51:19.140225: step 23540, loss = 0.80 (267.7 examples/sec; 0.478 sec/batch)
2018-03-22 14:51:23.425050: step 23550, loss = 0.77 (298.7 examples/sec; 0.428 sec/batch)
2018-03-22 14:51:28.295422: step 23560, loss = 0.75 (262.8 examples/sec; 0.487 sec/batch)
2018-03-22 14:51:32.969315: step 23570, loss = 0.82 (273.9 examples/sec; 0.467 sec/batch)
2018-03-22 14:51:38.093509: step 23580, loss = 0.77 (249.8 examples/sec; 0.512 sec/batch)
2018-03-22 14:51:43.020399: step 23590, loss = 0.96 (259.8 examples/sec; 0.493 sec/batch)
2018-03-22 14:51:48.265401: step 23600, loss = 0.75 (244.0 examples/sec; 0.524 sec/batch)
2018-03-22 14:51:53.241416: step 23610, loss = 0.69 (257.2 examples/sec; 0.498 sec/batch)
2018-03-22 14:51:58.509749: step 23620, loss = 0.86 (243.0 examples/sec; 0.527 sec/batch)
2018-03-22 14:52:04.023549: step 23630, loss = 1.01 (232.1 examples/sec; 0.551 sec/batch)
2018-03-22 14:52:09.013856: step 23640, loss = 0.74 (256.5 examples/sec; 0.499 sec/batch)
2018-03-22 14:52:14.150389: step 23650, loss = 1.03 (249.2 examples/sec; 0.514 sec/batch)
2018-03-22 14:52:19.481422: step 23660, loss = 0.69 (240.1 examples/sec; 0.533 sec/batch)
2018-03-22 14:52:24.801851: step 23670, loss = 0.97 (240.6 examples/sec; 0.532 sec/batch)
2018-03-22 14:52:29.553390: step 23680, loss = 0.95 (269.4 examples/sec; 0.475 sec/batch)
2018-03-22 14:52:34.717476: step 23690, loss = 0.87 (247.9 examples/sec; 0.516 sec/batch)
2018-03-22 14:52:40.619435: step 23700, loss = 0.78 (216.9 examples/sec; 0.590 sec/batch)
2018-03-22 14:52:45.851920: step 23710, loss = 0.61 (244.6 examples/sec; 0.523 sec/batch)
2018-03-22 14:52:50.641151: step 23720, loss = 0.95 (267.3 examples/sec; 0.479 sec/batch)
2018-03-22 14:52:55.242384: step 23730, loss = 0.70 (278.2 examples/sec; 0.460 sec/batch)
2018-03-22 14:53:00.558090: step 23740, loss = 1.08 (240.8 examples/sec; 0.532 sec/batch)
2018-03-22 14:53:05.506436: step 23750, loss = 0.84 (258.7 examples/sec; 0.495 sec/batch)
2018-03-22 14:53:10.583336: step 23760, loss = 0.78 (252.1 examples/sec; 0.508 sec/batch)
2018-03-22 14:53:15.137356: step 23770, loss = 0.75 (281.1 examples/sec; 0.455 sec/batch)
2018-03-22 14:53:19.464355: step 23780, loss = 0.68 (295.8 examples/sec; 0.433 sec/batch)
2018-03-22 14:53:24.325634: step 23790, loss = 0.68 (263.3 examples/sec; 0.486 sec/batch)
2018-03-22 14:53:29.313135: step 23800, loss = 0.88 (256.6 examples/sec; 0.499 sec/batch)
2018-03-22 14:53:33.886324: step 23810, loss = 0.73 (279.9 examples/sec; 0.457 sec/batch)
2018-03-22 14:53:39.226346: step 23820, loss = 0.77 (239.7 examples/sec; 0.534 sec/batch)
2018-03-22 14:53:44.039421: step 23830, loss = 0.84 (265.9 examples/sec; 0.481 sec/batch)
2018-03-22 14:53:48.623237: step 23840, loss = 0.81 (279.2 examples/sec; 0.458 sec/batch)
2018-03-22 14:53:53.798235: step 23850, loss = 0.77 (247.3 examples/sec; 0.517 sec/batch)
2018-03-22 14:53:58.727685: step 23860, loss = 0.84 (259.7 examples/sec; 0.493 sec/batch)
2018-03-22 14:54:03.806413: step 23870, loss = 0.75 (252.0 examples/sec; 0.508 sec/batch)
2018-03-22 14:54:09.222321: step 23880, loss = 0.91 (236.3 examples/sec; 0.542 sec/batch)
2018-03-22 14:54:14.384638: step 23890, loss = 0.87 (248.0 examples/sec; 0.516 sec/batch)
2018-03-22 14:54:19.412044: step 23900, loss = 0.73 (254.6 examples/sec; 0.503 sec/batch)
2018-03-22 14:54:24.325395: step 23910, loss = 0.85 (260.5 examples/sec; 0.491 sec/batch)
2018-03-22 14:54:29.702397: step 23920, loss = 0.86 (238.1 examples/sec; 0.538 sec/batch)
2018-03-22 14:54:34.998444: step 23930, loss = 0.85 (241.7 examples/sec; 0.530 sec/batch)
2018-03-22 14:54:39.997959: step 23940, loss = 0.88 (256.0 examples/sec; 0.500 sec/batch)
2018-03-22 14:54:44.551403: step 23950, loss = 0.80 (281.1 examples/sec; 0.455 sec/batch)
2018-03-22 14:54:49.841362: step 23960, loss = 0.75 (242.0 examples/sec; 0.529 sec/batch)
2018-03-22 14:54:54.567395: step 23970, loss = 0.82 (270.8 examples/sec; 0.473 sec/batch)
2018-03-22 14:55:00.168049: step 23980, loss = 0.85 (228.5 examples/sec; 0.560 sec/batch)
2018-03-22 14:55:05.385774: step 23990, loss = 0.83 (245.3 examples/sec; 0.522 sec/batch)
2018-03-22 14:55:10.148302: step 24000, loss = 0.73 (268.8 examples/sec; 0.476 sec/batch)
2018-03-22 14:55:14.868942: step 24010, loss = 0.87 (271.2 examples/sec; 0.472 sec/batch)
2018-03-22 14:55:19.478155: step 24020, loss = 0.81 (277.7 examples/sec; 0.461 sec/batch)
2018-03-22 14:55:24.750457: step 24030, loss = 0.80 (242.8 examples/sec; 0.527 sec/batch)
2018-03-22 14:55:29.981524: step 24040, loss = 0.82 (244.7 examples/sec; 0.523 sec/batch)
2018-03-22 14:55:34.805414: step 24050, loss = 0.84 (265.3 examples/sec; 0.482 sec/batch)
2018-03-22 14:55:40.579655: step 24060, loss = 0.84 (221.7 examples/sec; 0.577 sec/batch)
2018-03-22 14:55:45.918823: step 24070, loss = 0.79 (239.7 examples/sec; 0.534 sec/batch)
2018-03-22 14:55:51.111354: step 24080, loss = 0.86 (246.5 examples/sec; 0.519 sec/batch)
2018-03-22 14:55:56.523585: step 24090, loss = 0.81 (236.5 examples/sec; 0.541 sec/batch)
2018-03-22 14:56:02.076216: step 24100, loss = 0.87 (230.5 examples/sec; 0.555 sec/batch)
2018-03-22 14:56:07.375418: step 24110, loss = 0.76 (241.5 examples/sec; 0.530 sec/batch)
2018-03-22 14:56:12.532241: step 24120, loss = 1.00 (248.2 examples/sec; 0.516 sec/batch)
2018-03-22 14:56:17.404845: step 24130, loss = 0.71 (262.7 examples/sec; 0.487 sec/batch)
2018-03-22 14:56:22.581920: step 24140, loss = 0.86 (247.2 examples/sec; 0.518 sec/batch)
2018-03-22 14:56:28.020424: step 24150, loss = 0.73 (235.4 examples/sec; 0.544 sec/batch)
2018-03-22 14:56:33.244773: step 24160, loss = 0.89 (245.0 examples/sec; 0.522 sec/batch)
2018-03-22 14:56:37.979001: step 24170, loss = 0.74 (270.4 examples/sec; 0.473 sec/batch)
2018-03-22 14:56:43.082666: step 24180, loss = 0.86 (250.8 examples/sec; 0.510 sec/batch)
2018-03-22 14:56:48.190431: step 24190, loss = 0.83 (250.6 examples/sec; 0.511 sec/batch)
2018-03-22 14:56:53.921073: step 24200, loss = 0.70 (223.4 examples/sec; 0.573 sec/batch)
2018-03-22 14:56:58.479162: step 24210, loss = 0.82 (280.8 examples/sec; 0.456 sec/batch)
2018-03-22 14:57:03.208852: step 24220, loss = 0.81 (270.6 examples/sec; 0.473 sec/batch)
2018-03-22 14:57:08.056424: step 24230, loss = 0.76 (264.1 examples/sec; 0.485 sec/batch)
2018-03-22 14:57:13.357019: step 24240, loss = 0.81 (241.5 examples/sec; 0.530 sec/batch)
2018-03-22 14:57:18.014382: step 24250, loss = 0.72 (274.8 examples/sec; 0.466 sec/batch)
2018-03-22 14:57:23.203394: step 24260, loss = 0.81 (246.7 examples/sec; 0.519 sec/batch)
2018-03-22 14:57:28.398823: step 24270, loss = 0.65 (246.4 examples/sec; 0.520 sec/batch)
2018-03-22 14:57:33.202386: step 24280, loss = 0.81 (266.5 examples/sec; 0.480 sec/batch)
2018-03-22 14:57:38.090438: step 24290, loss = 0.77 (261.9 examples/sec; 0.489 sec/batch)
2018-03-22 14:57:43.673815: step 24300, loss = 0.84 (229.3 examples/sec; 0.558 sec/batch)
2018-03-22 14:57:48.632428: step 24310, loss = 0.74 (258.1 examples/sec; 0.496 sec/batch)
2018-03-22 14:57:53.734520: step 24320, loss = 0.77 (250.9 examples/sec; 0.510 sec/batch)
2018-03-22 14:57:58.628549: step 24330, loss = 0.80 (261.5 examples/sec; 0.489 sec/batch)
2018-03-22 14:58:03.347437: step 24340, loss = 0.74 (271.3 examples/sec; 0.472 sec/batch)
2018-03-22 14:58:08.746409: step 24350, loss = 0.98 (237.1 examples/sec; 0.540 sec/batch)
2018-03-22 14:58:14.411708: step 24360, loss = 0.88 (225.9 examples/sec; 0.567 sec/batch)
2018-03-22 14:58:19.589914: step 24370, loss = 0.77 (247.2 examples/sec; 0.518 sec/batch)
2018-03-22 14:58:24.231691: step 24380, loss = 1.08 (275.8 examples/sec; 0.464 sec/batch)
2018-03-22 14:58:29.571468: step 24390, loss = 0.70 (239.7 examples/sec; 0.534 sec/batch)
2018-03-22 14:58:35.005495: step 24400, loss = 0.77 (235.6 examples/sec; 0.543 sec/batch)
2018-03-22 14:58:39.588501: step 24410, loss = 0.85 (279.3 examples/sec; 0.458 sec/batch)
2018-03-22 14:58:44.495416: step 24420, loss = 0.96 (260.9 examples/sec; 0.491 sec/batch)
2018-03-22 14:58:49.751401: step 24430, loss = 0.64 (243.5 examples/sec; 0.526 sec/batch)
2018-03-22 14:58:55.161176: step 24440, loss = 0.96 (236.6 examples/sec; 0.541 sec/batch)
2018-03-22 14:59:00.090462: step 24450, loss = 0.75 (259.7 examples/sec; 0.493 sec/batch)
2018-03-22 14:59:04.882369: step 24460, loss = 0.85 (267.1 examples/sec; 0.479 sec/batch)
2018-03-22 14:59:10.207414: step 24470, loss = 0.77 (240.4 examples/sec; 0.533 sec/batch)
2018-03-22 14:59:15.770387: step 24480, loss = 0.75 (230.1 examples/sec; 0.556 sec/batch)
2018-03-22 14:59:20.787384: step 24490, loss = 0.84 (255.1 examples/sec; 0.502 sec/batch)
2018-03-22 14:59:25.853332: step 24500, loss = 0.73 (252.7 examples/sec; 0.507 sec/batch)
2018-03-22 14:59:31.234396: step 24510, loss = 0.68 (237.9 examples/sec; 0.538 sec/batch)
2018-03-22 14:59:36.571755: step 24520, loss = 0.90 (239.8 examples/sec; 0.534 sec/batch)
2018-03-22 14:59:41.947186: step 24530, loss = 0.99 (238.1 examples/sec; 0.538 sec/batch)
2018-03-22 14:59:46.660408: step 24540, loss = 0.83 (271.6 examples/sec; 0.471 sec/batch)
2018-03-22 14:59:51.841039: step 24550, loss = 0.68 (247.1 examples/sec; 0.518 sec/batch)
2018-03-22 14:59:57.081346: step 24560, loss = 0.90 (244.3 examples/sec; 0.524 sec/batch)
2018-03-22 15:00:01.979204: step 24570, loss = 0.63 (261.3 examples/sec; 0.490 sec/batch)
2018-03-22 15:00:07.154893: step 24580, loss = 0.94 (247.3 examples/sec; 0.518 sec/batch)
2018-03-22 15:00:12.406433: step 24590, loss = 0.71 (243.7 examples/sec; 0.525 sec/batch)
2018-03-22 15:00:17.999824: step 24600, loss = 0.81 (228.8 examples/sec; 0.559 sec/batch)
2018-03-22 15:00:22.817391: step 24610, loss = 0.89 (265.7 examples/sec; 0.482 sec/batch)
2018-03-22 15:00:27.588463: step 24620, loss = 0.86 (268.3 examples/sec; 0.477 sec/batch)
2018-03-22 15:00:32.792393: step 24630, loss = 0.88 (246.0 examples/sec; 0.520 sec/batch)
2018-03-22 15:00:38.042197: step 24640, loss = 0.80 (243.8 examples/sec; 0.525 sec/batch)
2018-03-22 15:00:42.910911: step 24650, loss = 0.80 (262.9 examples/sec; 0.487 sec/batch)
2018-03-22 15:00:47.844414: step 24660, loss = 0.83 (259.5 examples/sec; 0.493 sec/batch)
2018-03-22 15:00:53.070419: step 24670, loss = 0.74 (244.9 examples/sec; 0.523 sec/batch)
2018-03-22 15:00:58.235465: step 24680, loss = 0.83 (247.8 examples/sec; 0.517 sec/batch)
2018-03-22 15:01:03.874469: step 24690, loss = 0.80 (227.0 examples/sec; 0.564 sec/batch)
2018-03-22 15:01:09.007609: step 24700, loss = 0.90 (249.4 examples/sec; 0.513 sec/batch)
2018-03-22 15:01:14.283450: step 24710, loss = 0.80 (242.6 examples/sec; 0.528 sec/batch)
2018-03-22 15:01:18.767339: step 24720, loss = 0.69 (285.5 examples/sec; 0.448 sec/batch)
2018-03-22 15:01:24.023450: step 24730, loss = 0.79 (243.5 examples/sec; 0.526 sec/batch)
2018-03-22 15:01:29.267357: step 24740, loss = 0.81 (244.1 examples/sec; 0.524 sec/batch)
2018-03-22 15:01:34.250234: step 24750, loss = 0.76 (256.9 examples/sec; 0.498 sec/batch)
2018-03-22 15:01:39.130831: step 24760, loss = 0.91 (262.3 examples/sec; 0.488 sec/batch)
2018-03-22 15:01:44.402511: step 24770, loss = 0.81 (242.8 examples/sec; 0.527 sec/batch)
2018-03-22 15:01:49.599426: step 24780, loss = 0.99 (246.3 examples/sec; 0.520 sec/batch)
2018-03-22 15:01:54.714429: step 24790, loss = 0.83 (250.2 examples/sec; 0.512 sec/batch)
2018-03-22 15:01:59.903847: step 24800, loss = 0.94 (246.7 examples/sec; 0.519 sec/batch)
2018-03-22 15:02:04.978376: step 24810, loss = 0.72 (252.2 examples/sec; 0.507 sec/batch)
2018-03-22 15:02:09.658879: step 24820, loss = 0.79 (273.5 examples/sec; 0.468 sec/batch)
2018-03-22 15:02:14.985416: step 24830, loss = 0.84 (240.3 examples/sec; 0.533 sec/batch)
2018-03-22 15:02:20.264404: step 24840, loss = 0.72 (242.5 examples/sec; 0.528 sec/batch)
2018-03-22 15:02:24.635410: step 24850, loss = 0.74 (292.8 examples/sec; 0.437 sec/batch)
2018-03-22 15:02:29.989404: step 24860, loss = 0.73 (239.1 examples/sec; 0.535 sec/batch)
2018-03-22 15:02:34.409412: step 24870, loss = 0.73 (289.6 examples/sec; 0.442 sec/batch)
2018-03-22 15:02:39.353744: step 24880, loss = 0.79 (258.9 examples/sec; 0.494 sec/batch)
2018-03-22 15:02:44.270391: step 24890, loss = 0.81 (260.3 examples/sec; 0.492 sec/batch)
2018-03-22 15:02:49.972741: step 24900, loss = 0.80 (224.5 examples/sec; 0.570 sec/batch)
2018-03-22 15:02:55.384927: step 24910, loss = 0.81 (236.5 examples/sec; 0.541 sec/batch)
2018-03-22 15:03:00.639010: step 24920, loss = 0.84 (243.6 examples/sec; 0.525 sec/batch)
2018-03-22 15:03:05.431266: step 24930, loss = 0.65 (267.1 examples/sec; 0.479 sec/batch)
2018-03-22 15:03:11.197971: step 24940, loss = 0.76 (222.0 examples/sec; 0.577 sec/batch)
2018-03-22 15:03:16.774365: step 24950, loss = 0.84 (229.5 examples/sec; 0.558 sec/batch)
2018-03-22 15:03:21.821590: step 24960, loss = 0.63 (253.6 examples/sec; 0.505 sec/batch)
2018-03-22 15:03:26.696373: step 24970, loss = 0.88 (262.6 examples/sec; 0.488 sec/batch)
2018-03-22 15:03:31.887433: step 24980, loss = 0.80 (246.6 examples/sec; 0.519 sec/batch)
2018-03-22 15:03:36.776399: step 24990, loss = 0.86 (261.8 examples/sec; 0.489 sec/batch)
2018-03-22 15:03:42.333494: step 25000, loss = 0.81 (230.3 examples/sec; 0.556 sec/batch)
2018-03-22 15:03:47.367211: step 25010, loss = 0.73 (254.3 examples/sec; 0.503 sec/batch)
2018-03-22 15:03:52.385320: step 25020, loss = 0.83 (255.1 examples/sec; 0.502 sec/batch)
2018-03-22 15:03:57.416362: step 25030, loss = 0.70 (254.4 examples/sec; 0.503 sec/batch)
2018-03-22 15:04:02.588395: step 25040, loss = 0.92 (247.5 examples/sec; 0.517 sec/batch)
2018-03-22 15:04:08.008053: step 25050, loss = 0.94 (236.2 examples/sec; 0.542 sec/batch)
2018-03-22 15:04:13.243457: step 25060, loss = 0.83 (244.5 examples/sec; 0.524 sec/batch)
2018-03-22 15:04:18.141920: step 25070, loss = 0.79 (261.3 examples/sec; 0.490 sec/batch)
2018-03-22 15:04:22.946367: step 25080, loss = 0.76 (266.4 examples/sec; 0.480 sec/batch)
2018-03-22 15:04:27.849363: step 25090, loss = 0.68 (261.1 examples/sec; 0.490 sec/batch)
2018-03-22 15:04:32.515017: step 25100, loss = 0.68 (274.3 examples/sec; 0.467 sec/batch)
2018-03-22 15:04:37.515308: step 25110, loss = 0.69 (256.0 examples/sec; 0.500 sec/batch)
2018-03-22 15:04:42.835396: step 25120, loss = 0.73 (240.6 examples/sec; 0.532 sec/batch)
2018-03-22 15:04:48.420983: step 25130, loss = 0.80 (229.2 examples/sec; 0.559 sec/batch)
2018-03-22 15:04:53.534870: step 25140, loss = 0.98 (250.3 examples/sec; 0.511 sec/batch)
2018-03-22 15:04:58.110459: step 25150, loss = 0.83 (279.7 examples/sec; 0.458 sec/batch)
2018-03-22 15:05:03.637440: step 25160, loss = 0.85 (231.6 examples/sec; 0.553 sec/batch)
2018-03-22 15:05:08.319633: step 25170, loss = 0.79 (273.4 examples/sec; 0.468 sec/batch)
2018-03-22 15:05:13.365346: step 25180, loss = 0.87 (253.7 examples/sec; 0.505 sec/batch)
2018-03-22 15:05:18.805333: step 25190, loss = 0.88 (235.3 examples/sec; 0.544 sec/batch)
2018-03-22 15:05:23.787983: step 25200, loss = 0.83 (256.9 examples/sec; 0.498 sec/batch)
2018-03-22 15:05:28.364468: step 25210, loss = 0.72 (279.7 examples/sec; 0.458 sec/batch)
2018-03-22 15:05:33.249455: step 25220, loss = 0.92 (262.0 examples/sec; 0.488 sec/batch)
2018-03-22 15:05:38.379288: step 25230, loss = 0.67 (249.5 examples/sec; 0.513 sec/batch)
2018-03-22 15:05:43.970298: step 25240, loss = 0.81 (228.9 examples/sec; 0.559 sec/batch)
2018-03-22 15:05:48.794377: step 25250, loss = 0.85 (265.3 examples/sec; 0.482 sec/batch)
2018-03-22 15:05:53.589393: step 25260, loss = 0.76 (266.9 examples/sec; 0.480 sec/batch)
2018-03-22 15:05:58.397418: step 25270, loss = 0.92 (266.2 examples/sec; 0.481 sec/batch)
2018-03-22 15:06:03.743424: step 25280, loss = 0.80 (239.4 examples/sec; 0.535 sec/batch)
2018-03-22 15:06:08.161416: step 25290, loss = 0.89 (289.7 examples/sec; 0.442 sec/batch)
2018-03-22 15:06:13.229212: step 25300, loss = 0.90 (252.6 examples/sec; 0.507 sec/batch)
2018-03-22 15:06:18.338304: step 25310, loss = 0.72 (250.5 examples/sec; 0.511 sec/batch)
2018-03-22 15:06:23.189478: step 25320, loss = 0.87 (263.9 examples/sec; 0.485 sec/batch)
2018-03-22 15:06:28.595530: step 25330, loss = 0.76 (236.8 examples/sec; 0.541 sec/batch)
2018-03-22 15:06:33.832759: step 25340, loss = 0.76 (244.4 examples/sec; 0.524 sec/batch)
2018-03-22 15:06:38.859068: step 25350, loss = 0.83 (254.7 examples/sec; 0.503 sec/batch)
2018-03-22 15:06:43.854960: step 25360, loss = 0.69 (256.2 examples/sec; 0.500 sec/batch)
2018-03-22 15:06:48.721700: step 25370, loss = 0.71 (263.0 examples/sec; 0.487 sec/batch)
2018-03-22 15:06:54.359892: step 25380, loss = 0.69 (227.0 examples/sec; 0.564 sec/batch)
2018-03-22 15:07:00.257535: step 25390, loss = 0.94 (217.0 examples/sec; 0.590 sec/batch)
2018-03-22 15:07:05.832635: step 25400, loss = 0.80 (229.6 examples/sec; 0.558 sec/batch)
2018-03-22 15:07:10.837364: step 25410, loss = 0.68 (255.8 examples/sec; 0.500 sec/batch)
2018-03-22 15:07:15.269412: step 25420, loss = 0.74 (288.8 examples/sec; 0.443 sec/batch)
2018-03-22 15:07:20.756670: step 25430, loss = 0.98 (233.3 examples/sec; 0.549 sec/batch)
2018-03-22 15:07:26.117422: step 25440, loss = 0.99 (238.8 examples/sec; 0.536 sec/batch)
2018-03-22 15:07:31.227335: step 25450, loss = 0.83 (250.5 examples/sec; 0.511 sec/batch)
2018-03-22 15:07:35.775418: step 25460, loss = 0.70 (281.4 examples/sec; 0.455 sec/batch)
2018-03-22 15:07:40.901213: step 25470, loss = 0.77 (249.7 examples/sec; 0.513 sec/batch)
2018-03-22 15:07:46.534458: step 25480, loss = 0.76 (227.2 examples/sec; 0.563 sec/batch)
2018-03-22 15:07:51.898506: step 25490, loss = 0.79 (238.6 examples/sec; 0.536 sec/batch)
2018-03-22 15:07:56.773491: step 25500, loss = 0.82 (262.6 examples/sec; 0.487 sec/batch)
2018-03-22 15:08:01.776210: step 25510, loss = 1.01 (255.9 examples/sec; 0.500 sec/batch)
2018-03-22 15:08:06.877416: step 25520, loss = 0.78 (250.9 examples/sec; 0.510 sec/batch)
2018-03-22 15:08:11.779356: step 25530, loss = 0.73 (261.1 examples/sec; 0.490 sec/batch)
2018-03-22 15:08:15.791067: step 25540, loss = 0.86 (319.1 examples/sec; 0.401 sec/batch)
2018-03-22 15:08:20.644809: step 25550, loss = 0.78 (263.7 examples/sec; 0.485 sec/batch)
2018-03-22 15:08:25.792013: step 25560, loss = 0.92 (248.7 examples/sec; 0.515 sec/batch)
2018-03-22 15:08:31.040371: step 25570, loss = 0.89 (243.9 examples/sec; 0.525 sec/batch)
2018-03-22 15:08:36.643410: step 25580, loss = 0.92 (228.4 examples/sec; 0.560 sec/batch)
2018-03-22 15:08:41.406544: step 25590, loss = 0.77 (268.7 examples/sec; 0.476 sec/batch)
2018-03-22 15:08:46.369607: step 25600, loss = 0.83 (257.9 examples/sec; 0.496 sec/batch)
2018-03-22 15:08:51.288434: step 25610, loss = 0.80 (260.2 examples/sec; 0.492 sec/batch)
2018-03-22 15:08:56.825364: step 25620, loss = 0.74 (231.2 examples/sec; 0.554 sec/batch)
2018-03-22 15:09:01.764379: step 25630, loss = 0.78 (259.2 examples/sec; 0.494 sec/batch)
2018-03-22 15:09:06.371384: step 25640, loss = 0.69 (277.8 examples/sec; 0.461 sec/batch)
2018-03-22 15:09:11.762427: step 25650, loss = 0.68 (237.4 examples/sec; 0.539 sec/batch)
2018-03-22 15:09:16.995399: step 25660, loss = 0.81 (244.6 examples/sec; 0.523 sec/batch)
2018-03-22 15:09:21.864408: step 25670, loss = 0.79 (262.9 examples/sec; 0.487 sec/batch)
2018-03-22 15:09:26.676384: step 25680, loss = 0.81 (266.0 examples/sec; 0.481 sec/batch)
2018-03-22 15:09:31.944349: step 25690, loss = 0.81 (243.0 examples/sec; 0.527 sec/batch)
2018-03-22 15:09:37.499832: step 25700, loss = 0.71 (230.4 examples/sec; 0.556 sec/batch)
2018-03-22 15:09:42.339399: step 25710, loss = 0.73 (264.5 examples/sec; 0.484 sec/batch)
2018-03-22 15:09:47.370429: step 25720, loss = 0.85 (254.4 examples/sec; 0.503 sec/batch)
2018-03-22 15:09:52.325255: step 25730, loss = 0.73 (258.3 examples/sec; 0.495 sec/batch)
2018-03-22 15:09:57.159380: step 25740, loss = 0.84 (264.8 examples/sec; 0.483 sec/batch)
2018-03-22 15:10:01.943446: step 25750, loss = 0.85 (267.6 examples/sec; 0.478 sec/batch)
2018-03-22 15:10:06.946883: step 25760, loss = 0.88 (255.8 examples/sec; 0.500 sec/batch)
2018-03-22 15:10:12.174309: step 25770, loss = 0.75 (244.9 examples/sec; 0.523 sec/batch)
2018-03-22 15:10:16.873397: step 25780, loss = 0.84 (272.4 examples/sec; 0.470 sec/batch)
2018-03-22 15:10:22.344402: step 25790, loss = 0.79 (234.0 examples/sec; 0.547 sec/batch)
2018-03-22 15:10:27.280775: step 25800, loss = 0.91 (259.3 examples/sec; 0.494 sec/batch)
2018-03-22 15:10:32.175378: step 25810, loss = 0.82 (261.5 examples/sec; 0.489 sec/batch)
2018-03-22 15:10:37.999712: step 25820, loss = 0.88 (219.8 examples/sec; 0.582 sec/batch)
2018-03-22 15:10:43.180411: step 25830, loss = 0.81 (247.1 examples/sec; 0.518 sec/batch)
2018-03-22 15:10:48.615393: step 25840, loss = 0.64 (235.5 examples/sec; 0.543 sec/batch)
2018-03-22 15:10:53.454476: step 25850, loss = 0.89 (264.5 examples/sec; 0.484 sec/batch)
2018-03-22 15:10:58.578295: step 25860, loss = 0.91 (249.8 examples/sec; 0.512 sec/batch)
2018-03-22 15:11:03.875127: step 25870, loss = 0.79 (241.7 examples/sec; 0.530 sec/batch)
2018-03-22 15:11:09.006447: step 25880, loss = 0.94 (249.4 examples/sec; 0.513 sec/batch)
2018-03-22 15:11:14.548923: step 25890, loss = 0.79 (230.9 examples/sec; 0.554 sec/batch)
2018-03-22 15:11:19.955422: step 25900, loss = 0.69 (236.8 examples/sec; 0.541 sec/batch)
2018-03-22 15:11:24.747309: step 25910, loss = 0.78 (267.1 examples/sec; 0.479 sec/batch)
2018-03-22 15:11:29.626703: step 25920, loss = 0.93 (262.3 examples/sec; 0.488 sec/batch)
2018-03-22 15:11:34.895356: step 25930, loss = 0.72 (242.9 examples/sec; 0.527 sec/batch)
2018-03-22 15:11:40.036624: step 25940, loss = 0.94 (249.0 examples/sec; 0.514 sec/batch)
2018-03-22 15:11:44.447377: step 25950, loss = 0.75 (290.2 examples/sec; 0.441 sec/batch)
2018-03-22 15:11:49.162436: step 25960, loss = 0.76 (271.5 examples/sec; 0.472 sec/batch)
2018-03-22 15:11:54.635841: step 25970, loss = 0.77 (233.9 examples/sec; 0.547 sec/batch)
2018-03-22 15:11:59.454998: step 25980, loss = 0.75 (265.6 examples/sec; 0.482 sec/batch)
2018-03-22 15:12:03.655828: step 25990, loss = 0.75 (304.7 examples/sec; 0.420 sec/batch)
2018-03-22 15:12:08.607362: step 26000, loss = 0.83 (258.5 examples/sec; 0.495 sec/batch)
2018-03-22 15:12:13.559533: step 26010, loss = 0.74 (258.5 examples/sec; 0.495 sec/batch)
2018-03-22 15:12:18.105939: step 26020, loss = 0.73 (281.5 examples/sec; 0.455 sec/batch)
2018-03-22 15:12:23.074399: step 26030, loss = 0.75 (257.6 examples/sec; 0.497 sec/batch)
2018-03-22 15:12:28.400380: step 26040, loss = 0.85 (240.3 examples/sec; 0.533 sec/batch)
2018-03-22 15:12:33.632509: step 26050, loss = 0.75 (244.6 examples/sec; 0.523 sec/batch)
2018-03-22 15:12:38.102399: step 26060, loss = 0.89 (286.4 examples/sec; 0.447 sec/batch)
2018-03-22 15:12:43.425434: step 26070, loss = 0.67 (240.5 examples/sec; 0.532 sec/batch)
2018-03-22 15:12:48.834380: step 26080, loss = 0.74 (236.6 examples/sec; 0.541 sec/batch)
2018-03-22 15:12:53.712392: step 26090, loss = 0.86 (262.4 examples/sec; 0.488 sec/batch)
2018-03-22 15:12:58.549559: step 26100, loss = 0.71 (264.6 examples/sec; 0.484 sec/batch)
2018-03-22 15:13:03.438322: step 26110, loss = 0.73 (261.8 examples/sec; 0.489 sec/batch)
2018-03-22 15:13:08.108949: step 26120, loss = 0.66 (274.1 examples/sec; 0.467 sec/batch)
2018-03-22 15:13:13.212503: step 26130, loss = 0.81 (250.8 examples/sec; 0.510 sec/batch)
2018-03-22 15:13:18.416389: step 26140, loss = 0.69 (246.0 examples/sec; 0.520 sec/batch)
2018-03-22 15:13:23.523428: step 26150, loss = 0.74 (250.6 examples/sec; 0.511 sec/batch)
2018-03-22 15:13:28.761404: step 26160, loss = 0.95 (244.4 examples/sec; 0.524 sec/batch)
2018-03-22 15:13:33.756777: step 26170, loss = 0.90 (256.2 examples/sec; 0.500 sec/batch)
2018-03-22 15:13:38.541656: step 26180, loss = 0.74 (267.5 examples/sec; 0.479 sec/batch)
2018-03-22 15:13:43.603492: step 26190, loss = 0.84 (252.9 examples/sec; 0.506 sec/batch)
2018-03-22 15:13:48.883241: step 26200, loss = 0.98 (242.4 examples/sec; 0.528 sec/batch)
2018-03-22 15:13:53.890247: step 26210, loss = 0.86 (255.6 examples/sec; 0.501 sec/batch)
2018-03-22 15:13:58.630437: step 26220, loss = 0.86 (270.0 examples/sec; 0.474 sec/batch)
2018-03-22 15:14:03.733025: step 26230, loss = 0.75 (250.9 examples/sec; 0.510 sec/batch)
2018-03-22 15:14:08.393414: step 26240, loss = 0.87 (274.7 examples/sec; 0.466 sec/batch)
2018-03-22 15:14:13.549428: step 26250, loss = 0.69 (248.3 examples/sec; 0.516 sec/batch)
2018-03-22 15:14:18.483314: step 26260, loss = 0.80 (259.4 examples/sec; 0.493 sec/batch)
2018-03-22 15:14:23.590438: step 26270, loss = 0.70 (250.6 examples/sec; 0.511 sec/batch)
2018-03-22 15:14:28.388079: step 26280, loss = 0.73 (266.8 examples/sec; 0.480 sec/batch)
2018-03-22 15:14:34.069807: step 26290, loss = 0.82 (225.3 examples/sec; 0.568 sec/batch)
2018-03-22 15:14:39.213075: step 26300, loss = 1.04 (248.9 examples/sec; 0.514 sec/batch)
2018-03-22 15:14:44.263457: step 26310, loss = 0.72 (253.4 examples/sec; 0.505 sec/batch)
2018-03-22 15:14:49.150417: step 26320, loss = 0.90 (261.9 examples/sec; 0.489 sec/batch)
2018-03-22 15:14:54.196416: step 26330, loss = 0.79 (253.7 examples/sec; 0.505 sec/batch)
2018-03-22 15:14:59.169468: step 26340, loss = 0.67 (257.4 examples/sec; 0.497 sec/batch)
2018-03-22 15:15:03.740462: step 26350, loss = 0.80 (280.0 examples/sec; 0.457 sec/batch)
2018-03-22 15:15:08.750473: step 26360, loss = 0.79 (255.5 examples/sec; 0.501 sec/batch)
2018-03-22 15:15:13.889298: step 26370, loss = 0.72 (249.1 examples/sec; 0.514 sec/batch)
2018-03-22 15:15:18.742572: step 26380, loss = 0.69 (263.7 examples/sec; 0.485 sec/batch)
2018-03-22 15:15:23.674887: step 26390, loss = 0.66 (259.5 examples/sec; 0.493 sec/batch)
2018-03-22 15:15:28.680991: step 26400, loss = 0.76 (255.7 examples/sec; 0.501 sec/batch)
2018-03-22 15:15:33.966353: step 26410, loss = 0.95 (242.2 examples/sec; 0.529 sec/batch)
2018-03-22 15:15:38.725415: step 26420, loss = 0.86 (269.0 examples/sec; 0.476 sec/batch)
2018-03-22 15:15:43.581484: step 26430, loss = 0.81 (263.6 examples/sec; 0.486 sec/batch)
2018-03-22 15:15:48.729447: step 26440, loss = 0.78 (248.6 examples/sec; 0.515 sec/batch)
2018-03-22 15:15:53.384712: step 26450, loss = 0.84 (275.0 examples/sec; 0.466 sec/batch)
2018-03-22 15:15:58.027318: step 26460, loss = 0.74 (275.7 examples/sec; 0.464 sec/batch)
2018-03-22 15:16:02.225720: step 26470, loss = 0.98 (304.9 examples/sec; 0.420 sec/batch)
2018-03-22 15:16:07.569416: step 26480, loss = 0.86 (239.5 examples/sec; 0.534 sec/batch)
2018-03-22 15:16:12.746315: step 26490, loss = 0.88 (247.3 examples/sec; 0.518 sec/batch)
2018-03-22 15:16:18.222151: step 26500, loss = 0.75 (233.8 examples/sec; 0.548 sec/batch)
2018-03-22 15:16:23.763041: step 26510, loss = 0.82 (231.0 examples/sec; 0.554 sec/batch)
2018-03-22 15:16:28.375373: step 26520, loss = 0.74 (277.5 examples/sec; 0.461 sec/batch)
2018-03-22 15:16:32.848384: step 26530, loss = 0.78 (286.2 examples/sec; 0.447 sec/batch)
2018-03-22 15:16:37.352885: step 26540, loss = 0.83 (284.2 examples/sec; 0.450 sec/batch)
2018-03-22 15:16:42.435397: step 26550, loss = 0.77 (251.8 examples/sec; 0.508 sec/batch)
2018-03-22 15:16:47.384650: step 26560, loss = 0.71 (258.6 examples/sec; 0.495 sec/batch)
2018-03-22 15:16:52.292397: step 26570, loss = 0.85 (260.8 examples/sec; 0.491 sec/batch)
2018-03-22 15:16:57.256609: step 26580, loss = 0.71 (257.8 examples/sec; 0.496 sec/batch)
2018-03-22 15:17:01.995471: step 26590, loss = 0.76 (270.1 examples/sec; 0.474 sec/batch)
2018-03-22 15:17:07.805631: step 26600, loss = 0.74 (220.3 examples/sec; 0.581 sec/batch)
2018-03-22 15:17:13.467904: step 26610, loss = 0.79 (226.1 examples/sec; 0.566 sec/batch)
2018-03-22 15:17:18.518355: step 26620, loss = 0.90 (253.4 examples/sec; 0.505 sec/batch)
2018-03-22 15:17:23.310927: step 26630, loss = 0.66 (267.1 examples/sec; 0.479 sec/batch)
2018-03-22 15:17:28.034077: step 26640, loss = 0.78 (271.0 examples/sec; 0.472 sec/batch)
2018-03-22 15:17:32.527053: step 26650, loss = 0.79 (284.9 examples/sec; 0.449 sec/batch)
2018-03-22 15:17:37.478414: step 26660, loss = 0.73 (258.5 examples/sec; 0.495 sec/batch)
2018-03-22 15:17:42.101376: step 26670, loss = 0.84 (276.9 examples/sec; 0.462 sec/batch)
2018-03-22 15:17:46.835178: step 26680, loss = 0.70 (270.4 examples/sec; 0.473 sec/batch)
2018-03-22 15:17:52.349343: step 26690, loss = 0.77 (232.1 examples/sec; 0.551 sec/batch)
2018-03-22 15:17:57.434592: step 26700, loss = 0.90 (251.7 examples/sec; 0.509 sec/batch)
2018-03-22 15:18:02.692845: step 26710, loss = 0.78 (243.4 examples/sec; 0.526 sec/batch)
2018-03-22 15:18:08.101713: step 26720, loss = 0.75 (236.6 examples/sec; 0.541 sec/batch)
2018-03-22 15:18:13.798614: step 26730, loss = 1.10 (224.7 examples/sec; 0.570 sec/batch)
2018-03-22 15:18:18.778230: step 26740, loss = 0.83 (257.0 examples/sec; 0.498 sec/batch)
2018-03-22 15:18:23.940429: step 26750, loss = 0.75 (248.0 examples/sec; 0.516 sec/batch)
2018-03-22 15:18:29.347604: step 26760, loss = 0.86 (236.7 examples/sec; 0.541 sec/batch)
2018-03-22 15:18:34.021362: step 26770, loss = 0.86 (273.9 examples/sec; 0.467 sec/batch)
2018-03-22 15:18:39.351345: step 26780, loss = 0.88 (240.2 examples/sec; 0.533 sec/batch)
2018-03-22 15:18:44.900929: step 26790, loss = 0.79 (230.6 examples/sec; 0.555 sec/batch)
2018-03-22 15:18:50.514425: step 26800, loss = 0.81 (228.0 examples/sec; 0.561 sec/batch)
2018-03-22 15:18:55.679887: step 26810, loss = 0.78 (247.8 examples/sec; 0.517 sec/batch)
2018-03-22 15:19:00.669080: step 26820, loss = 0.82 (256.6 examples/sec; 0.499 sec/batch)
2018-03-22 15:19:05.475666: step 26830, loss = 1.14 (266.3 examples/sec; 0.481 sec/batch)
2018-03-22 15:19:11.085774: step 26840, loss = 0.76 (228.2 examples/sec; 0.561 sec/batch)
2018-03-22 15:19:15.929303: step 26850, loss = 1.08 (264.3 examples/sec; 0.484 sec/batch)
2018-03-22 15:19:21.661381: step 26860, loss = 0.65 (223.3 examples/sec; 0.573 sec/batch)
2018-03-22 15:19:26.615235: step 26870, loss = 0.94 (258.4 examples/sec; 0.495 sec/batch)
2018-03-22 15:19:31.891936: step 26880, loss = 0.75 (242.6 examples/sec; 0.528 sec/batch)
2018-03-22 15:19:36.723349: step 26890, loss = 0.73 (264.9 examples/sec; 0.483 sec/batch)
2018-03-22 15:19:41.716565: step 26900, loss = 0.66 (256.3 examples/sec; 0.499 sec/batch)
2018-03-22 15:19:45.957294: step 26910, loss = 0.65 (301.8 examples/sec; 0.424 sec/batch)
2018-03-22 15:19:50.750388: step 26920, loss = 0.76 (267.1 examples/sec; 0.479 sec/batch)
2018-03-22 15:19:56.007373: step 26930, loss = 0.80 (243.5 examples/sec; 0.526 sec/batch)
2018-03-22 15:20:00.920448: step 26940, loss = 0.79 (260.5 examples/sec; 0.491 sec/batch)
2018-03-22 15:20:05.487867: step 26950, loss = 0.83 (280.2 examples/sec; 0.457 sec/batch)
2018-03-22 15:20:10.472690: step 26960, loss = 0.75 (256.8 examples/sec; 0.498 sec/batch)
2018-03-22 15:20:15.597441: step 26970, loss = 1.00 (249.8 examples/sec; 0.512 sec/batch)
2018-03-22 15:20:20.165886: step 26980, loss = 0.77 (280.2 examples/sec; 0.457 sec/batch)
2018-03-22 15:20:25.437480: step 26990, loss = 0.79 (242.8 examples/sec; 0.527 sec/batch)
2018-03-22 15:20:31.098603: step 27000, loss = 0.81 (226.1 examples/sec; 0.566 sec/batch)
2018-03-22 15:20:36.121445: step 27010, loss = 0.67 (254.8 examples/sec; 0.502 sec/batch)
2018-03-22 15:20:41.022349: step 27020, loss = 0.82 (261.2 examples/sec; 0.490 sec/batch)
2018-03-22 15:20:45.743640: step 27030, loss = 0.81 (271.1 examples/sec; 0.472 sec/batch)
2018-03-22 15:20:51.293312: step 27040, loss = 0.73 (230.6 examples/sec; 0.555 sec/batch)
2018-03-22 15:20:56.531700: step 27050, loss = 0.85 (244.3 examples/sec; 0.524 sec/batch)
2018-03-22 15:21:01.748396: step 27060, loss = 0.88 (245.4 examples/sec; 0.522 sec/batch)
2018-03-22 15:21:06.262371: step 27070, loss = 0.85 (283.6 examples/sec; 0.451 sec/batch)
2018-03-22 15:21:11.300431: step 27080, loss = 0.92 (254.1 examples/sec; 0.504 sec/batch)
2018-03-22 15:21:16.184400: step 27090, loss = 0.85 (262.1 examples/sec; 0.488 sec/batch)
2018-03-22 15:21:21.247041: step 27100, loss = 0.69 (252.8 examples/sec; 0.506 sec/batch)
2018-03-22 15:21:25.717861: step 27110, loss = 0.66 (286.3 examples/sec; 0.447 sec/batch)
2018-03-22 15:21:30.708431: step 27120, loss = 1.00 (256.5 examples/sec; 0.499 sec/batch)
2018-03-22 15:21:35.992369: step 27130, loss = 0.64 (242.2 examples/sec; 0.528 sec/batch)
2018-03-22 15:21:41.090344: step 27140, loss = 0.79 (251.1 examples/sec; 0.510 sec/batch)
2018-03-22 15:21:46.533498: step 27150, loss = 0.73 (235.2 examples/sec; 0.544 sec/batch)
2018-03-22 15:21:52.032438: step 27160, loss = 0.78 (232.8 examples/sec; 0.550 sec/batch)
2018-03-22 15:21:57.120255: step 27170, loss = 0.88 (251.6 examples/sec; 0.509 sec/batch)
2018-03-22 15:22:02.108778: step 27180, loss = 0.74 (256.6 examples/sec; 0.499 sec/batch)
2018-03-22 15:22:07.279596: step 27190, loss = 0.74 (247.5 examples/sec; 0.517 sec/batch)
2018-03-22 15:22:13.183476: step 27200, loss = 0.93 (216.8 examples/sec; 0.590 sec/batch)
2018-03-22 15:22:18.371413: step 27210, loss = 0.77 (246.7 examples/sec; 0.519 sec/batch)
2018-03-22 15:22:23.313433: step 27220, loss = 0.57 (259.0 examples/sec; 0.494 sec/batch)
2018-03-22 15:22:28.199495: step 27230, loss = 0.91 (262.0 examples/sec; 0.489 sec/batch)
2018-03-22 15:22:33.777412: step 27240, loss = 0.90 (229.5 examples/sec; 0.558 sec/batch)
2018-03-22 15:22:38.776402: step 27250, loss = 0.82 (256.1 examples/sec; 0.500 sec/batch)
2018-03-22 15:22:44.100260: step 27260, loss = 0.80 (240.4 examples/sec; 0.532 sec/batch)
2018-03-22 15:22:49.333790: step 27270, loss = 0.69 (244.6 examples/sec; 0.523 sec/batch)
2018-03-22 15:22:54.671750: step 27280, loss = 0.90 (239.8 examples/sec; 0.534 sec/batch)
2018-03-22 15:22:59.057397: step 27290, loss = 0.83 (291.9 examples/sec; 0.439 sec/batch)
2018-03-22 15:23:04.316389: step 27300, loss = 0.95 (243.4 examples/sec; 0.526 sec/batch)
2018-03-22 15:23:09.771962: step 27310, loss = 0.90 (234.6 examples/sec; 0.546 sec/batch)
2018-03-22 15:23:14.848344: step 27320, loss = 0.78 (252.1 examples/sec; 0.508 sec/batch)
2018-03-22 15:23:19.880352: step 27330, loss = 0.75 (254.4 examples/sec; 0.503 sec/batch)
2018-03-22 15:23:24.453399: step 27340, loss = 0.62 (279.9 examples/sec; 0.457 sec/batch)
2018-03-22 15:23:29.730658: step 27350, loss = 0.83 (242.6 examples/sec; 0.528 sec/batch)
2018-03-22 15:23:34.946992: step 27360, loss = 0.79 (245.4 examples/sec; 0.522 sec/batch)
2018-03-22 15:23:39.889353: step 27370, loss = 0.68 (259.0 examples/sec; 0.494 sec/batch)
2018-03-22 15:23:44.657828: step 27380, loss = 0.76 (268.4 examples/sec; 0.477 sec/batch)
2018-03-22 15:23:49.661443: step 27390, loss = 0.78 (255.8 examples/sec; 0.500 sec/batch)
2018-03-22 15:23:55.094856: step 27400, loss = 0.82 (235.6 examples/sec; 0.543 sec/batch)
2018-03-22 15:24:00.124360: step 27410, loss = 0.72 (254.5 examples/sec; 0.503 sec/batch)
2018-03-22 15:24:04.822808: step 27420, loss = 0.80 (272.4 examples/sec; 0.470 sec/batch)
2018-03-22 15:24:09.376230: step 27430, loss = 0.73 (281.1 examples/sec; 0.455 sec/batch)
2018-03-22 15:24:14.261379: step 27440, loss = 0.79 (262.0 examples/sec; 0.489 sec/batch)
2018-03-22 15:24:19.329308: step 27450, loss = 0.94 (252.6 examples/sec; 0.507 sec/batch)
2018-03-22 15:24:24.646311: step 27460, loss = 0.76 (240.7 examples/sec; 0.532 sec/batch)
2018-03-22 15:24:29.652006: step 27470, loss = 0.77 (255.7 examples/sec; 0.501 sec/batch)
2018-03-22 15:24:34.298359: step 27480, loss = 0.83 (275.5 examples/sec; 0.465 sec/batch)
2018-03-22 15:24:39.544439: step 27490, loss = 0.93 (244.0 examples/sec; 0.525 sec/batch)
2018-03-22 15:24:45.338189: step 27500, loss = 0.60 (220.9 examples/sec; 0.579 sec/batch)
2018-03-22 15:24:50.691317: step 27510, loss = 0.81 (239.1 examples/sec; 0.535 sec/batch)
2018-03-22 15:24:55.852281: step 27520, loss = 0.61 (248.0 examples/sec; 0.516 sec/batch)
2018-03-22 15:25:01.105807: step 27530, loss = 0.78 (243.6 examples/sec; 0.525 sec/batch)
2018-03-22 15:25:05.392451: step 27540, loss = 0.70 (298.6 examples/sec; 0.429 sec/batch)
2018-03-22 15:25:10.423288: step 27550, loss = 0.76 (254.4 examples/sec; 0.503 sec/batch)
2018-03-22 15:25:14.878389: step 27560, loss = 0.92 (287.3 examples/sec; 0.446 sec/batch)
2018-03-22 15:25:19.595476: step 27570, loss = 0.85 (271.4 examples/sec; 0.472 sec/batch)
2018-03-22 15:25:24.799461: step 27580, loss = 0.60 (246.0 examples/sec; 0.520 sec/batch)
2018-03-22 15:25:30.481366: step 27590, loss = 0.78 (225.3 examples/sec; 0.568 sec/batch)
2018-03-22 15:25:35.912056: step 27600, loss = 0.66 (235.7 examples/sec; 0.543 sec/batch)
2018-03-22 15:25:41.185445: step 27610, loss = 0.97 (242.7 examples/sec; 0.527 sec/batch)
2018-03-22 15:25:46.089434: step 27620, loss = 0.80 (261.0 examples/sec; 0.490 sec/batch)
2018-03-22 15:25:51.778412: step 27630, loss = 0.84 (225.0 examples/sec; 0.569 sec/batch)
2018-03-22 15:25:56.848393: step 27640, loss = 0.78 (252.5 examples/sec; 0.507 sec/batch)
2018-03-22 15:26:02.086815: step 27650, loss = 0.81 (244.3 examples/sec; 0.524 sec/batch)
2018-03-22 15:26:07.281560: step 27660, loss = 0.72 (246.4 examples/sec; 0.519 sec/batch)
2018-03-22 15:26:12.509452: step 27670, loss = 0.84 (244.8 examples/sec; 0.523 sec/batch)
2018-03-22 15:26:17.673811: step 27680, loss = 0.75 (247.9 examples/sec; 0.516 sec/batch)
2018-03-22 15:26:22.909348: step 27690, loss = 0.82 (244.5 examples/sec; 0.524 sec/batch)
2018-03-22 15:26:28.161409: step 27700, loss = 0.82 (243.7 examples/sec; 0.525 sec/batch)
2018-03-22 15:26:33.079845: step 27710, loss = 0.67 (260.2 examples/sec; 0.492 sec/batch)
2018-03-22 15:26:38.048755: step 27720, loss = 0.69 (257.6 examples/sec; 0.497 sec/batch)
2018-03-22 15:26:42.910387: step 27730, loss = 0.79 (263.3 examples/sec; 0.486 sec/batch)
2018-03-22 15:26:48.030325: step 27740, loss = 0.73 (250.0 examples/sec; 0.512 sec/batch)
2018-03-22 15:26:53.244397: step 27750, loss = 0.66 (245.5 examples/sec; 0.521 sec/batch)
2018-03-22 15:26:58.344310: step 27760, loss = 0.62 (251.0 examples/sec; 0.510 sec/batch)
2018-03-22 15:27:03.149992: step 27770, loss = 0.86 (266.4 examples/sec; 0.481 sec/batch)
2018-03-22 15:27:07.753498: step 27780, loss = 0.89 (278.0 examples/sec; 0.460 sec/batch)
2018-03-22 15:27:12.797161: step 27790, loss = 1.04 (253.8 examples/sec; 0.504 sec/batch)
2018-03-22 15:27:17.674100: step 27800, loss = 0.81 (262.5 examples/sec; 0.488 sec/batch)
2018-03-22 15:27:22.770338: step 27810, loss = 0.78 (251.2 examples/sec; 0.510 sec/batch)
2018-03-22 15:27:26.936355: step 27820, loss = 0.97 (307.2 examples/sec; 0.417 sec/batch)
2018-03-22 15:27:31.554371: step 27830, loss = 0.79 (277.2 examples/sec; 0.462 sec/batch)
2018-03-22 15:27:36.586687: step 27840, loss = 0.68 (254.4 examples/sec; 0.503 sec/batch)
2018-03-22 15:27:41.995360: step 27850, loss = 0.76 (236.7 examples/sec; 0.541 sec/batch)
2018-03-22 15:27:46.603451: step 27860, loss = 0.80 (277.8 examples/sec; 0.461 sec/batch)
2018-03-22 15:27:51.697841: step 27870, loss = 0.89 (251.3 examples/sec; 0.509 sec/batch)
2018-03-22 15:27:56.447330: step 27880, loss = 0.83 (269.5 examples/sec; 0.475 sec/batch)
2018-03-22 15:28:00.812293: step 27890, loss = 0.86 (293.2 examples/sec; 0.436 sec/batch)
2018-03-22 15:28:05.867221: step 27900, loss = 1.05 (253.2 examples/sec; 0.505 sec/batch)
2018-03-22 15:28:10.684511: step 27910, loss = 0.83 (265.7 examples/sec; 0.482 sec/batch)
2018-03-22 15:28:15.724565: step 27920, loss = 0.88 (254.0 examples/sec; 0.504 sec/batch)
2018-03-22 15:28:20.875404: step 27930, loss = 0.77 (248.5 examples/sec; 0.515 sec/batch)
2018-03-22 15:28:25.658130: step 27940, loss = 0.90 (267.6 examples/sec; 0.478 sec/batch)
2018-03-22 15:28:30.864670: step 27950, loss = 0.83 (245.8 examples/sec; 0.521 sec/batch)
2018-03-22 15:28:36.252416: step 27960, loss = 1.09 (237.6 examples/sec; 0.539 sec/batch)
2018-03-22 15:28:41.808395: step 27970, loss = 0.73 (230.4 examples/sec; 0.556 sec/batch)
2018-03-22 15:28:46.758501: step 27980, loss = 0.89 (258.6 examples/sec; 0.495 sec/batch)
2018-03-22 15:28:51.516426: step 27990, loss = 0.75 (269.0 examples/sec; 0.476 sec/batch)
2018-03-22 15:28:56.484855: step 28000, loss = 0.85 (257.6 examples/sec; 0.497 sec/batch)
2018-03-22 15:29:01.101477: step 28010, loss = 0.84 (277.3 examples/sec; 0.462 sec/batch)
2018-03-22 15:29:05.105277: step 28020, loss = 0.86 (319.7 examples/sec; 0.400 sec/batch)
2018-03-22 15:29:10.314458: step 28030, loss = 0.90 (245.7 examples/sec; 0.521 sec/batch)
2018-03-22 15:29:15.546427: step 28040, loss = 0.82 (244.6 examples/sec; 0.523 sec/batch)
2018-03-22 15:29:20.772264: step 28050, loss = 0.69 (244.9 examples/sec; 0.523 sec/batch)
2018-03-22 15:29:25.206372: step 28060, loss = 0.81 (288.7 examples/sec; 0.443 sec/batch)
2018-03-22 15:29:30.761370: step 28070, loss = 0.80 (230.4 examples/sec; 0.556 sec/batch)
2018-03-22 15:29:35.482321: step 28080, loss = 0.74 (271.1 examples/sec; 0.472 sec/batch)
2018-03-22 15:29:39.819399: step 28090, loss = 0.69 (295.1 examples/sec; 0.434 sec/batch)
2018-03-22 15:29:44.891605: step 28100, loss = 0.85 (252.4 examples/sec; 0.507 sec/batch)
2018-03-22 15:29:50.251340: step 28110, loss = 0.90 (238.8 examples/sec; 0.536 sec/batch)
2018-03-22 15:29:55.267383: step 28120, loss = 0.90 (255.2 examples/sec; 0.502 sec/batch)
2018-03-22 15:30:00.241447: step 28130, loss = 0.79 (257.3 examples/sec; 0.497 sec/batch)
2018-03-22 15:30:04.796846: step 28140, loss = 0.89 (281.0 examples/sec; 0.456 sec/batch)
2018-03-22 15:30:10.034677: step 28150, loss = 0.88 (244.4 examples/sec; 0.524 sec/batch)
2018-03-22 15:30:15.728345: step 28160, loss = 0.66 (224.8 examples/sec; 0.569 sec/batch)
2018-03-22 15:30:21.157394: step 28170, loss = 0.77 (235.8 examples/sec; 0.543 sec/batch)
2018-03-22 15:30:26.210809: step 28180, loss = 0.83 (253.3 examples/sec; 0.505 sec/batch)
2018-03-22 15:30:31.747809: step 28190, loss = 0.68 (231.2 examples/sec; 0.554 sec/batch)
2018-03-22 15:30:36.670611: step 28200, loss = 0.76 (260.0 examples/sec; 0.492 sec/batch)
2018-03-22 15:30:41.530262: step 28210, loss = 0.80 (263.4 examples/sec; 0.486 sec/batch)
2018-03-22 15:30:46.677316: step 28220, loss = 0.77 (248.7 examples/sec; 0.515 sec/batch)
2018-03-22 15:30:52.192893: step 28230, loss = 0.80 (232.1 examples/sec; 0.552 sec/batch)
2018-03-22 15:30:57.063453: step 28240, loss = 0.67 (262.8 examples/sec; 0.487 sec/batch)
2018-03-22 15:31:01.960258: step 28250, loss = 0.76 (261.4 examples/sec; 0.490 sec/batch)
2018-03-22 15:31:07.004397: step 28260, loss = 0.80 (253.8 examples/sec; 0.504 sec/batch)
2018-03-22 15:31:11.841428: step 28270, loss = 0.95 (264.6 examples/sec; 0.484 sec/batch)
2018-03-22 15:31:16.779366: step 28280, loss = 0.72 (259.2 examples/sec; 0.494 sec/batch)
2018-03-22 15:31:21.509908: step 28290, loss = 0.75 (270.6 examples/sec; 0.473 sec/batch)
2018-03-22 15:31:26.309867: step 28300, loss = 0.95 (266.7 examples/sec; 0.480 sec/batch)
2018-03-22 15:31:31.714423: step 28310, loss = 0.88 (236.8 examples/sec; 0.540 sec/batch)
2018-03-22 15:31:37.082465: step 28320, loss = 0.76 (238.4 examples/sec; 0.537 sec/batch)
2018-03-22 15:31:42.584035: step 28330, loss = 0.69 (232.7 examples/sec; 0.550 sec/batch)
2018-03-22 15:31:47.294108: step 28340, loss = 0.80 (271.8 examples/sec; 0.471 sec/batch)
2018-03-22 15:31:52.039773: step 28350, loss = 0.90 (269.7 examples/sec; 0.475 sec/batch)
2018-03-22 15:31:57.241387: step 28360, loss = 0.83 (246.1 examples/sec; 0.520 sec/batch)
2018-03-22 15:32:02.483901: step 28370, loss = 0.73 (244.2 examples/sec; 0.524 sec/batch)
2018-03-22 15:32:07.344526: step 28380, loss = 0.74 (263.4 examples/sec; 0.486 sec/batch)
2018-03-22 15:32:12.391410: step 28390, loss = 0.74 (253.6 examples/sec; 0.505 sec/batch)
2018-03-22 15:32:17.247168: step 28400, loss = 0.69 (263.6 examples/sec; 0.486 sec/batch)
2018-03-22 15:32:22.155961: step 28410, loss = 0.76 (260.8 examples/sec; 0.491 sec/batch)
2018-03-22 15:32:26.785374: step 28420, loss = 0.76 (276.5 examples/sec; 0.463 sec/batch)
2018-03-22 15:32:32.387427: step 28430, loss = 0.90 (228.5 examples/sec; 0.560 sec/batch)
2018-03-22 15:32:37.430498: step 28440, loss = 0.88 (253.8 examples/sec; 0.504 sec/batch)
2018-03-22 15:32:43.182565: step 28450, loss = 0.95 (222.5 examples/sec; 0.575 sec/batch)
2018-03-22 15:32:47.760499: step 28460, loss = 0.90 (279.6 examples/sec; 0.458 sec/batch)
2018-03-22 15:32:52.162337: step 28470, loss = 0.70 (290.8 examples/sec; 0.440 sec/batch)
2018-03-22 15:32:56.889450: step 28480, loss = 0.91 (270.8 examples/sec; 0.473 sec/batch)
2018-03-22 15:33:01.640472: step 28490, loss = 0.75 (269.4 examples/sec; 0.475 sec/batch)
2018-03-22 15:33:06.823020: step 28500, loss = 0.74 (247.0 examples/sec; 0.518 sec/batch)
2018-03-22 15:33:12.177240: step 28510, loss = 0.61 (239.1 examples/sec; 0.535 sec/batch)
2018-03-22 15:33:17.219840: step 28520, loss = 0.73 (253.8 examples/sec; 0.504 sec/batch)
2018-03-22 15:33:22.031900: step 28530, loss = 0.64 (266.0 examples/sec; 0.481 sec/batch)
2018-03-22 15:33:27.178499: step 28540, loss = 0.72 (248.7 examples/sec; 0.515 sec/batch)
2018-03-22 15:33:32.324943: step 28550, loss = 0.76 (248.7 examples/sec; 0.515 sec/batch)
2018-03-22 15:33:37.092283: step 28560, loss = 0.72 (268.5 examples/sec; 0.477 sec/batch)
2018-03-22 15:33:42.238572: step 28570, loss = 0.90 (248.7 examples/sec; 0.515 sec/batch)
2018-03-22 15:33:47.443347: step 28580, loss = 0.71 (245.9 examples/sec; 0.520 sec/batch)
2018-03-22 15:33:52.512373: step 28590, loss = 0.74 (252.5 examples/sec; 0.507 sec/batch)
2018-03-22 15:33:57.354144: step 28600, loss = 0.77 (264.4 examples/sec; 0.484 sec/batch)
2018-03-22 15:34:02.355369: step 28610, loss = 0.91 (255.9 examples/sec; 0.500 sec/batch)
2018-03-22 15:34:07.435348: step 28620, loss = 0.77 (252.0 examples/sec; 0.508 sec/batch)
2018-03-22 15:34:12.663993: step 28630, loss = 0.82 (244.8 examples/sec; 0.523 sec/batch)
2018-03-22 15:34:16.921323: step 28640, loss = 0.84 (300.7 examples/sec; 0.426 sec/batch)
2018-03-22 15:34:22.373447: step 28650, loss = 0.73 (234.8 examples/sec; 0.545 sec/batch)
2018-03-22 15:34:27.559049: step 28660, loss = 0.84 (246.8 examples/sec; 0.519 sec/batch)
2018-03-22 15:34:32.493386: step 28670, loss = 0.83 (259.4 examples/sec; 0.493 sec/batch)
2018-03-22 15:34:37.555396: step 28680, loss = 0.82 (252.9 examples/sec; 0.506 sec/batch)
2018-03-22 15:34:42.466928: step 28690, loss = 0.86 (260.6 examples/sec; 0.491 sec/batch)
2018-03-22 15:34:47.704801: step 28700, loss = 0.88 (244.4 examples/sec; 0.524 sec/batch)
2018-03-22 15:34:52.968405: step 28710, loss = 0.82 (243.2 examples/sec; 0.526 sec/batch)
2018-03-22 15:34:58.016100: step 28720, loss = 0.71 (253.6 examples/sec; 0.505 sec/batch)
2018-03-22 15:35:03.759591: step 28730, loss = 0.71 (222.9 examples/sec; 0.574 sec/batch)
2018-03-22 15:35:08.468844: step 28740, loss = 0.69 (271.8 examples/sec; 0.471 sec/batch)
2018-03-22 15:35:13.012469: step 28750, loss = 0.78 (281.7 examples/sec; 0.454 sec/batch)
2018-03-22 15:35:17.476927: step 28760, loss = 0.75 (286.7 examples/sec; 0.446 sec/batch)
2018-03-22 15:35:22.133435: step 28770, loss = 0.92 (274.9 examples/sec; 0.466 sec/batch)
2018-03-22 15:35:27.241416: step 28780, loss = 0.87 (250.6 examples/sec; 0.511 sec/batch)
2018-03-22 15:35:33.001426: step 28790, loss = 0.67 (222.2 examples/sec; 0.576 sec/batch)
2018-03-22 15:35:38.141477: step 28800, loss = 0.73 (249.0 examples/sec; 0.514 sec/batch)
2018-03-22 15:35:43.096745: step 28810, loss = 0.71 (258.3 examples/sec; 0.496 sec/batch)
2018-03-22 15:35:48.044768: step 28820, loss = 0.72 (258.7 examples/sec; 0.495 sec/batch)
2018-03-22 15:35:53.150406: step 28830, loss = 0.78 (250.7 examples/sec; 0.511 sec/batch)
2018-03-22 15:35:58.461655: step 28840, loss = 0.84 (241.0 examples/sec; 0.531 sec/batch)
2018-03-22 15:36:03.875466: step 28850, loss = 0.86 (236.4 examples/sec; 0.541 sec/batch)
2018-03-22 15:36:08.589087: step 28860, loss = 0.68 (271.6 examples/sec; 0.471 sec/batch)
2018-03-22 15:36:13.578454: step 28870, loss = 0.94 (256.5 examples/sec; 0.499 sec/batch)
2018-03-22 15:36:18.401381: step 28880, loss = 0.76 (265.4 examples/sec; 0.482 sec/batch)
2018-03-22 15:36:23.966573: step 28890, loss = 0.87 (230.0 examples/sec; 0.557 sec/batch)
2018-03-22 15:36:28.825626: step 28900, loss = 0.88 (263.4 examples/sec; 0.486 sec/batch)
2018-03-22 15:36:33.839349: step 28910, loss = 0.63 (255.3 examples/sec; 0.501 sec/batch)
2018-03-22 15:36:38.594955: step 28920, loss = 0.77 (269.2 examples/sec; 0.476 sec/batch)
2018-03-22 15:36:43.659380: step 28930, loss = 0.76 (252.7 examples/sec; 0.506 sec/batch)
2018-03-22 15:36:48.102343: step 28940, loss = 0.72 (288.1 examples/sec; 0.444 sec/batch)
2018-03-22 15:36:52.812779: step 28950, loss = 0.95 (271.7 examples/sec; 0.471 sec/batch)
2018-03-22 15:36:57.315981: step 28960, loss = 0.83 (284.2 examples/sec; 0.450 sec/batch)
2018-03-22 15:37:02.885393: step 28970, loss = 0.83 (229.8 examples/sec; 0.557 sec/batch)
2018-03-22 15:37:08.281358: step 28980, loss = 0.85 (237.2 examples/sec; 0.540 sec/batch)
2018-03-22 15:37:13.402398: step 28990, loss = 0.66 (249.9 examples/sec; 0.512 sec/batch)
2018-03-22 15:37:18.524431: step 29000, loss = 0.61 (249.9 examples/sec; 0.512 sec/batch)
2018-03-22 15:37:22.927426: step 29010, loss = 0.96 (290.7 examples/sec; 0.440 sec/batch)
2018-03-22 15:37:28.525357: step 29020, loss = 0.82 (228.7 examples/sec; 0.560 sec/batch)
2018-03-22 15:37:33.638165: step 29030, loss = 0.60 (250.4 examples/sec; 0.511 sec/batch)
2018-03-22 15:37:38.922465: step 29040, loss = 0.91 (242.2 examples/sec; 0.528 sec/batch)
2018-03-22 15:37:43.822155: step 29050, loss = 0.83 (261.2 examples/sec; 0.490 sec/batch)
2018-03-22 15:37:48.644161: step 29060, loss = 0.88 (265.4 examples/sec; 0.482 sec/batch)
2018-03-22 15:37:53.725134: step 29070, loss = 0.79 (251.9 examples/sec; 0.508 sec/batch)
2018-03-22 15:37:58.574134: step 29080, loss = 0.60 (264.0 examples/sec; 0.485 sec/batch)
2018-03-22 15:38:03.538442: step 29090, loss = 0.80 (257.8 examples/sec; 0.496 sec/batch)
2018-03-22 15:38:08.581964: step 29100, loss = 0.76 (253.8 examples/sec; 0.504 sec/batch)
2018-03-22 15:38:13.420487: step 29110, loss = 0.71 (264.5 examples/sec; 0.484 sec/batch)
2018-03-22 15:38:18.170615: step 29120, loss = 0.75 (269.5 examples/sec; 0.475 sec/batch)
2018-03-22 15:38:23.223414: step 29130, loss = 0.92 (253.3 examples/sec; 0.505 sec/batch)
2018-03-22 15:38:28.489365: step 29140, loss = 0.76 (243.1 examples/sec; 0.527 sec/batch)
2018-03-22 15:38:33.616424: step 29150, loss = 0.87 (249.7 examples/sec; 0.513 sec/batch)
2018-03-22 15:38:38.448628: step 29160, loss = 0.79 (264.9 examples/sec; 0.483 sec/batch)
2018-03-22 15:38:43.051061: step 29170, loss = 0.71 (278.1 examples/sec; 0.460 sec/batch)
2018-03-22 15:38:48.481326: step 29180, loss = 0.72 (235.7 examples/sec; 0.543 sec/batch)
2018-03-22 15:38:53.254246: step 29190, loss = 0.78 (268.2 examples/sec; 0.477 sec/batch)
2018-03-22 15:38:58.175616: step 29200, loss = 0.80 (260.1 examples/sec; 0.492 sec/batch)
2018-03-22 15:39:03.214355: step 29210, loss = 0.71 (254.0 examples/sec; 0.504 sec/batch)
2018-03-22 15:39:07.924568: step 29220, loss = 0.92 (271.8 examples/sec; 0.471 sec/batch)
2018-03-22 15:39:12.836398: step 29230, loss = 0.86 (260.6 examples/sec; 0.491 sec/batch)
2018-03-22 15:39:18.046475: step 29240, loss = 0.80 (245.7 examples/sec; 0.521 sec/batch)
2018-03-22 15:39:23.186370: step 29250, loss = 0.74 (249.0 examples/sec; 0.514 sec/batch)
2018-03-22 15:39:28.422450: step 29260, loss = 0.76 (244.5 examples/sec; 0.524 sec/batch)
2018-03-22 15:39:33.674379: step 29270, loss = 0.77 (243.7 examples/sec; 0.525 sec/batch)
2018-03-22 15:39:38.471804: step 29280, loss = 0.95 (266.8 examples/sec; 0.480 sec/batch)
2018-03-22 15:39:43.585346: step 29290, loss = 0.74 (250.3 examples/sec; 0.511 sec/batch)
2018-03-22 15:39:48.555441: step 29300, loss = 0.77 (257.5 examples/sec; 0.497 sec/batch)
2018-03-22 15:39:53.651408: step 29310, loss = 0.74 (251.2 examples/sec; 0.510 sec/batch)
2018-03-22 15:39:58.840376: step 29320, loss = 0.81 (246.7 examples/sec; 0.519 sec/batch)
2018-03-22 15:40:04.119254: step 29330, loss = 0.76 (242.5 examples/sec; 0.528 sec/batch)
2018-03-22 15:40:08.648932: step 29340, loss = 0.74 (282.6 examples/sec; 0.453 sec/batch)
2018-03-22 15:40:13.689087: step 29350, loss = 0.81 (254.0 examples/sec; 0.504 sec/batch)
2018-03-22 15:40:18.926827: step 29360, loss = 0.90 (244.4 examples/sec; 0.524 sec/batch)
2018-03-22 15:40:24.523454: step 29370, loss = 0.87 (228.7 examples/sec; 0.560 sec/batch)
2018-03-22 15:40:29.436466: step 29380, loss = 0.80 (260.5 examples/sec; 0.491 sec/batch)
2018-03-22 15:40:34.188484: step 29390, loss = 0.83 (269.4 examples/sec; 0.475 sec/batch)
2018-03-22 15:40:39.322478: step 29400, loss = 0.69 (249.3 examples/sec; 0.513 sec/batch)
2018-03-22 15:40:44.372329: step 29410, loss = 0.65 (253.5 examples/sec; 0.505 sec/batch)
2018-03-22 15:40:48.615265: step 29420, loss = 0.93 (301.7 examples/sec; 0.424 sec/batch)
2018-03-22 15:40:54.132339: step 29430, loss = 0.81 (232.0 examples/sec; 0.552 sec/batch)
2018-03-22 15:40:59.371004: step 29440, loss = 0.95 (244.3 examples/sec; 0.524 sec/batch)
2018-03-22 15:41:04.945440: step 29450, loss = 0.76 (229.6 examples/sec; 0.557 sec/batch)
2018-03-22 15:41:10.431364: step 29460, loss = 0.87 (233.3 examples/sec; 0.549 sec/batch)
2018-03-22 15:41:14.955984: step 29470, loss = 0.79 (282.9 examples/sec; 0.452 sec/batch)
2018-03-22 15:41:20.603402: step 29480, loss = 0.61 (226.7 examples/sec; 0.565 sec/batch)
2018-03-22 15:41:25.486462: step 29490, loss = 0.78 (262.1 examples/sec; 0.488 sec/batch)
2018-03-22 15:41:30.980063: step 29500, loss = 0.64 (233.0 examples/sec; 0.549 sec/batch)
2018-03-22 15:41:35.529055: step 29510, loss = 0.92 (281.4 examples/sec; 0.455 sec/batch)
2018-03-22 15:41:41.039468: step 29520, loss = 0.83 (232.3 examples/sec; 0.551 sec/batch)
2018-03-22 15:41:46.383389: step 29530, loss = 0.72 (239.5 examples/sec; 0.534 sec/batch)
2018-03-22 15:41:51.447416: step 29540, loss = 0.81 (252.8 examples/sec; 0.506 sec/batch)
2018-03-22 15:41:56.304624: step 29550, loss = 1.09 (263.5 examples/sec; 0.486 sec/batch)
2018-03-22 15:42:01.683392: step 29560, loss = 0.75 (238.0 examples/sec; 0.538 sec/batch)
2018-03-22 15:42:06.948387: step 29570, loss = 0.75 (243.1 examples/sec; 0.526 sec/batch)
2018-03-22 15:42:11.814361: step 29580, loss = 0.81 (263.0 examples/sec; 0.487 sec/batch)
2018-03-22 15:42:16.878672: step 29590, loss = 0.74 (252.7 examples/sec; 0.506 sec/batch)
2018-03-22 15:42:22.340648: step 29600, loss = 0.79 (234.3 examples/sec; 0.546 sec/batch)
2018-03-22 15:42:27.253393: step 29610, loss = 0.69 (260.5 examples/sec; 0.491 sec/batch)
2018-03-22 15:42:32.608392: step 29620, loss = 0.63 (239.0 examples/sec; 0.536 sec/batch)
2018-03-22 15:42:37.569382: step 29630, loss = 0.89 (258.0 examples/sec; 0.496 sec/batch)
2018-03-22 15:42:43.369825: step 29640, loss = 0.87 (220.7 examples/sec; 0.580 sec/batch)
2018-03-22 15:42:47.535775: step 29650, loss = 0.71 (307.3 examples/sec; 0.417 sec/batch)
2018-03-22 15:42:52.530438: step 29660, loss = 0.61 (256.3 examples/sec; 0.499 sec/batch)
2018-03-22 15:42:57.543440: step 29670, loss = 0.60 (255.3 examples/sec; 0.501 sec/batch)
2018-03-22 15:43:02.454492: step 29680, loss = 0.82 (260.6 examples/sec; 0.491 sec/batch)
2018-03-22 15:43:07.731379: step 29690, loss = 0.90 (242.6 examples/sec; 0.528 sec/batch)
2018-03-22 15:43:12.943294: step 29700, loss = 0.70 (245.6 examples/sec; 0.521 sec/batch)
2018-03-22 15:43:17.470355: step 29710, loss = 0.63 (282.7 examples/sec; 0.453 sec/batch)
2018-03-22 15:43:22.954460: step 29720, loss = 0.86 (233.4 examples/sec; 0.548 sec/batch)
2018-03-22 15:43:28.159476: step 29730, loss = 0.74 (245.9 examples/sec; 0.520 sec/batch)
2018-03-22 15:43:33.183449: step 29740, loss = 0.77 (254.8 examples/sec; 0.502 sec/batch)
2018-03-22 15:43:38.579366: step 29750, loss = 0.73 (237.2 examples/sec; 0.540 sec/batch)
2018-03-22 15:43:43.851422: step 29760, loss = 0.95 (242.8 examples/sec; 0.527 sec/batch)
2018-03-22 15:43:48.373435: step 29770, loss = 0.81 (283.1 examples/sec; 0.452 sec/batch)
2018-03-22 15:43:53.384276: step 29780, loss = 0.75 (255.4 examples/sec; 0.501 sec/batch)
2018-03-22 15:43:58.262495: step 29790, loss = 0.82 (262.4 examples/sec; 0.488 sec/batch)
2018-03-22 15:44:04.158092: step 29800, loss = 0.70 (217.1 examples/sec; 0.590 sec/batch)
2018-03-22 15:44:08.858494: step 29810, loss = 0.73 (272.3 examples/sec; 0.470 sec/batch)
2018-03-22 15:44:14.200720: step 29820, loss = 0.88 (239.6 examples/sec; 0.534 sec/batch)
2018-03-22 15:44:18.977384: step 29830, loss = 0.73 (268.0 examples/sec; 0.478 sec/batch)
2018-03-22 15:44:24.101495: step 29840, loss = 1.02 (249.8 examples/sec; 0.512 sec/batch)
2018-03-22 15:44:28.537268: step 29850, loss = 0.78 (288.6 examples/sec; 0.444 sec/batch)
2018-03-22 15:44:33.764472: step 29860, loss = 0.92 (244.9 examples/sec; 0.523 sec/batch)
2018-03-22 15:44:38.483317: step 29870, loss = 0.89 (271.3 examples/sec; 0.472 sec/batch)
2018-03-22 15:44:43.572700: step 29880, loss = 0.90 (251.5 examples/sec; 0.509 sec/batch)
2018-03-22 15:44:48.577387: step 29890, loss = 0.60 (255.8 examples/sec; 0.500 sec/batch)
2018-03-22 15:44:53.783297: step 29900, loss = 0.74 (245.9 examples/sec; 0.521 sec/batch)
2018-03-22 15:44:58.745459: step 29910, loss = 0.82 (258.0 examples/sec; 0.496 sec/batch)
2018-03-22 15:45:04.502432: step 29920, loss = 0.93 (222.3 examples/sec; 0.576 sec/batch)
2018-03-22 15:45:09.304711: step 29930, loss = 0.86 (266.5 examples/sec; 0.480 sec/batch)
2018-03-22 15:45:14.390548: step 29940, loss = 0.85 (251.7 examples/sec; 0.509 sec/batch)
2018-03-22 15:45:20.277402: step 29950, loss = 0.83 (217.4 examples/sec; 0.589 sec/batch)
2018-03-22 15:45:25.318410: step 29960, loss = 0.73 (253.9 examples/sec; 0.504 sec/batch)
2018-03-22 15:45:30.384434: step 29970, loss = 0.77 (252.7 examples/sec; 0.507 sec/batch)
2018-03-22 15:45:35.394807: step 29980, loss = 0.98 (255.5 examples/sec; 0.501 sec/batch)
2018-03-22 15:45:40.128244: step 29990, loss = 0.76 (270.4 examples/sec; 0.473 sec/batch)
2018-03-22 15:45:45.700041: step 30000, loss = 0.75 (229.7 examples/sec; 0.557 sec/batch)
2018-03-22 15:45:50.916443: step 30010, loss = 0.89 (245.4 examples/sec; 0.522 sec/batch)
2018-03-22 15:45:55.705278: step 30020, loss = 0.82 (267.3 examples/sec; 0.479 sec/batch)
2018-03-22 15:46:00.453284: step 30030, loss = 0.86 (269.6 examples/sec; 0.475 sec/batch)
2018-03-22 15:46:05.436417: step 30040, loss = 0.73 (256.9 examples/sec; 0.498 sec/batch)
2018-03-22 15:46:10.783070: step 30050, loss = 0.90 (239.4 examples/sec; 0.535 sec/batch)
2018-03-22 15:46:16.145083: step 30060, loss = 0.83 (238.7 examples/sec; 0.536 sec/batch)
2018-03-22 15:46:21.372851: step 30070, loss = 0.87 (244.8 examples/sec; 0.523 sec/batch)
2018-03-22 15:46:26.188321: step 30080, loss = 0.73 (265.8 examples/sec; 0.482 sec/batch)
2018-03-22 15:46:31.339661: step 30090, loss = 0.82 (248.5 examples/sec; 0.515 sec/batch)
2018-03-22 15:46:36.221066: step 30100, loss = 0.69 (262.2 examples/sec; 0.488 sec/batch)
2018-03-22 15:46:41.669023: step 30110, loss = 0.78 (234.9 examples/sec; 0.545 sec/batch)
2018-03-22 15:46:46.410734: step 30120, loss = 0.71 (269.9 examples/sec; 0.474 sec/batch)
2018-03-22 15:46:51.917368: step 30130, loss = 0.72 (232.4 examples/sec; 0.551 sec/batch)
2018-03-22 15:46:57.193741: step 30140, loss = 0.66 (242.6 examples/sec; 0.528 sec/batch)
2018-03-22 15:47:01.993025: step 30150, loss = 0.68 (266.7 examples/sec; 0.480 sec/batch)
2018-03-22 15:47:06.673239: step 30160, loss = 0.69 (273.5 examples/sec; 0.468 sec/batch)
2018-03-22 15:47:11.957454: step 30170, loss = 0.86 (242.2 examples/sec; 0.528 sec/batch)
2018-03-22 15:47:17.299370: step 30180, loss = 0.91 (239.6 examples/sec; 0.534 sec/batch)
2018-03-22 15:47:22.173047: step 30190, loss = 0.78 (262.6 examples/sec; 0.487 sec/batch)
2018-03-22 15:47:27.402483: step 30200, loss = 0.82 (244.8 examples/sec; 0.523 sec/batch)
2018-03-22 15:47:32.139296: step 30210, loss = 0.96 (270.2 examples/sec; 0.474 sec/batch)
2018-03-22 15:47:37.073714: step 30220, loss = 0.79 (259.4 examples/sec; 0.493 sec/batch)
2018-03-22 15:47:41.973379: step 30230, loss = 0.94 (261.2 examples/sec; 0.490 sec/batch)
2018-03-22 15:47:46.498398: step 30240, loss = 0.85 (282.9 examples/sec; 0.453 sec/batch)
2018-03-22 15:47:52.006033: step 30250, loss = 0.72 (232.4 examples/sec; 0.551 sec/batch)
2018-03-22 15:47:56.944428: step 30260, loss = 0.67 (259.2 examples/sec; 0.494 sec/batch)
2018-03-22 15:48:02.231342: step 30270, loss = 0.92 (242.1 examples/sec; 0.529 sec/batch)
2018-03-22 15:48:07.085581: step 30280, loss = 0.75 (263.7 examples/sec; 0.485 sec/batch)
2018-03-22 15:48:12.513469: step 30290, loss = 0.86 (235.8 examples/sec; 0.543 sec/batch)
2018-03-22 15:48:17.833128: step 30300, loss = 0.84 (240.6 examples/sec; 0.532 sec/batch)
2018-03-22 15:48:23.243118: step 30310, loss = 0.71 (236.6 examples/sec; 0.541 sec/batch)
2018-03-22 15:48:28.142349: step 30320, loss = 0.77 (261.3 examples/sec; 0.490 sec/batch)
2018-03-22 15:48:33.033402: step 30330, loss = 0.84 (261.7 examples/sec; 0.489 sec/batch)
2018-03-22 15:48:38.379610: step 30340, loss = 0.83 (239.4 examples/sec; 0.535 sec/batch)
2018-03-22 15:48:43.830435: step 30350, loss = 0.71 (234.8 examples/sec; 0.545 sec/batch)
2018-03-22 15:48:49.095414: step 30360, loss = 0.76 (243.1 examples/sec; 0.526 sec/batch)
2018-03-22 15:48:54.669397: step 30370, loss = 0.90 (229.6 examples/sec; 0.557 sec/batch)
2018-03-22 15:49:00.077324: step 30380, loss = 0.79 (236.7 examples/sec; 0.541 sec/batch)
2018-03-22 15:49:04.818466: step 30390, loss = 0.77 (270.0 examples/sec; 0.474 sec/batch)
2018-03-22 15:49:10.202347: step 30400, loss = 0.80 (237.7 examples/sec; 0.538 sec/batch)
2018-03-22 15:49:15.219019: step 30410, loss = 0.83 (255.1 examples/sec; 0.502 sec/batch)
2018-03-22 15:49:19.856430: step 30420, loss = 0.80 (276.0 examples/sec; 0.464 sec/batch)
2018-03-22 15:49:24.726376: step 30430, loss = 0.72 (262.8 examples/sec; 0.487 sec/batch)
2018-03-22 15:49:29.935412: step 30440, loss = 0.67 (245.7 examples/sec; 0.521 sec/batch)
2018-03-22 15:49:34.992386: step 30450, loss = 0.75 (253.1 examples/sec; 0.506 sec/batch)
2018-03-22 15:49:39.764533: step 30460, loss = 0.90 (268.2 examples/sec; 0.477 sec/batch)
2018-03-22 15:49:44.517303: step 30470, loss = 0.67 (269.3 examples/sec; 0.475 sec/batch)
2018-03-22 15:49:49.384321: step 30480, loss = 0.82 (263.0 examples/sec; 0.487 sec/batch)
2018-03-22 15:49:54.431359: step 30490, loss = 0.74 (253.6 examples/sec; 0.505 sec/batch)
2018-03-22 15:50:00.236819: step 30500, loss = 0.75 (220.5 examples/sec; 0.581 sec/batch)
2018-03-22 15:50:04.899384: step 30510, loss = 0.83 (274.5 examples/sec; 0.466 sec/batch)
2018-03-22 15:50:09.599881: step 30520, loss = 0.66 (272.3 examples/sec; 0.470 sec/batch)
2018-03-22 15:50:14.294279: step 30530, loss = 0.73 (272.7 examples/sec; 0.469 sec/batch)
2018-03-22 15:50:19.323342: step 30540, loss = 0.69 (254.5 examples/sec; 0.503 sec/batch)
2018-03-22 15:50:24.689402: step 30550, loss = 0.74 (238.5 examples/sec; 0.537 sec/batch)
2018-03-22 15:50:29.463487: step 30560, loss = 0.84 (268.1 examples/sec; 0.477 sec/batch)
2018-03-22 15:50:34.377426: step 30570, loss = 0.70 (260.5 examples/sec; 0.491 sec/batch)
2018-03-22 15:50:39.749444: step 30580, loss = 0.91 (238.3 examples/sec; 0.537 sec/batch)
2018-03-22 15:50:44.770473: step 30590, loss = 0.82 (254.9 examples/sec; 0.502 sec/batch)
2018-03-22 15:50:49.656838: step 30600, loss = 0.54 (262.0 examples/sec; 0.489 sec/batch)
2018-03-22 15:50:55.051434: step 30610, loss = 0.75 (237.3 examples/sec; 0.539 sec/batch)
2018-03-22 15:51:00.121368: step 30620, loss = 0.64 (252.5 examples/sec; 0.507 sec/batch)
2018-03-22 15:51:05.476645: step 30630, loss = 0.82 (239.0 examples/sec; 0.536 sec/batch)
2018-03-22 15:51:10.471395: step 30640, loss = 0.67 (256.3 examples/sec; 0.499 sec/batch)
2018-03-22 15:51:15.397333: step 30650, loss = 0.76 (259.8 examples/sec; 0.493 sec/batch)
2018-03-22 15:51:20.118391: step 30660, loss = 0.75 (271.1 examples/sec; 0.472 sec/batch)
2018-03-22 15:51:25.192777: step 30670, loss = 0.84 (252.2 examples/sec; 0.507 sec/batch)
2018-03-22 15:51:30.644440: step 30680, loss = 0.82 (234.8 examples/sec; 0.545 sec/batch)
2018-03-22 15:51:35.457355: step 30690, loss = 0.77 (266.0 examples/sec; 0.481 sec/batch)
2018-03-22 15:51:40.782354: step 30700, loss = 0.87 (240.4 examples/sec; 0.532 sec/batch)
2018-03-22 15:51:45.660447: step 30710, loss = 0.73 (262.4 examples/sec; 0.488 sec/batch)
2018-03-22 15:51:51.015996: step 30720, loss = 0.72 (239.0 examples/sec; 0.536 sec/batch)
2018-03-22 15:51:55.554554: step 30730, loss = 0.73 (282.0 examples/sec; 0.454 sec/batch)
2018-03-22 15:52:01.155477: step 30740, loss = 0.73 (228.5 examples/sec; 0.560 sec/batch)
2018-03-22 15:52:06.166708: step 30750, loss = 0.89 (255.4 examples/sec; 0.501 sec/batch)
2018-03-22 15:52:11.111415: step 30760, loss = 0.79 (258.9 examples/sec; 0.494 sec/batch)
2018-03-22 15:52:15.909427: step 30770, loss = 0.67 (266.8 examples/sec; 0.480 sec/batch)
2018-03-22 15:52:21.517415: step 30780, loss = 0.77 (228.2 examples/sec; 0.561 sec/batch)
2018-03-22 15:52:26.081441: step 30790, loss = 0.87 (280.5 examples/sec; 0.456 sec/batch)
2018-03-22 15:52:31.494615: step 30800, loss = 0.72 (236.5 examples/sec; 0.541 sec/batch)
2018-03-22 15:52:36.781388: step 30810, loss = 0.76 (242.1 examples/sec; 0.529 sec/batch)
2018-03-22 15:52:41.991406: step 30820, loss = 0.68 (245.7 examples/sec; 0.521 sec/batch)
2018-03-22 15:52:47.220392: step 30830, loss = 0.78 (244.8 examples/sec; 0.523 sec/batch)
2018-03-22 15:52:52.557420: step 30840, loss = 0.99 (239.8 examples/sec; 0.534 sec/batch)
2018-03-22 15:52:56.957200: step 30850, loss = 0.82 (290.9 examples/sec; 0.440 sec/batch)
2018-03-22 15:53:02.385512: step 30860, loss = 0.70 (235.8 examples/sec; 0.543 sec/batch)
2018-03-22 15:53:07.503834: step 30870, loss = 0.81 (250.1 examples/sec; 0.512 sec/batch)
2018-03-22 15:53:13.198462: step 30880, loss = 0.97 (224.8 examples/sec; 0.569 sec/batch)
2018-03-22 15:53:17.985368: step 30890, loss = 0.79 (267.4 examples/sec; 0.479 sec/batch)
2018-03-22 15:53:23.163860: step 30900, loss = 0.66 (247.2 examples/sec; 0.518 sec/batch)
2018-03-22 15:53:27.947414: step 30910, loss = 0.69 (267.6 examples/sec; 0.478 sec/batch)
2018-03-22 15:53:33.059533: step 30920, loss = 0.77 (250.4 examples/sec; 0.511 sec/batch)
2018-03-22 15:53:38.530399: step 30930, loss = 0.83 (234.0 examples/sec; 0.547 sec/batch)
2018-03-22 15:53:44.008719: step 30940, loss = 0.94 (233.6 examples/sec; 0.548 sec/batch)
2018-03-22 15:53:48.405098: step 30950, loss = 0.80 (291.1 examples/sec; 0.440 sec/batch)
2018-03-22 15:53:53.356674: step 30960, loss = 0.75 (258.5 examples/sec; 0.495 sec/batch)
2018-03-22 15:53:57.564226: step 30970, loss = 0.78 (304.2 examples/sec; 0.421 sec/batch)
2018-03-22 15:54:02.540421: step 30980, loss = 0.75 (257.2 examples/sec; 0.498 sec/batch)
2018-03-22 15:54:07.160379: step 30990, loss = 0.85 (277.1 examples/sec; 0.462 sec/batch)
2018-03-22 15:54:12.624712: step 31000, loss = 0.80 (234.2 examples/sec; 0.546 sec/batch)
2018-03-22 15:54:17.599413: step 31010, loss = 0.86 (257.3 examples/sec; 0.497 sec/batch)
2018-03-22 15:54:22.861473: step 31020, loss = 0.88 (243.3 examples/sec; 0.526 sec/batch)
2018-03-22 15:54:27.646443: step 31030, loss = 0.83 (267.5 examples/sec; 0.478 sec/batch)
2018-03-22 15:54:32.697335: step 31040, loss = 0.86 (253.4 examples/sec; 0.505 sec/batch)
2018-03-22 15:54:37.809717: step 31050, loss = 0.77 (250.4 examples/sec; 0.511 sec/batch)
2018-03-22 15:54:43.283373: step 31060, loss = 0.69 (233.8 examples/sec; 0.547 sec/batch)
2018-03-22 15:54:48.307346: step 31070, loss = 0.71 (254.8 examples/sec; 0.502 sec/batch)
2018-03-22 15:54:53.706407: step 31080, loss = 0.99 (237.1 examples/sec; 0.540 sec/batch)
2018-03-22 15:54:58.981674: step 31090, loss = 0.69 (242.6 examples/sec; 0.528 sec/batch)
2018-03-22 15:55:04.057137: step 31100, loss = 0.82 (252.2 examples/sec; 0.508 sec/batch)
2018-03-22 15:55:09.168456: step 31110, loss = 0.75 (250.4 examples/sec; 0.511 sec/batch)
2018-03-22 15:55:14.451371: step 31120, loss = 0.77 (242.3 examples/sec; 0.528 sec/batch)
2018-03-22 15:55:19.150423: step 31130, loss = 0.69 (272.4 examples/sec; 0.470 sec/batch)
2018-03-22 15:55:23.970221: step 31140, loss = 0.61 (265.6 examples/sec; 0.482 sec/batch)
2018-03-22 15:55:30.058403: step 31150, loss = 0.72 (210.2 examples/sec; 0.609 sec/batch)
2018-03-22 15:55:35.309398: step 31160, loss = 0.76 (243.8 examples/sec; 0.525 sec/batch)
2018-03-22 15:55:40.625439: step 31170, loss = 0.60 (240.8 examples/sec; 0.532 sec/batch)
2018-03-22 15:55:45.504395: step 31180, loss = 0.73 (262.4 examples/sec; 0.488 sec/batch)
2018-03-22 15:55:50.153440: step 31190, loss = 0.70 (275.3 examples/sec; 0.465 sec/batch)
2018-03-22 15:55:55.909562: step 31200, loss = 0.73 (222.4 examples/sec; 0.576 sec/batch)
2018-03-22 15:56:01.271426: step 31210, loss = 0.70 (238.7 examples/sec; 0.536 sec/batch)
2018-03-22 15:56:06.005444: step 31220, loss = 0.85 (270.4 examples/sec; 0.473 sec/batch)
2018-03-22 15:56:11.225652: step 31230, loss = 0.79 (245.2 examples/sec; 0.522 sec/batch)
2018-03-22 15:56:16.210360: step 31240, loss = 0.80 (256.8 examples/sec; 0.498 sec/batch)
2018-03-22 15:56:21.117702: step 31250, loss = 0.86 (260.8 examples/sec; 0.491 sec/batch)
2018-03-22 15:56:26.193016: step 31260, loss = 0.88 (252.2 examples/sec; 0.508 sec/batch)
2018-03-22 15:56:31.082702: step 31270, loss = 0.69 (261.8 examples/sec; 0.489 sec/batch)
2018-03-22 15:56:36.143444: step 31280, loss = 0.82 (252.9 examples/sec; 0.506 sec/batch)
2018-03-22 15:56:41.501364: step 31290, loss = 0.92 (238.9 examples/sec; 0.536 sec/batch)
2018-03-22 15:56:46.329042: step 31300, loss = 0.81 (265.1 examples/sec; 0.483 sec/batch)
2018-03-22 15:56:51.348166: step 31310, loss = 0.86 (255.0 examples/sec; 0.502 sec/batch)
2018-03-22 15:56:56.180511: step 31320, loss = 0.87 (264.9 examples/sec; 0.483 sec/batch)
2018-03-22 15:57:01.038131: step 31330, loss = 0.73 (263.5 examples/sec; 0.486 sec/batch)
2018-03-22 15:57:06.084301: step 31340, loss = 0.75 (253.7 examples/sec; 0.505 sec/batch)
2018-03-22 15:57:11.131396: step 31350, loss = 0.99 (253.6 examples/sec; 0.505 sec/batch)
2018-03-22 15:57:16.286380: step 31360, loss = 0.98 (248.3 examples/sec; 0.515 sec/batch)
2018-03-22 15:57:21.501720: step 31370, loss = 0.66 (245.4 examples/sec; 0.522 sec/batch)
2018-03-22 15:57:26.063472: step 31380, loss = 0.76 (280.6 examples/sec; 0.456 sec/batch)
2018-03-22 15:57:31.395355: step 31390, loss = 0.84 (240.1 examples/sec; 0.533 sec/batch)
2018-03-22 15:57:36.478547: step 31400, loss = 0.71 (251.8 examples/sec; 0.508 sec/batch)
2018-03-22 15:57:41.099332: step 31410, loss = 0.74 (277.0 examples/sec; 0.462 sec/batch)
2018-03-22 15:57:45.275767: step 31420, loss = 0.76 (306.5 examples/sec; 0.418 sec/batch)
2018-03-22 15:57:50.255324: step 31430, loss = 0.82 (257.1 examples/sec; 0.498 sec/batch)
2018-03-22 15:57:55.148424: step 31440, loss = 0.78 (261.6 examples/sec; 0.489 sec/batch)
2018-03-22 15:58:00.674422: step 31450, loss = 0.86 (231.6 examples/sec; 0.553 sec/batch)
2018-03-22 15:58:05.883413: step 31460, loss = 0.77 (245.7 examples/sec; 0.521 sec/batch)
2018-03-22 15:58:10.987445: step 31470, loss = 0.76 (250.8 examples/sec; 0.510 sec/batch)
2018-03-22 15:58:15.920439: step 31480, loss = 0.84 (259.5 examples/sec; 0.493 sec/batch)
2018-03-22 15:58:21.483089: step 31490, loss = 0.76 (230.1 examples/sec; 0.556 sec/batch)
2018-03-22 15:58:26.683360: step 31500, loss = 0.81 (246.1 examples/sec; 0.520 sec/batch)
2018-03-22 15:58:31.587455: step 31510, loss = 0.87 (261.0 examples/sec; 0.490 sec/batch)
2018-03-22 15:58:36.756282: step 31520, loss = 0.83 (247.6 examples/sec; 0.517 sec/batch)
2018-03-22 15:58:42.265488: step 31530, loss = 0.73 (232.3 examples/sec; 0.551 sec/batch)
2018-03-22 15:58:46.517388: step 31540, loss = 0.70 (301.0 examples/sec; 0.425 sec/batch)
2018-03-22 15:58:51.515445: step 31550, loss = 0.83 (256.1 examples/sec; 0.500 sec/batch)
2018-03-22 15:58:56.307482: step 31560, loss = 0.90 (267.1 examples/sec; 0.479 sec/batch)
2018-03-22 15:59:01.291428: step 31570, loss = 0.74 (256.8 examples/sec; 0.498 sec/batch)
2018-03-22 15:59:05.929450: step 31580, loss = 0.72 (276.0 examples/sec; 0.464 sec/batch)
2018-03-22 15:59:10.946590: step 31590, loss = 0.77 (255.1 examples/sec; 0.502 sec/batch)
2018-03-22 15:59:16.310354: step 31600, loss = 0.71 (238.6 examples/sec; 0.536 sec/batch)
2018-03-22 15:59:21.595616: step 31610, loss = 0.69 (242.2 examples/sec; 0.529 sec/batch)
2018-03-22 15:59:25.861322: step 31620, loss = 0.77 (300.1 examples/sec; 0.427 sec/batch)
2018-03-22 15:59:30.891623: step 31630, loss = 0.88 (254.5 examples/sec; 0.503 sec/batch)
2018-03-22 15:59:35.803471: step 31640, loss = 0.80 (260.6 examples/sec; 0.491 sec/batch)
2018-03-22 15:59:41.318754: step 31650, loss = 0.72 (232.1 examples/sec; 0.552 sec/batch)
2018-03-22 15:59:46.309320: step 31660, loss = 0.83 (256.5 examples/sec; 0.499 sec/batch)
2018-03-22 15:59:51.759387: step 31670, loss = 0.75 (234.9 examples/sec; 0.545 sec/batch)
2018-03-22 15:59:56.390119: step 31680, loss = 0.75 (276.4 examples/sec; 0.463 sec/batch)
2018-03-22 16:00:01.795426: step 31690, loss = 0.90 (236.8 examples/sec; 0.541 sec/batch)
2018-03-22 16:00:07.573482: step 31700, loss = 0.88 (221.5 examples/sec; 0.578 sec/batch)
2018-03-22 16:00:12.924361: step 31710, loss = 0.79 (239.2 examples/sec; 0.535 sec/batch)
2018-03-22 16:00:17.847544: step 31720, loss = 0.65 (260.0 examples/sec; 0.492 sec/batch)
2018-03-22 16:00:23.091454: step 31730, loss = 0.67 (244.1 examples/sec; 0.524 sec/batch)
2018-03-22 16:00:27.952993: step 31740, loss = 0.82 (263.3 examples/sec; 0.486 sec/batch)
2018-03-22 16:00:32.998336: step 31750, loss = 0.72 (253.7 examples/sec; 0.505 sec/batch)
2018-03-22 16:00:38.003403: step 31760, loss = 0.80 (255.7 examples/sec; 0.501 sec/batch)
2018-03-22 16:00:42.766398: step 31770, loss = 0.73 (268.7 examples/sec; 0.476 sec/batch)
2018-03-22 16:00:47.800358: step 31780, loss = 0.78 (254.3 examples/sec; 0.503 sec/batch)
2018-03-22 16:00:53.154581: step 31790, loss = 0.88 (239.1 examples/sec; 0.535 sec/batch)
2018-03-22 16:00:57.965895: step 31800, loss = 0.84 (266.0 examples/sec; 0.481 sec/batch)
2018-03-22 16:01:03.162472: step 31810, loss = 0.86 (246.3 examples/sec; 0.520 sec/batch)
2018-03-22 16:01:08.194750: step 31820, loss = 0.88 (254.4 examples/sec; 0.503 sec/batch)
2018-03-22 16:01:13.357311: step 31830, loss = 0.70 (247.9 examples/sec; 0.516 sec/batch)
2018-03-22 16:01:18.484984: step 31840, loss = 0.77 (249.6 examples/sec; 0.513 sec/batch)
2018-03-22 16:01:23.355530: step 31850, loss = 0.74 (262.8 examples/sec; 0.487 sec/batch)
2018-03-22 16:01:28.133373: step 31860, loss = 0.90 (267.9 examples/sec; 0.478 sec/batch)
2018-03-22 16:01:32.876372: step 31870, loss = 0.77 (269.9 examples/sec; 0.474 sec/batch)
2018-03-22 16:01:37.389351: step 31880, loss = 0.63 (283.6 examples/sec; 0.451 sec/batch)
2018-03-22 16:01:42.474713: step 31890, loss = 0.72 (251.7 examples/sec; 0.509 sec/batch)
2018-03-22 16:01:47.922024: step 31900, loss = 0.77 (235.0 examples/sec; 0.545 sec/batch)
2018-03-22 16:01:53.354445: step 31910, loss = 0.69 (235.6 examples/sec; 0.543 sec/batch)
2018-03-22 16:01:58.086880: step 31920, loss = 0.81 (270.5 examples/sec; 0.473 sec/batch)
2018-03-22 16:02:02.611970: step 31930, loss = 0.71 (282.9 examples/sec; 0.453 sec/batch)
2018-03-22 16:02:07.526370: step 31940, loss = 1.04 (260.5 examples/sec; 0.491 sec/batch)
2018-03-22 16:02:12.451707: step 31950, loss = 0.86 (259.9 examples/sec; 0.493 sec/batch)
2018-03-22 16:02:17.336045: step 31960, loss = 0.90 (262.1 examples/sec; 0.488 sec/batch)
2018-03-22 16:02:22.038689: step 31970, loss = 0.82 (272.2 examples/sec; 0.470 sec/batch)
2018-03-22 16:02:27.004452: step 31980, loss = 0.88 (257.8 examples/sec; 0.497 sec/batch)
2018-03-22 16:02:32.117252: step 31990, loss = 0.71 (250.4 examples/sec; 0.511 sec/batch)
2018-03-22 16:02:36.969517: step 32000, loss = 0.94 (263.8 examples/sec; 0.485 sec/batch)
2018-03-22 16:02:41.902697: step 32010, loss = 0.72 (259.5 examples/sec; 0.493 sec/batch)
2018-03-22 16:02:46.689442: step 32020, loss = 0.71 (267.4 examples/sec; 0.479 sec/batch)
2018-03-22 16:02:52.036361: step 32030, loss = 0.74 (239.4 examples/sec; 0.535 sec/batch)
2018-03-22 16:02:56.491366: step 32040, loss = 0.90 (287.3 examples/sec; 0.446 sec/batch)
2018-03-22 16:03:01.634138: step 32050, loss = 0.68 (248.9 examples/sec; 0.514 sec/batch)
2018-03-22 16:03:06.676379: step 32060, loss = 0.77 (253.9 examples/sec; 0.504 sec/batch)
2018-03-22 16:03:12.002639: step 32070, loss = 0.71 (240.3 examples/sec; 0.533 sec/batch)
2018-03-22 16:03:17.006506: step 32080, loss = 0.87 (255.8 examples/sec; 0.500 sec/batch)
2018-03-22 16:03:22.182332: step 32090, loss = 0.83 (247.3 examples/sec; 0.518 sec/batch)
2018-03-22 16:03:27.270132: step 32100, loss = 0.97 (251.6 examples/sec; 0.509 sec/batch)
2018-03-22 16:03:32.720473: step 32110, loss = 0.64 (234.8 examples/sec; 0.545 sec/batch)
2018-03-22 16:03:38.192431: step 32120, loss = 0.84 (233.9 examples/sec; 0.547 sec/batch)
2018-03-22 16:03:43.301355: step 32130, loss = 0.86 (250.5 examples/sec; 0.511 sec/batch)
2018-03-22 16:03:48.497012: step 32140, loss = 0.81 (246.4 examples/sec; 0.520 sec/batch)
2018-03-22 16:03:53.317364: step 32150, loss = 0.67 (265.5 examples/sec; 0.482 sec/batch)
2018-03-22 16:03:58.519593: step 32160, loss = 0.84 (246.0 examples/sec; 0.520 sec/batch)
2018-03-22 16:04:03.888408: step 32170, loss = 0.67 (238.4 examples/sec; 0.537 sec/batch)
2018-03-22 16:04:08.415982: step 32180, loss = 0.87 (282.7 examples/sec; 0.453 sec/batch)
2018-03-22 16:04:13.050348: step 32190, loss = 0.73 (276.2 examples/sec; 0.463 sec/batch)
2018-03-22 16:04:18.342155: step 32200, loss = 0.74 (241.9 examples/sec; 0.529 sec/batch)
2018-03-22 16:04:23.869443: step 32210, loss = 0.83 (231.6 examples/sec; 0.553 sec/batch)
2018-03-22 16:04:28.370384: step 32220, loss = 0.84 (284.4 examples/sec; 0.450 sec/batch)
2018-03-22 16:04:33.676983: step 32230, loss = 0.84 (241.2 examples/sec; 0.531 sec/batch)
2018-03-22 16:04:38.867418: step 32240, loss = 0.79 (246.6 examples/sec; 0.519 sec/batch)
2018-03-22 16:04:44.198435: step 32250, loss = 0.76 (240.1 examples/sec; 0.533 sec/batch)
2018-03-22 16:04:49.474360: step 32260, loss = 0.92 (242.6 examples/sec; 0.528 sec/batch)
2018-03-22 16:04:54.603433: step 32270, loss = 0.68 (249.6 examples/sec; 0.513 sec/batch)
2018-03-22 16:04:59.942699: step 32280, loss = 0.80 (239.7 examples/sec; 0.534 sec/batch)
2018-03-22 16:05:05.283432: step 32290, loss = 0.86 (239.7 examples/sec; 0.534 sec/batch)
2018-03-22 16:05:10.121086: step 32300, loss = 0.73 (264.6 examples/sec; 0.484 sec/batch)
2018-03-22 16:05:14.484539: step 32310, loss = 0.79 (293.3 examples/sec; 0.436 sec/batch)
2018-03-22 16:05:18.991214: step 32320, loss = 0.57 (284.1 examples/sec; 0.451 sec/batch)
2018-03-22 16:05:23.794710: step 32330, loss = 0.72 (266.4 examples/sec; 0.480 sec/batch)
2018-03-22 16:05:28.752378: step 32340, loss = 0.86 (258.2 examples/sec; 0.496 sec/batch)
2018-03-22 16:05:33.718398: step 32350, loss = 0.73 (257.8 examples/sec; 0.497 sec/batch)
2018-03-22 16:05:39.316431: step 32360, loss = 0.78 (228.7 examples/sec; 0.560 sec/batch)
2018-03-22 16:05:44.139831: step 32370, loss = 0.83 (265.4 examples/sec; 0.482 sec/batch)
2018-03-22 16:05:48.537984: step 32380, loss = 0.81 (291.0 examples/sec; 0.440 sec/batch)
2018-03-22 16:05:53.878287: step 32390, loss = 0.73 (239.7 examples/sec; 0.534 sec/batch)
2018-03-22 16:05:59.064002: step 32400, loss = 0.77 (246.8 examples/sec; 0.519 sec/batch)
2018-03-22 16:06:03.799038: step 32410, loss = 0.81 (270.3 examples/sec; 0.474 sec/batch)
2018-03-22 16:06:09.286305: step 32420, loss = 0.80 (233.3 examples/sec; 0.549 sec/batch)
2018-03-22 16:06:14.871990: step 32430, loss = 0.79 (229.2 examples/sec; 0.559 sec/batch)
2018-03-22 16:06:19.577363: step 32440, loss = 0.80 (272.0 examples/sec; 0.471 sec/batch)
2018-03-22 16:06:24.397237: step 32450, loss = 0.74 (265.6 examples/sec; 0.482 sec/batch)
2018-03-22 16:06:29.404525: step 32460, loss = 0.77 (255.6 examples/sec; 0.501 sec/batch)
2018-03-22 16:06:33.865300: step 32470, loss = 0.79 (286.9 examples/sec; 0.446 sec/batch)
2018-03-22 16:06:39.328371: step 32480, loss = 0.86 (234.3 examples/sec; 0.546 sec/batch)
2018-03-22 16:06:43.839427: step 32490, loss = 0.82 (283.7 examples/sec; 0.451 sec/batch)
2018-03-22 16:06:48.430593: step 32500, loss = 0.83 (278.8 examples/sec; 0.459 sec/batch)
2018-03-22 16:06:53.495032: step 32510, loss = 0.77 (252.7 examples/sec; 0.506 sec/batch)
2018-03-22 16:06:58.892363: step 32520, loss = 0.67 (237.2 examples/sec; 0.540 sec/batch)
2018-03-22 16:07:04.083395: step 32530, loss = 1.01 (246.6 examples/sec; 0.519 sec/batch)
2018-03-22 16:07:09.040370: step 32540, loss = 0.74 (258.2 examples/sec; 0.496 sec/batch)
2018-03-22 16:07:13.909401: step 32550, loss = 0.70 (262.9 examples/sec; 0.487 sec/batch)
2018-03-22 16:07:19.600427: step 32560, loss = 0.75 (224.9 examples/sec; 0.569 sec/batch)
2018-03-22 16:07:24.615380: step 32570, loss = 0.73 (255.2 examples/sec; 0.501 sec/batch)
2018-03-22 16:07:29.093592: step 32580, loss = 0.66 (285.8 examples/sec; 0.448 sec/batch)
2018-03-22 16:07:34.177785: step 32590, loss = 0.70 (251.8 examples/sec; 0.508 sec/batch)
2018-03-22 16:07:39.728449: step 32600, loss = 0.81 (230.6 examples/sec; 0.555 sec/batch)
2018-03-22 16:07:44.809825: step 32610, loss = 0.71 (251.9 examples/sec; 0.508 sec/batch)
2018-03-22 16:07:49.663423: step 32620, loss = 0.87 (263.7 examples/sec; 0.485 sec/batch)
2018-03-22 16:07:54.628842: step 32630, loss = 0.80 (257.8 examples/sec; 0.497 sec/batch)
2018-03-22 16:07:59.835787: step 32640, loss = 0.71 (245.8 examples/sec; 0.521 sec/batch)
2018-03-22 16:08:04.893449: step 32650, loss = 0.65 (253.1 examples/sec; 0.506 sec/batch)
2018-03-22 16:08:09.749317: step 32660, loss = 0.74 (263.6 examples/sec; 0.486 sec/batch)
2018-03-22 16:08:14.425805: step 32670, loss = 0.80 (273.7 examples/sec; 0.468 sec/batch)
2018-03-22 16:08:19.504489: step 32680, loss = 0.78 (252.0 examples/sec; 0.508 sec/batch)
2018-03-22 16:08:24.665324: step 32690, loss = 0.85 (248.0 examples/sec; 0.516 sec/batch)
2018-03-22 16:08:30.130370: step 32700, loss = 0.73 (234.2 examples/sec; 0.547 sec/batch)
2018-03-22 16:08:34.489226: step 32710, loss = 0.80 (293.7 examples/sec; 0.436 sec/batch)
2018-03-22 16:08:39.782423: step 32720, loss = 0.86 (241.8 examples/sec; 0.529 sec/batch)
2018-03-22 16:08:45.134368: step 32730, loss = 0.70 (239.2 examples/sec; 0.535 sec/batch)
2018-03-22 16:08:50.082369: step 32740, loss = 0.69 (258.7 examples/sec; 0.495 sec/batch)
2018-03-22 16:08:55.107357: step 32750, loss = 0.82 (254.7 examples/sec; 0.502 sec/batch)
2018-03-22 16:08:59.532385: step 32760, loss = 0.71 (289.3 examples/sec; 0.443 sec/batch)
2018-03-22 16:09:04.569859: step 32770, loss = 0.92 (254.1 examples/sec; 0.504 sec/batch)
2018-03-22 16:09:09.054354: step 32780, loss = 0.85 (285.4 examples/sec; 0.448 sec/batch)
2018-03-22 16:09:14.019936: step 32790, loss = 0.90 (257.8 examples/sec; 0.497 sec/batch)
2018-03-22 16:09:19.384207: step 32800, loss = 0.72 (238.6 examples/sec; 0.536 sec/batch)
2018-03-22 16:09:25.190315: step 32810, loss = 0.66 (220.5 examples/sec; 0.581 sec/batch)
2018-03-22 16:09:30.647342: step 32820, loss = 0.83 (234.6 examples/sec; 0.546 sec/batch)
2018-03-22 16:09:35.277394: step 32830, loss = 0.81 (276.5 examples/sec; 0.463 sec/batch)
2018-03-22 16:09:40.285690: step 32840, loss = 0.72 (255.6 examples/sec; 0.501 sec/batch)
2018-03-22 16:09:45.080310: step 32850, loss = 0.77 (267.0 examples/sec; 0.479 sec/batch)
2018-03-22 16:09:50.287637: step 32860, loss = 0.67 (245.8 examples/sec; 0.521 sec/batch)
2018-03-22 16:09:55.325489: step 32870, loss = 0.75 (254.1 examples/sec; 0.504 sec/batch)
2018-03-22 16:10:00.337832: step 32880, loss = 0.85 (255.4 examples/sec; 0.501 sec/batch)
2018-03-22 16:10:05.585328: step 32890, loss = 0.84 (243.9 examples/sec; 0.525 sec/batch)
2018-03-22 16:10:10.614856: step 32900, loss = 0.72 (254.5 examples/sec; 0.503 sec/batch)
2018-03-22 16:10:15.076384: step 32910, loss = 0.72 (286.9 examples/sec; 0.446 sec/batch)
2018-03-22 16:10:19.670567: step 32920, loss = 0.55 (278.6 examples/sec; 0.459 sec/batch)
2018-03-22 16:10:24.837766: step 32930, loss = 0.76 (247.7 examples/sec; 0.517 sec/batch)
2018-03-22 16:10:29.911393: step 32940, loss = 0.68 (252.3 examples/sec; 0.507 sec/batch)
2018-03-22 16:10:34.662447: step 32950, loss = 0.72 (269.4 examples/sec; 0.475 sec/batch)
2018-03-22 16:10:39.967412: step 32960, loss = 0.75 (241.3 examples/sec; 0.530 sec/batch)
2018-03-22 16:10:45.183357: step 32970, loss = 0.69 (245.4 examples/sec; 0.522 sec/batch)
2018-03-22 16:10:50.354275: step 32980, loss = 1.06 (247.5 examples/sec; 0.517 sec/batch)
2018-03-22 16:10:55.012841: step 32990, loss = 0.67 (274.8 examples/sec; 0.466 sec/batch)
2018-03-22 16:11:00.637061: step 33000, loss = 0.67 (227.6 examples/sec; 0.562 sec/batch)
2018-03-22 16:11:05.780350: step 33010, loss = 0.64 (248.9 examples/sec; 0.514 sec/batch)
2018-03-22 16:11:11.166429: step 33020, loss = 0.64 (237.6 examples/sec; 0.539 sec/batch)
2018-03-22 16:11:16.123203: step 33030, loss = 0.85 (258.2 examples/sec; 0.496 sec/batch)
2018-03-22 16:11:21.365430: step 33040, loss = 0.54 (244.2 examples/sec; 0.524 sec/batch)
2018-03-22 16:11:26.219139: step 33050, loss = 0.80 (263.7 examples/sec; 0.485 sec/batch)
2018-03-22 16:11:31.345390: step 33060, loss = 0.85 (249.7 examples/sec; 0.513 sec/batch)
2018-03-22 16:11:36.026335: step 33070, loss = 0.88 (273.4 examples/sec; 0.468 sec/batch)
2018-03-22 16:11:41.087313: step 33080, loss = 0.80 (252.9 examples/sec; 0.506 sec/batch)
2018-03-22 16:11:46.365768: step 33090, loss = 0.79 (242.5 examples/sec; 0.528 sec/batch)
2018-03-22 16:11:51.655387: step 33100, loss = 0.81 (242.0 examples/sec; 0.529 sec/batch)
2018-03-22 16:11:56.332380: step 33110, loss = 0.73 (273.7 examples/sec; 0.468 sec/batch)
2018-03-22 16:12:01.468516: step 33120, loss = 0.88 (249.2 examples/sec; 0.514 sec/batch)
2018-03-22 16:12:06.595413: step 33130, loss = 0.65 (249.7 examples/sec; 0.513 sec/batch)
2018-03-22 16:12:11.938451: step 33140, loss = 0.82 (239.6 examples/sec; 0.534 sec/batch)
2018-03-22 16:12:16.591406: step 33150, loss = 0.66 (275.1 examples/sec; 0.465 sec/batch)
2018-03-22 16:12:21.522393: step 33160, loss = 0.88 (259.6 examples/sec; 0.493 sec/batch)
2018-03-22 16:12:26.772341: step 33170, loss = 0.70 (243.8 examples/sec; 0.525 sec/batch)
2018-03-22 16:12:32.262058: step 33180, loss = 0.74 (233.2 examples/sec; 0.549 sec/batch)
2018-03-22 16:12:37.235010: step 33190, loss = 0.84 (257.4 examples/sec; 0.497 sec/batch)
2018-03-22 16:12:42.427932: step 33200, loss = 0.81 (246.5 examples/sec; 0.519 sec/batch)
2018-03-22 16:12:47.337673: step 33210, loss = 0.70 (260.7 examples/sec; 0.491 sec/batch)
2018-03-22 16:12:52.072382: step 33220, loss = 0.80 (270.3 examples/sec; 0.473 sec/batch)
2018-03-22 16:12:56.919276: step 33230, loss = 0.70 (264.1 examples/sec; 0.485 sec/batch)
2018-03-22 16:13:01.538374: step 33240, loss = 0.69 (277.1 examples/sec; 0.462 sec/batch)
2018-03-22 16:13:05.597294: step 33250, loss = 0.81 (315.4 examples/sec; 0.406 sec/batch)
2018-03-22 16:13:11.002690: step 33260, loss = 0.72 (236.8 examples/sec; 0.541 sec/batch)
2018-03-22 16:13:15.793364: step 33270, loss = 0.79 (267.2 examples/sec; 0.479 sec/batch)
2018-03-22 16:13:20.691470: step 33280, loss = 0.79 (261.3 examples/sec; 0.490 sec/batch)
2018-03-22 16:13:25.711192: step 33290, loss = 0.82 (255.0 examples/sec; 0.502 sec/batch)
2018-03-22 16:13:30.571369: step 33300, loss = 0.97 (263.4 examples/sec; 0.486 sec/batch)
2018-03-22 16:13:35.181483: step 33310, loss = 0.76 (277.7 examples/sec; 0.461 sec/batch)
2018-03-22 16:13:41.193101: step 33320, loss = 0.84 (212.9 examples/sec; 0.601 sec/batch)
2018-03-22 16:13:46.474419: step 33330, loss = 0.67 (242.4 examples/sec; 0.528 sec/batch)
2018-03-22 16:13:51.621243: step 33340, loss = 0.86 (248.7 examples/sec; 0.515 sec/batch)
2018-03-22 16:13:56.379193: step 33350, loss = 0.79 (269.0 examples/sec; 0.476 sec/batch)
2018-03-22 16:14:01.304776: step 33360, loss = 0.81 (259.9 examples/sec; 0.493 sec/batch)
2018-03-22 16:14:06.148647: step 33370, loss = 0.70 (264.2 examples/sec; 0.484 sec/batch)
2018-03-22 16:14:10.751113: step 33380, loss = 0.87 (278.1 examples/sec; 0.460 sec/batch)
2018-03-22 16:14:15.176415: step 33390, loss = 0.94 (289.2 examples/sec; 0.443 sec/batch)
2018-03-22 16:14:21.130691: step 33400, loss = 0.63 (215.0 examples/sec; 0.595 sec/batch)
2018-03-22 16:14:26.088395: step 33410, loss = 0.81 (258.2 examples/sec; 0.496 sec/batch)
2018-03-22 16:14:31.298674: step 33420, loss = 0.86 (245.7 examples/sec; 0.521 sec/batch)
2018-03-22 16:14:35.631531: step 33430, loss = 0.81 (295.4 examples/sec; 0.433 sec/batch)
2018-03-22 16:14:41.097447: step 33440, loss = 0.71 (234.2 examples/sec; 0.547 sec/batch)
2018-03-22 16:14:46.205384: step 33450, loss = 0.87 (250.6 examples/sec; 0.511 sec/batch)
2018-03-22 16:14:51.303739: step 33460, loss = 0.63 (251.1 examples/sec; 0.510 sec/batch)
2018-03-22 16:14:56.584359: step 33470, loss = 0.69 (242.4 examples/sec; 0.528 sec/batch)
2018-03-22 16:15:01.633185: step 33480, loss = 0.80 (253.5 examples/sec; 0.505 sec/batch)
2018-03-22 16:15:06.806456: step 33490, loss = 0.88 (247.4 examples/sec; 0.517 sec/batch)
2018-03-22 16:15:12.313372: step 33500, loss = 0.79 (232.4 examples/sec; 0.551 sec/batch)
2018-03-22 16:15:17.043928: step 33510, loss = 0.62 (270.6 examples/sec; 0.473 sec/batch)
2018-03-22 16:15:22.299163: step 33520, loss = 0.71 (243.6 examples/sec; 0.526 sec/batch)
2018-03-22 16:15:27.610414: step 33530, loss = 0.75 (241.0 examples/sec; 0.531 sec/batch)
2018-03-22 16:15:32.629115: step 33540, loss = 0.81 (255.0 examples/sec; 0.502 sec/batch)
2018-03-22 16:15:37.571122: step 33550, loss = 0.78 (259.0 examples/sec; 0.494 sec/batch)
2018-03-22 16:15:42.402131: step 33560, loss = 0.67 (265.0 examples/sec; 0.483 sec/batch)
2018-03-22 16:15:47.453316: step 33570, loss = 0.62 (253.4 examples/sec; 0.505 sec/batch)
2018-03-22 16:15:52.846395: step 33580, loss = 0.82 (237.3 examples/sec; 0.539 sec/batch)
2018-03-22 16:15:57.845314: step 33590, loss = 0.81 (256.1 examples/sec; 0.500 sec/batch)
2018-03-22 16:16:03.293190: step 33600, loss = 0.75 (235.0 examples/sec; 0.545 sec/batch)
2018-03-22 16:16:08.423251: step 33610, loss = 0.81 (249.5 examples/sec; 0.513 sec/batch)
2018-03-22 16:16:14.388472: step 33620, loss = 0.82 (214.6 examples/sec; 0.597 sec/batch)
2018-03-22 16:16:18.865261: step 33630, loss = 0.71 (285.9 examples/sec; 0.448 sec/batch)
2018-03-22 16:16:23.672916: step 33640, loss = 0.67 (266.2 examples/sec; 0.481 sec/batch)
2018-03-22 16:16:28.783592: step 33650, loss = 0.90 (250.5 examples/sec; 0.511 sec/batch)
2018-03-22 16:16:34.017413: step 33660, loss = 0.85 (244.6 examples/sec; 0.523 sec/batch)
2018-03-22 16:16:39.140550: step 33670, loss = 0.86 (249.8 examples/sec; 0.512 sec/batch)
2018-03-22 16:16:44.021324: step 33680, loss = 0.79 (262.3 examples/sec; 0.488 sec/batch)
2018-03-22 16:16:48.361206: step 33690, loss = 0.91 (294.9 examples/sec; 0.434 sec/batch)
2018-03-22 16:16:52.730610: step 33700, loss = 0.69 (292.9 examples/sec; 0.437 sec/batch)
2018-03-22 16:16:58.187390: step 33710, loss = 0.87 (234.6 examples/sec; 0.546 sec/batch)
2018-03-22 16:17:03.394376: step 33720, loss = 0.80 (245.8 examples/sec; 0.521 sec/batch)
2018-03-22 16:17:08.096554: step 33730, loss = 0.76 (272.2 examples/sec; 0.470 sec/batch)
2018-03-22 16:17:13.380409: step 33740, loss = 0.78 (242.2 examples/sec; 0.528 sec/batch)
2018-03-22 16:17:18.416284: step 33750, loss = 0.84 (254.2 examples/sec; 0.504 sec/batch)
2018-03-22 16:17:23.471338: step 33760, loss = 0.71 (253.2 examples/sec; 0.506 sec/batch)
2018-03-22 16:17:29.147448: step 33770, loss = 0.79 (225.5 examples/sec; 0.568 sec/batch)
2018-03-22 16:17:33.936349: step 33780, loss = 0.74 (267.3 examples/sec; 0.479 sec/batch)
2018-03-22 16:17:39.136390: step 33790, loss = 0.84 (246.2 examples/sec; 0.520 sec/batch)
2018-03-22 16:17:44.822041: step 33800, loss = 0.74 (225.1 examples/sec; 0.569 sec/batch)
2018-03-22 16:17:49.941246: step 33810, loss = 0.68 (250.0 examples/sec; 0.512 sec/batch)
2018-03-22 16:17:54.750434: step 33820, loss = 0.81 (266.2 examples/sec; 0.481 sec/batch)
2018-03-22 16:17:59.087920: step 33830, loss = 0.71 (295.1 examples/sec; 0.434 sec/batch)
2018-03-22 16:18:03.789455: step 33840, loss = 0.93 (272.3 examples/sec; 0.470 sec/batch)
2018-03-22 16:18:08.788943: step 33850, loss = 0.83 (256.0 examples/sec; 0.500 sec/batch)
2018-03-22 16:18:14.081408: step 33860, loss = 0.70 (241.9 examples/sec; 0.529 sec/batch)
2018-03-22 16:18:18.307404: step 33870, loss = 0.75 (302.9 examples/sec; 0.423 sec/batch)
2018-03-22 16:18:23.665430: step 33880, loss = 0.87 (238.9 examples/sec; 0.536 sec/batch)
2018-03-22 16:18:28.638660: step 33890, loss = 0.76 (257.4 examples/sec; 0.497 sec/batch)
2018-03-22 16:18:34.095328: step 33900, loss = 0.72 (234.6 examples/sec; 0.546 sec/batch)
2018-03-22 16:18:39.153424: step 33910, loss = 0.75 (253.1 examples/sec; 0.506 sec/batch)
2018-03-22 16:18:44.581436: step 33920, loss = 0.88 (235.8 examples/sec; 0.543 sec/batch)
2018-03-22 16:18:49.239718: step 33930, loss = 0.74 (274.8 examples/sec; 0.466 sec/batch)
2018-03-22 16:18:54.493772: step 33940, loss = 0.69 (243.6 examples/sec; 0.525 sec/batch)
2018-03-22 16:18:59.270430: step 33950, loss = 0.74 (268.0 examples/sec; 0.478 sec/batch)
2018-03-22 16:19:04.217407: step 33960, loss = 0.83 (258.7 examples/sec; 0.495 sec/batch)
2018-03-22 16:19:08.873077: step 33970, loss = 0.90 (274.9 examples/sec; 0.466 sec/batch)
2018-03-22 16:19:14.069075: step 33980, loss = 0.65 (246.3 examples/sec; 0.520 sec/batch)
2018-03-22 16:19:19.884005: step 33990, loss = 0.90 (220.1 examples/sec; 0.581 sec/batch)
2018-03-22 16:19:24.686222: step 34000, loss = 0.79 (266.5 examples/sec; 0.480 sec/batch)
2018-03-22 16:19:30.096391: step 34010, loss = 0.68 (236.6 examples/sec; 0.541 sec/batch)
2018-03-22 16:19:35.171432: step 34020, loss = 0.80 (252.2 examples/sec; 0.508 sec/batch)
2018-03-22 16:19:40.388427: step 34030, loss = 0.75 (245.4 examples/sec; 0.522 sec/batch)
2018-03-22 16:19:45.761538: step 34040, loss = 1.09 (238.2 examples/sec; 0.537 sec/batch)
2018-03-22 16:19:51.390406: step 34050, loss = 0.70 (227.4 examples/sec; 0.563 sec/batch)
2018-03-22 16:19:56.393645: step 34060, loss = 0.75 (255.8 examples/sec; 0.500 sec/batch)
2018-03-22 16:20:01.456432: step 34070, loss = 0.80 (252.8 examples/sec; 0.506 sec/batch)
2018-03-22 16:20:06.452107: step 34080, loss = 0.86 (256.2 examples/sec; 0.500 sec/batch)
2018-03-22 16:20:11.405371: step 34090, loss = 0.81 (258.4 examples/sec; 0.495 sec/batch)
2018-03-22 16:20:16.166334: step 34100, loss = 0.86 (268.9 examples/sec; 0.476 sec/batch)
2018-03-22 16:20:21.755447: step 34110, loss = 0.70 (229.0 examples/sec; 0.559 sec/batch)
2018-03-22 16:20:26.746375: step 34120, loss = 0.84 (256.5 examples/sec; 0.499 sec/batch)
2018-03-22 16:20:31.848933: step 34130, loss = 0.68 (250.9 examples/sec; 0.510 sec/batch)
2018-03-22 16:20:36.448405: step 34140, loss = 0.80 (278.3 examples/sec; 0.460 sec/batch)
2018-03-22 16:20:41.331403: step 34150, loss = 0.85 (262.1 examples/sec; 0.488 sec/batch)
2018-03-22 16:20:45.919832: step 34160, loss = 0.68 (279.0 examples/sec; 0.459 sec/batch)
2018-03-22 16:20:50.709419: step 34170, loss = 0.72 (267.2 examples/sec; 0.479 sec/batch)
2018-03-22 16:20:55.624076: step 34180, loss = 0.83 (260.4 examples/sec; 0.491 sec/batch)
2018-03-22 16:21:01.310896: step 34190, loss = 0.79 (225.1 examples/sec; 0.569 sec/batch)
2018-03-22 16:21:06.374640: step 34200, loss = 0.72 (252.8 examples/sec; 0.506 sec/batch)
2018-03-22 16:21:11.317307: step 34210, loss = 0.78 (259.0 examples/sec; 0.494 sec/batch)
2018-03-22 16:21:16.029580: step 34220, loss = 0.82 (271.6 examples/sec; 0.471 sec/batch)
2018-03-22 16:21:21.333418: step 34230, loss = 0.64 (241.3 examples/sec; 0.530 sec/batch)
2018-03-22 16:21:26.622470: step 34240, loss = 0.84 (242.0 examples/sec; 0.529 sec/batch)
2018-03-22 16:21:31.746399: step 34250, loss = 0.71 (249.8 examples/sec; 0.512 sec/batch)
2018-03-22 16:21:35.933828: step 34260, loss = 0.79 (305.7 examples/sec; 0.419 sec/batch)
2018-03-22 16:21:41.365705: step 34270, loss = 0.64 (235.6 examples/sec; 0.543 sec/batch)
2018-03-22 16:21:46.156334: step 34280, loss = 0.88 (267.2 examples/sec; 0.479 sec/batch)
2018-03-22 16:21:50.730383: step 34290, loss = 0.90 (279.8 examples/sec; 0.457 sec/batch)
2018-03-22 16:21:55.171381: step 34300, loss = 0.96 (288.2 examples/sec; 0.444 sec/batch)
2018-03-22 16:22:00.365446: step 34310, loss = 0.82 (246.4 examples/sec; 0.519 sec/batch)
2018-03-22 16:22:05.535507: step 34320, loss = 0.92 (247.6 examples/sec; 0.517 sec/batch)
2018-03-22 16:22:11.276408: step 34330, loss = 0.79 (223.0 examples/sec; 0.574 sec/batch)
2018-03-22 16:22:16.337383: step 34340, loss = 0.77 (252.9 examples/sec; 0.506 sec/batch)
2018-03-22 16:22:21.088318: step 34350, loss = 0.62 (269.4 examples/sec; 0.475 sec/batch)
2018-03-22 16:22:25.654411: step 34360, loss = 0.83 (280.3 examples/sec; 0.457 sec/batch)
2018-03-22 16:22:31.165421: step 34370, loss = 0.77 (232.3 examples/sec; 0.551 sec/batch)
2018-03-22 16:22:36.704207: step 34380, loss = 0.72 (231.1 examples/sec; 0.554 sec/batch)
2018-03-22 16:22:41.819279: step 34390, loss = 0.82 (250.2 examples/sec; 0.511 sec/batch)
2018-03-22 16:22:47.293395: step 34400, loss = 0.67 (233.8 examples/sec; 0.547 sec/batch)
2018-03-22 16:22:52.168409: step 34410, loss = 0.78 (262.6 examples/sec; 0.488 sec/batch)
2018-03-22 16:22:57.152366: step 34420, loss = 0.69 (256.8 examples/sec; 0.498 sec/batch)
2018-03-22 16:23:02.479442: step 34430, loss = 0.67 (240.3 examples/sec; 0.533 sec/batch)
2018-03-22 16:23:07.497423: step 34440, loss = 0.74 (255.1 examples/sec; 0.502 sec/batch)
2018-03-22 16:23:12.525381: step 34450, loss = 0.77 (254.6 examples/sec; 0.503 sec/batch)
2018-03-22 16:23:17.800639: step 34460, loss = 0.81 (242.6 examples/sec; 0.528 sec/batch)
2018-03-22 16:23:22.905213: step 34470, loss = 0.92 (250.8 examples/sec; 0.510 sec/batch)
2018-03-22 16:23:28.205390: step 34480, loss = 0.70 (241.5 examples/sec; 0.530 sec/batch)
2018-03-22 16:23:33.336381: step 34490, loss = 0.67 (249.5 examples/sec; 0.513 sec/batch)
2018-03-22 16:23:38.335157: step 34500, loss = 0.84 (256.1 examples/sec; 0.500 sec/batch)
2018-03-22 16:23:43.325418: step 34510, loss = 0.75 (256.5 examples/sec; 0.499 sec/batch)
2018-03-22 16:23:48.401065: step 34520, loss = 0.84 (252.2 examples/sec; 0.508 sec/batch)
2018-03-22 16:23:54.044881: step 34530, loss = 0.80 (226.8 examples/sec; 0.564 sec/batch)
2018-03-22 16:23:58.635954: step 34540, loss = 0.89 (278.8 examples/sec; 0.459 sec/batch)
2018-03-22 16:24:03.650213: step 34550, loss = 0.77 (255.3 examples/sec; 0.501 sec/batch)
2018-03-22 16:24:08.371676: step 34560, loss = 0.67 (271.1 examples/sec; 0.472 sec/batch)
2018-03-22 16:24:13.774501: step 34570, loss = 0.81 (236.9 examples/sec; 0.540 sec/batch)
2018-03-22 16:24:18.585186: step 34580, loss = 0.71 (266.1 examples/sec; 0.481 sec/batch)
2018-03-22 16:24:23.285464: step 34590, loss = 0.74 (272.3 examples/sec; 0.470 sec/batch)
2018-03-22 16:24:28.397541: step 34600, loss = 0.83 (250.4 examples/sec; 0.511 sec/batch)
2018-03-22 16:24:33.432401: step 34610, loss = 0.83 (254.2 examples/sec; 0.503 sec/batch)
2018-03-22 16:24:38.153351: step 34620, loss = 0.84 (271.1 examples/sec; 0.472 sec/batch)
2018-03-22 16:24:43.387343: step 34630, loss = 0.58 (244.6 examples/sec; 0.523 sec/batch)
2018-03-22 16:24:48.423273: step 34640, loss = 0.64 (254.2 examples/sec; 0.504 sec/batch)
2018-03-22 16:24:53.721808: step 34650, loss = 0.86 (241.6 examples/sec; 0.530 sec/batch)
2018-03-22 16:24:58.650194: step 34660, loss = 0.74 (259.7 examples/sec; 0.493 sec/batch)
2018-03-22 16:25:03.516793: step 34670, loss = 0.79 (263.0 examples/sec; 0.487 sec/batch)
2018-03-22 16:25:08.501380: step 34680, loss = 0.69 (256.8 examples/sec; 0.498 sec/batch)
2018-03-22 16:25:13.568555: step 34690, loss = 0.70 (252.6 examples/sec; 0.507 sec/batch)
2018-03-22 16:25:18.888921: step 34700, loss = 0.70 (240.6 examples/sec; 0.532 sec/batch)
2018-03-22 16:25:24.347379: step 34710, loss = 0.75 (234.5 examples/sec; 0.546 sec/batch)
2018-03-22 16:25:28.928942: step 34720, loss = 0.90 (279.4 examples/sec; 0.458 sec/batch)
2018-03-22 16:25:33.291321: step 34730, loss = 0.85 (293.4 examples/sec; 0.436 sec/batch)
2018-03-22 16:25:37.745054: step 34740, loss = 0.89 (287.4 examples/sec; 0.445 sec/batch)
2018-03-22 16:25:42.587785: step 34750, loss = 0.85 (264.3 examples/sec; 0.484 sec/batch)
2018-03-22 16:25:47.616418: step 34760, loss = 0.89 (254.5 examples/sec; 0.503 sec/batch)
2018-03-22 16:25:52.777387: step 34770, loss = 0.77 (248.0 examples/sec; 0.516 sec/batch)
2018-03-22 16:25:57.324448: step 34780, loss = 0.81 (281.5 examples/sec; 0.455 sec/batch)
2018-03-22 16:26:02.105513: step 34790, loss = 0.73 (267.7 examples/sec; 0.478 sec/batch)
2018-03-22 16:26:07.246132: step 34800, loss = 0.66 (249.0 examples/sec; 0.514 sec/batch)
2018-03-22 16:26:12.205810: step 34810, loss = 0.85 (258.1 examples/sec; 0.496 sec/batch)
2018-03-22 16:26:16.676439: step 34820, loss = 0.80 (286.3 examples/sec; 0.447 sec/batch)
2018-03-22 16:26:21.843435: step 34830, loss = 0.70 (247.7 examples/sec; 0.517 sec/batch)
2018-03-22 16:26:26.909355: step 34840, loss = 0.81 (252.7 examples/sec; 0.507 sec/batch)
2018-03-22 16:26:31.557004: step 34850, loss = 0.66 (275.4 examples/sec; 0.465 sec/batch)
2018-03-22 16:26:36.151099: step 34860, loss = 0.71 (278.6 examples/sec; 0.459 sec/batch)
2018-03-22 16:26:40.654701: step 34870, loss = 0.67 (284.2 examples/sec; 0.450 sec/batch)
2018-03-22 16:26:45.639495: step 34880, loss = 0.81 (256.8 examples/sec; 0.498 sec/batch)
2018-03-22 16:26:50.622750: step 34890, loss = 0.87 (256.9 examples/sec; 0.498 sec/batch)
2018-03-22 16:26:55.683678: step 34900, loss = 0.95 (252.9 examples/sec; 0.506 sec/batch)
2018-03-22 16:27:01.123337: step 34910, loss = 0.71 (235.3 examples/sec; 0.544 sec/batch)
2018-03-22 16:27:05.958640: step 34920, loss = 1.00 (264.7 examples/sec; 0.484 sec/batch)
2018-03-22 16:27:11.009481: step 34930, loss = 0.73 (253.4 examples/sec; 0.505 sec/batch)
2018-03-22 16:27:15.633412: step 34940, loss = 0.77 (276.8 examples/sec; 0.462 sec/batch)
2018-03-22 16:27:21.096384: step 34950, loss = 0.69 (234.3 examples/sec; 0.546 sec/batch)
2018-03-22 16:27:26.314896: step 34960, loss = 0.82 (245.3 examples/sec; 0.522 sec/batch)
2018-03-22 16:27:31.551548: step 34970, loss = 0.75 (244.4 examples/sec; 0.524 sec/batch)
2018-03-22 16:27:36.286098: step 34980, loss = 0.69 (270.4 examples/sec; 0.473 sec/batch)
2018-03-22 16:27:41.322356: step 34990, loss = 0.70 (254.2 examples/sec; 0.504 sec/batch)
2018-03-22 16:27:46.733886: step 35000, loss = 0.68 (236.5 examples/sec; 0.541 sec/batch)
2018-03-22 16:27:51.493341: step 35010, loss = 0.84 (268.9 examples/sec; 0.476 sec/batch)
2018-03-22 16:27:56.079335: step 35020, loss = 0.71 (279.1 examples/sec; 0.459 sec/batch)
2018-03-22 16:28:01.273387: step 35030, loss = 0.71 (246.4 examples/sec; 0.519 sec/batch)
2018-03-22 16:28:06.373527: step 35040, loss = 0.66 (251.0 examples/sec; 0.510 sec/batch)
2018-03-22 16:28:11.614431: step 35050, loss = 0.71 (244.2 examples/sec; 0.524 sec/batch)
2018-03-22 16:28:15.966194: step 35060, loss = 0.89 (294.1 examples/sec; 0.435 sec/batch)
2018-03-22 16:28:20.898304: step 35070, loss = 0.85 (259.5 examples/sec; 0.493 sec/batch)
2018-03-22 16:28:25.375122: step 35080, loss = 0.77 (285.9 examples/sec; 0.448 sec/batch)
2018-03-22 16:28:29.902432: step 35090, loss = 0.80 (282.7 examples/sec; 0.453 sec/batch)
2018-03-22 16:28:35.579309: step 35100, loss = 0.72 (225.5 examples/sec; 0.568 sec/batch)
2018-03-22 16:28:40.774469: step 35110, loss = 0.67 (246.4 examples/sec; 0.520 sec/batch)
2018-03-22 16:28:45.961101: step 35120, loss = 0.72 (246.8 examples/sec; 0.519 sec/batch)
2018-03-22 16:28:51.083344: step 35130, loss = 0.65 (249.9 examples/sec; 0.512 sec/batch)
2018-03-22 16:28:55.191328: step 35140, loss = 0.80 (311.6 examples/sec; 0.411 sec/batch)
2018-03-22 16:29:00.361810: step 35150, loss = 0.90 (247.6 examples/sec; 0.517 sec/batch)
2018-03-22 16:29:05.286382: step 35160, loss = 0.73 (259.9 examples/sec; 0.492 sec/batch)
2018-03-22 16:29:10.414416: step 35170, loss = 0.85 (249.6 examples/sec; 0.513 sec/batch)
2018-03-22 16:29:14.849354: step 35180, loss = 0.84 (288.6 examples/sec; 0.443 sec/batch)
2018-03-22 16:29:19.926984: step 35190, loss = 0.76 (252.1 examples/sec; 0.508 sec/batch)
2018-03-22 16:29:25.430080: step 35200, loss = 0.91 (232.6 examples/sec; 0.550 sec/batch)
2018-03-22 16:29:30.206357: step 35210, loss = 0.74 (268.0 examples/sec; 0.478 sec/batch)
2018-03-22 16:29:35.012323: step 35220, loss = 0.87 (266.3 examples/sec; 0.481 sec/batch)
2018-03-22 16:29:39.999175: step 35230, loss = 0.71 (256.7 examples/sec; 0.499 sec/batch)
2018-03-22 16:29:44.808569: step 35240, loss = 0.71 (266.1 examples/sec; 0.481 sec/batch)
2018-03-22 16:29:49.511802: step 35250, loss = 0.79 (272.2 examples/sec; 0.470 sec/batch)
2018-03-22 16:29:54.241371: step 35260, loss = 0.86 (270.6 examples/sec; 0.473 sec/batch)
2018-03-22 16:29:59.758133: step 35270, loss = 0.71 (232.0 examples/sec; 0.552 sec/batch)
2018-03-22 16:30:05.109410: step 35280, loss = 0.88 (239.2 examples/sec; 0.535 sec/batch)
2018-03-22 16:30:10.657390: step 35290, loss = 0.94 (230.7 examples/sec; 0.555 sec/batch)
2018-03-22 16:30:15.786466: step 35300, loss = 0.79 (249.6 examples/sec; 0.513 sec/batch)
2018-03-22 16:30:20.990351: step 35310, loss = 0.63 (246.0 examples/sec; 0.520 sec/batch)
2018-03-22 16:30:25.957383: step 35320, loss = 0.69 (257.7 examples/sec; 0.497 sec/batch)
2018-03-22 16:30:31.420378: step 35330, loss = 0.78 (234.3 examples/sec; 0.546 sec/batch)
2018-03-22 16:30:36.416465: step 35340, loss = 0.83 (256.2 examples/sec; 0.500 sec/batch)
2018-03-22 16:30:41.364445: step 35350, loss = 0.84 (258.7 examples/sec; 0.495 sec/batch)
2018-03-22 16:30:46.385885: step 35360, loss = 0.84 (254.9 examples/sec; 0.502 sec/batch)
2018-03-22 16:30:51.723353: step 35370, loss = 0.82 (239.8 examples/sec; 0.534 sec/batch)
2018-03-22 16:30:57.097019: step 35380, loss = 0.74 (238.2 examples/sec; 0.537 sec/batch)
2018-03-22 16:31:02.244320: step 35390, loss = 0.80 (248.7 examples/sec; 0.515 sec/batch)
2018-03-22 16:31:07.661656: step 35400, loss = 0.71 (236.3 examples/sec; 0.542 sec/batch)
2018-03-22 16:31:12.245862: step 35410, loss = 0.75 (279.2 examples/sec; 0.458 sec/batch)
2018-03-22 16:31:17.212099: step 35420, loss = 0.72 (257.7 examples/sec; 0.497 sec/batch)
2018-03-22 16:31:23.158220: step 35430, loss = 0.81 (215.3 examples/sec; 0.595 sec/batch)
2018-03-22 16:31:28.017567: step 35440, loss = 0.74 (263.4 examples/sec; 0.486 sec/batch)
2018-03-22 16:31:33.274742: step 35450, loss = 0.75 (243.5 examples/sec; 0.526 sec/batch)
2018-03-22 16:31:38.253290: step 35460, loss = 0.95 (257.1 examples/sec; 0.498 sec/batch)
2018-03-22 16:31:43.159410: step 35470, loss = 0.68 (260.9 examples/sec; 0.491 sec/batch)
2018-03-22 16:31:48.114316: step 35480, loss = 0.74 (258.3 examples/sec; 0.495 sec/batch)
2018-03-22 16:31:53.155424: step 35490, loss = 0.94 (253.9 examples/sec; 0.504 sec/batch)
2018-03-22 16:31:58.550311: step 35500, loss = 0.80 (237.3 examples/sec; 0.539 sec/batch)
2018-03-22 16:32:03.283422: step 35510, loss = 0.67 (270.4 examples/sec; 0.473 sec/batch)
2018-03-22 16:32:07.923893: step 35520, loss = 0.79 (275.8 examples/sec; 0.464 sec/batch)
2018-03-22 16:32:12.590432: step 35530, loss = 0.67 (274.3 examples/sec; 0.467 sec/batch)
2018-03-22 16:32:17.619472: step 35540, loss = 0.89 (254.5 examples/sec; 0.503 sec/batch)
2018-03-22 16:32:23.313427: step 35550, loss = 0.83 (224.8 examples/sec; 0.569 sec/batch)
2018-03-22 16:32:28.220008: step 35560, loss = 0.98 (260.9 examples/sec; 0.491 sec/batch)
2018-03-22 16:32:33.557477: step 35570, loss = 0.63 (239.8 examples/sec; 0.534 sec/batch)
2018-03-22 16:32:38.320283: step 35580, loss = 0.67 (268.7 examples/sec; 0.476 sec/batch)
2018-03-22 16:32:43.485467: step 35590, loss = 0.58 (247.8 examples/sec; 0.517 sec/batch)
2018-03-22 16:32:49.284051: step 35600, loss = 0.77 (220.7 examples/sec; 0.580 sec/batch)
2018-03-22 16:32:54.389651: step 35610, loss = 0.87 (250.7 examples/sec; 0.511 sec/batch)
2018-03-22 16:33:00.043984: step 35620, loss = 0.65 (226.4 examples/sec; 0.565 sec/batch)
2018-03-22 16:33:04.852364: step 35630, loss = 0.74 (266.2 examples/sec; 0.481 sec/batch)
2018-03-22 16:33:09.726923: step 35640, loss = 0.73 (262.6 examples/sec; 0.487 sec/batch)
2018-03-22 16:33:14.541451: step 35650, loss = 0.93 (265.9 examples/sec; 0.481 sec/batch)
2018-03-22 16:33:19.515668: step 35660, loss = 0.66 (257.3 examples/sec; 0.497 sec/batch)
2018-03-22 16:33:24.890427: step 35670, loss = 0.72 (238.2 examples/sec; 0.537 sec/batch)
2018-03-22 16:33:29.684195: step 35680, loss = 0.75 (267.0 examples/sec; 0.479 sec/batch)
2018-03-22 16:33:34.648007: step 35690, loss = 0.69 (257.9 examples/sec; 0.496 sec/batch)
2018-03-22 16:33:40.167769: step 35700, loss = 0.71 (231.9 examples/sec; 0.552 sec/batch)
2018-03-22 16:33:45.351650: step 35710, loss = 0.82 (246.9 examples/sec; 0.518 sec/batch)
2018-03-22 16:33:50.490678: step 35720, loss = 0.72 (249.1 examples/sec; 0.514 sec/batch)
2018-03-22 16:33:54.861370: step 35730, loss = 0.67 (292.9 examples/sec; 0.437 sec/batch)
2018-03-22 16:34:00.423961: step 35740, loss = 0.86 (230.1 examples/sec; 0.556 sec/batch)
2018-03-22 16:34:05.583444: step 35750, loss = 0.91 (248.1 examples/sec; 0.516 sec/batch)
2018-03-22 16:34:10.347467: step 35760, loss = 0.80 (268.7 examples/sec; 0.476 sec/batch)
2018-03-22 16:34:15.363521: step 35770, loss = 0.92 (255.2 examples/sec; 0.502 sec/batch)
2018-03-22 16:34:20.377222: step 35780, loss = 0.71 (255.3 examples/sec; 0.501 sec/batch)
2018-03-22 16:34:25.058994: step 35790, loss = 0.65 (273.4 examples/sec; 0.468 sec/batch)
2018-03-22 16:34:30.010960: step 35800, loss = 0.79 (258.5 examples/sec; 0.495 sec/batch)
2018-03-22 16:34:34.694345: step 35810, loss = 0.79 (273.3 examples/sec; 0.468 sec/batch)
2018-03-22 16:34:40.052448: step 35820, loss = 0.84 (238.9 examples/sec; 0.536 sec/batch)
2018-03-22 16:34:44.509338: step 35830, loss = 0.78 (287.2 examples/sec; 0.446 sec/batch)
2018-03-22 16:34:49.538412: step 35840, loss = 0.75 (254.5 examples/sec; 0.503 sec/batch)
2018-03-22 16:34:54.187498: step 35850, loss = 0.73 (275.3 examples/sec; 0.465 sec/batch)
2018-03-22 16:34:59.296954: step 35860, loss = 0.68 (250.5 examples/sec; 0.511 sec/batch)
2018-03-22 16:35:04.123398: step 35870, loss = 0.71 (265.2 examples/sec; 0.483 sec/batch)
2018-03-22 16:35:09.764843: step 35880, loss = 0.78 (226.9 examples/sec; 0.564 sec/batch)
2018-03-22 16:35:15.239758: step 35890, loss = 0.69 (233.8 examples/sec; 0.547 sec/batch)
2018-03-22 16:35:20.685518: step 35900, loss = 0.61 (235.0 examples/sec; 0.545 sec/batch)
2018-03-22 16:35:25.389905: step 35910, loss = 0.82 (272.1 examples/sec; 0.470 sec/batch)
2018-03-22 16:35:30.905954: step 35920, loss = 0.86 (232.1 examples/sec; 0.552 sec/batch)
2018-03-22 16:35:35.766241: step 35930, loss = 0.72 (263.4 examples/sec; 0.486 sec/batch)
2018-03-22 16:35:41.109438: step 35940, loss = 0.67 (239.6 examples/sec; 0.534 sec/batch)
2018-03-22 16:35:45.810536: step 35950, loss = 0.86 (272.3 examples/sec; 0.470 sec/batch)
2018-03-22 16:35:50.795859: step 35960, loss = 0.87 (256.8 examples/sec; 0.499 sec/batch)
2018-03-22 16:35:55.158911: step 35970, loss = 0.83 (293.4 examples/sec; 0.436 sec/batch)
2018-03-22 16:35:59.921008: step 35980, loss = 0.87 (268.8 examples/sec; 0.476 sec/batch)
2018-03-22 16:36:05.297081: step 35990, loss = 0.76 (238.1 examples/sec; 0.538 sec/batch)
2018-03-22 16:36:10.860893: step 36000, loss = 0.71 (230.1 examples/sec; 0.556 sec/batch)
2018-03-22 16:36:15.805382: step 36010, loss = 1.01 (258.9 examples/sec; 0.494 sec/batch)
2018-03-22 16:36:20.872393: step 36020, loss = 0.87 (252.6 examples/sec; 0.507 sec/batch)
2018-03-22 16:36:25.299362: step 36030, loss = 0.81 (289.1 examples/sec; 0.443 sec/batch)
2018-03-22 16:36:30.647654: step 36040, loss = 0.80 (239.3 examples/sec; 0.535 sec/batch)
2018-03-22 16:36:35.582492: step 36050, loss = 0.71 (259.4 examples/sec; 0.493 sec/batch)
2018-03-22 16:36:40.750453: step 36060, loss = 0.79 (247.7 examples/sec; 0.517 sec/batch)
2018-03-22 16:36:46.100960: step 36070, loss = 0.71 (239.2 examples/sec; 0.535 sec/batch)
2018-03-22 16:36:51.559363: step 36080, loss = 0.75 (234.5 examples/sec; 0.546 sec/batch)
2018-03-22 16:36:56.251433: step 36090, loss = 0.95 (272.8 examples/sec; 0.469 sec/batch)
2018-03-22 16:37:01.166063: step 36100, loss = 0.69 (260.4 examples/sec; 0.491 sec/batch)
2018-03-22 16:37:05.779867: step 36110, loss = 0.73 (277.4 examples/sec; 0.461 sec/batch)
2018-03-22 16:37:11.215390: step 36120, loss = 0.85 (235.5 examples/sec; 0.544 sec/batch)
2018-03-22 16:37:16.653384: step 36130, loss = 0.74 (235.4 examples/sec; 0.544 sec/batch)
2018-03-22 16:37:22.122749: step 36140, loss = 0.77 (234.0 examples/sec; 0.547 sec/batch)
2018-03-22 16:37:26.600423: step 36150, loss = 0.95 (285.9 examples/sec; 0.448 sec/batch)
2018-03-22 16:37:32.523898: step 36160, loss = 0.69 (216.1 examples/sec; 0.592 sec/batch)
2018-03-22 16:37:37.291286: step 36170, loss = 0.80 (268.5 examples/sec; 0.477 sec/batch)
2018-03-22 16:37:42.004381: step 36180, loss = 0.73 (271.6 examples/sec; 0.471 sec/batch)
2018-03-22 16:37:46.791213: step 36190, loss = 0.74 (267.4 examples/sec; 0.479 sec/batch)
2018-03-22 16:37:51.900004: step 36200, loss = 0.78 (250.5 examples/sec; 0.511 sec/batch)
2018-03-22 16:37:56.311516: step 36210, loss = 0.57 (290.2 examples/sec; 0.441 sec/batch)
2018-03-22 16:38:01.942381: step 36220, loss = 0.71 (227.3 examples/sec; 0.563 sec/batch)
2018-03-22 16:38:06.886447: step 36230, loss = 0.73 (258.9 examples/sec; 0.494 sec/batch)
2018-03-22 16:38:12.157456: step 36240, loss = 0.83 (242.8 examples/sec; 0.527 sec/batch)
2018-03-22 16:38:17.388449: step 36250, loss = 0.77 (244.7 examples/sec; 0.523 sec/batch)
2018-03-22 16:38:22.269371: step 36260, loss = 0.82 (262.2 examples/sec; 0.488 sec/batch)
2018-03-22 16:38:27.406221: step 36270, loss = 0.78 (249.2 examples/sec; 0.514 sec/batch)
2018-03-22 16:38:32.746441: step 36280, loss = 0.75 (239.7 examples/sec; 0.534 sec/batch)
2018-03-22 16:38:37.195428: step 36290, loss = 0.71 (287.7 examples/sec; 0.445 sec/batch)
2018-03-22 16:38:42.566767: step 36300, loss = 0.82 (238.3 examples/sec; 0.537 sec/batch)
2018-03-22 16:38:47.727443: step 36310, loss = 0.83 (248.0 examples/sec; 0.516 sec/batch)
2018-03-22 16:38:52.355837: step 36320, loss = 0.70 (276.6 examples/sec; 0.463 sec/batch)
2018-03-22 16:38:57.265380: step 36330, loss = 0.72 (260.7 examples/sec; 0.491 sec/batch)
2018-03-22 16:39:02.029551: step 36340, loss = 0.69 (268.7 examples/sec; 0.476 sec/batch)
2018-03-22 16:39:07.154335: step 36350, loss = 0.71 (249.8 examples/sec; 0.512 sec/batch)
2018-03-22 16:39:12.478398: step 36360, loss = 0.72 (240.4 examples/sec; 0.532 sec/batch)
2018-03-22 16:39:17.513429: step 36370, loss = 0.78 (254.2 examples/sec; 0.504 sec/batch)
2018-03-22 16:39:23.177809: step 36380, loss = 1.04 (226.0 examples/sec; 0.566 sec/batch)
2018-03-22 16:39:28.031829: step 36390, loss = 0.81 (263.7 examples/sec; 0.485 sec/batch)
2018-03-22 16:39:33.204066: step 36400, loss = 0.78 (247.5 examples/sec; 0.517 sec/batch)
2018-03-22 16:39:38.102149: step 36410, loss = 0.74 (261.3 examples/sec; 0.490 sec/batch)
2018-03-22 16:39:42.483522: step 36420, loss = 0.85 (292.1 examples/sec; 0.438 sec/batch)
2018-03-22 16:39:47.037413: step 36430, loss = 0.88 (281.1 examples/sec; 0.455 sec/batch)
2018-03-22 16:39:52.151891: step 36440, loss = 0.75 (250.3 examples/sec; 0.511 sec/batch)
2018-03-22 16:39:56.899425: step 36450, loss = 0.71 (269.6 examples/sec; 0.475 sec/batch)
2018-03-22 16:40:01.852382: step 36460, loss = 0.61 (258.4 examples/sec; 0.495 sec/batch)
2018-03-22 16:40:07.136420: step 36470, loss = 0.75 (242.2 examples/sec; 0.528 sec/batch)
2018-03-22 16:40:11.862789: step 36480, loss = 0.83 (270.8 examples/sec; 0.473 sec/batch)
2018-03-22 16:40:16.502016: step 36490, loss = 0.79 (275.9 examples/sec; 0.464 sec/batch)
2018-03-22 16:40:22.135401: step 36500, loss = 0.78 (227.2 examples/sec; 0.563 sec/batch)
2018-03-22 16:40:27.107424: step 36510, loss = 0.81 (257.4 examples/sec; 0.497 sec/batch)
2018-03-22 16:40:32.190847: step 36520, loss = 0.74 (251.8 examples/sec; 0.508 sec/batch)
2018-03-22 16:40:37.402462: step 36530, loss = 0.58 (245.6 examples/sec; 0.521 sec/batch)
2018-03-22 16:40:42.341272: step 36540, loss = 0.78 (259.2 examples/sec; 0.494 sec/batch)
2018-03-22 16:40:47.509425: step 36550, loss = 0.95 (247.7 examples/sec; 0.517 sec/batch)
2018-03-22 16:40:52.329384: step 36560, loss = 0.89 (265.6 examples/sec; 0.482 sec/batch)
2018-03-22 16:40:57.051751: step 36570, loss = 0.80 (271.1 examples/sec; 0.472 sec/batch)
2018-03-22 16:41:02.166370: step 36580, loss = 0.78 (250.3 examples/sec; 0.511 sec/batch)
2018-03-22 16:41:07.765428: step 36590, loss = 0.77 (228.6 examples/sec; 0.560 sec/batch)
2018-03-22 16:41:13.023889: step 36600, loss = 0.86 (243.4 examples/sec; 0.526 sec/batch)
2018-03-22 16:41:18.299408: step 36610, loss = 0.83 (242.6 examples/sec; 0.528 sec/batch)
2018-03-22 16:41:23.256822: step 36620, loss = 0.79 (258.2 examples/sec; 0.496 sec/batch)
2018-03-22 16:41:28.689611: step 36630, loss = 0.71 (235.6 examples/sec; 0.543 sec/batch)
2018-03-22 16:41:33.785965: step 36640, loss = 0.71 (251.2 examples/sec; 0.510 sec/batch)
2018-03-22 16:41:39.041402: step 36650, loss = 0.76 (243.6 examples/sec; 0.526 sec/batch)
2018-03-22 16:41:44.625416: step 36660, loss = 0.82 (229.2 examples/sec; 0.558 sec/batch)
2018-03-22 16:41:49.180422: step 36670, loss = 0.81 (281.0 examples/sec; 0.456 sec/batch)
2018-03-22 16:41:54.167760: step 36680, loss = 0.78 (256.7 examples/sec; 0.499 sec/batch)
2018-03-22 16:42:00.147329: step 36690, loss = 0.67 (214.1 examples/sec; 0.598 sec/batch)
2018-03-22 16:42:05.529009: step 36700, loss = 0.89 (237.8 examples/sec; 0.538 sec/batch)
2018-03-22 16:42:11.081430: step 36710, loss = 0.92 (230.5 examples/sec; 0.555 sec/batch)
2018-03-22 16:42:15.808637: step 36720, loss = 0.66 (270.8 examples/sec; 0.473 sec/batch)
2018-03-22 16:42:20.785412: step 36730, loss = 0.75 (257.2 examples/sec; 0.498 sec/batch)
2018-03-22 16:42:26.109017: step 36740, loss = 0.73 (240.4 examples/sec; 0.532 sec/batch)
2018-03-22 16:42:30.887720: step 36750, loss = 0.71 (267.9 examples/sec; 0.478 sec/batch)
2018-03-22 16:42:35.696518: step 36760, loss = 0.72 (266.2 examples/sec; 0.481 sec/batch)
2018-03-22 16:42:41.315447: step 36770, loss = 0.82 (227.8 examples/sec; 0.562 sec/batch)
2018-03-22 16:42:46.512476: step 36780, loss = 0.68 (246.3 examples/sec; 0.520 sec/batch)
2018-03-22 16:42:51.481465: step 36790, loss = 0.90 (257.6 examples/sec; 0.497 sec/batch)
2018-03-22 16:42:56.392260: step 36800, loss = 0.73 (260.6 examples/sec; 0.491 sec/batch)
2018-03-22 16:43:01.423382: step 36810, loss = 0.86 (254.4 examples/sec; 0.503 sec/batch)
2018-03-22 16:43:06.585412: step 36820, loss = 0.93 (248.0 examples/sec; 0.516 sec/batch)
2018-03-22 16:43:11.615426: step 36830, loss = 0.96 (254.5 examples/sec; 0.503 sec/batch)
2018-03-22 16:43:16.353291: step 36840, loss = 0.83 (270.2 examples/sec; 0.474 sec/batch)
2018-03-22 16:43:21.318069: step 36850, loss = 0.80 (257.8 examples/sec; 0.496 sec/batch)
2018-03-22 16:43:26.538343: step 36860, loss = 0.94 (245.2 examples/sec; 0.522 sec/batch)
2018-03-22 16:43:30.994232: step 36870, loss = 0.87 (287.3 examples/sec; 0.446 sec/batch)
2018-03-22 16:43:35.463377: step 36880, loss = 0.71 (286.4 examples/sec; 0.447 sec/batch)
2018-03-22 16:43:41.041406: step 36890, loss = 0.94 (229.5 examples/sec; 0.558 sec/batch)
2018-03-22 16:43:46.684487: step 36900, loss = 0.82 (226.8 examples/sec; 0.564 sec/batch)
2018-03-22 16:43:51.969411: step 36910, loss = 0.75 (242.2 examples/sec; 0.528 sec/batch)
2018-03-22 16:43:56.464741: step 36920, loss = 0.79 (284.7 examples/sec; 0.450 sec/batch)
2018-03-22 16:44:01.106157: step 36930, loss = 0.63 (275.8 examples/sec; 0.464 sec/batch)
2018-03-22 16:44:06.099354: step 36940, loss = 0.88 (256.3 examples/sec; 0.499 sec/batch)
2018-03-22 16:44:11.067783: step 36950, loss = 0.73 (257.6 examples/sec; 0.497 sec/batch)
2018-03-22 16:44:16.096323: step 36960, loss = 0.71 (254.5 examples/sec; 0.503 sec/batch)
2018-03-22 16:44:21.607761: step 36970, loss = 0.69 (232.2 examples/sec; 0.551 sec/batch)
2018-03-22 16:44:26.483452: step 36980, loss = 0.77 (262.5 examples/sec; 0.488 sec/batch)
2018-03-22 16:44:31.484388: step 36990, loss = 0.81 (256.0 examples/sec; 0.500 sec/batch)
2018-03-22 16:44:36.756658: step 37000, loss = 0.71 (242.8 examples/sec; 0.527 sec/batch)
2018-03-22 16:44:41.896429: step 37010, loss = 0.63 (249.0 examples/sec; 0.514 sec/batch)
2018-03-22 16:44:46.920447: step 37020, loss = 0.62 (254.8 examples/sec; 0.502 sec/batch)
2018-03-22 16:44:51.901438: step 37030, loss = 0.85 (257.0 examples/sec; 0.498 sec/batch)
2018-03-22 16:44:56.320702: step 37040, loss = 0.81 (289.6 examples/sec; 0.442 sec/batch)
2018-03-22 16:45:01.508869: step 37050, loss = 0.72 (246.7 examples/sec; 0.519 sec/batch)
2018-03-22 16:45:06.674452: step 37060, loss = 0.70 (247.8 examples/sec; 0.517 sec/batch)
2018-03-22 16:45:11.755005: step 37070, loss = 0.81 (251.9 examples/sec; 0.508 sec/batch)
2018-03-22 16:45:16.354443: step 37080, loss = 0.71 (278.3 examples/sec; 0.460 sec/batch)
2018-03-22 16:45:21.387393: step 37090, loss = 0.80 (254.3 examples/sec; 0.503 sec/batch)
2018-03-22 16:45:26.870967: step 37100, loss = 0.77 (233.4 examples/sec; 0.548 sec/batch)
2018-03-22 16:45:31.860582: step 37110, loss = 0.86 (256.5 examples/sec; 0.499 sec/batch)
2018-03-22 16:45:36.753403: step 37120, loss = 0.94 (261.6 examples/sec; 0.489 sec/batch)
2018-03-22 16:45:41.989097: step 37130, loss = 0.65 (244.5 examples/sec; 0.524 sec/batch)
2018-03-22 16:45:47.261618: step 37140, loss = 0.77 (242.8 examples/sec; 0.527 sec/batch)
2018-03-22 16:45:52.073596: step 37150, loss = 0.67 (266.0 examples/sec; 0.481 sec/batch)
2018-03-22 16:45:56.989534: step 37160, loss = 0.64 (260.4 examples/sec; 0.492 sec/batch)
2018-03-22 16:46:02.178790: step 37170, loss = 0.61 (246.7 examples/sec; 0.519 sec/batch)
2018-03-22 16:46:07.294612: step 37180, loss = 0.68 (250.2 examples/sec; 0.512 sec/batch)
2018-03-22 16:46:12.817943: step 37190, loss = 0.79 (231.7 examples/sec; 0.552 sec/batch)
2018-03-22 16:46:17.673757: step 37200, loss = 0.82 (263.6 examples/sec; 0.486 sec/batch)
2018-03-22 16:46:22.677411: step 37210, loss = 0.90 (255.8 examples/sec; 0.500 sec/batch)
2018-03-22 16:46:27.930342: step 37220, loss = 0.74 (243.7 examples/sec; 0.525 sec/batch)
2018-03-22 16:46:33.352468: step 37230, loss = 0.68 (236.1 examples/sec; 0.542 sec/batch)
2018-03-22 16:46:38.270537: step 37240, loss = 0.75 (260.3 examples/sec; 0.492 sec/batch)
2018-03-22 16:46:43.540403: step 37250, loss = 0.73 (242.9 examples/sec; 0.527 sec/batch)
2018-03-22 16:46:48.371662: step 37260, loss = 0.78 (264.9 examples/sec; 0.483 sec/batch)
2018-03-22 16:46:53.735873: step 37270, loss = 0.72 (238.6 examples/sec; 0.536 sec/batch)
2018-03-22 16:46:58.222966: step 37280, loss = 0.64 (285.3 examples/sec; 0.449 sec/batch)
2018-03-22 16:47:03.465706: step 37290, loss = 0.56 (244.1 examples/sec; 0.524 sec/batch)
2018-03-22 16:47:09.027291: step 37300, loss = 0.78 (230.2 examples/sec; 0.556 sec/batch)
2018-03-22 16:47:14.129352: step 37310, loss = 0.74 (250.9 examples/sec; 0.510 sec/batch)
2018-03-22 16:47:18.183824: step 37320, loss = 0.72 (315.7 examples/sec; 0.405 sec/batch)
2018-03-22 16:47:22.636679: step 37330, loss = 0.80 (287.5 examples/sec; 0.445 sec/batch)
2018-03-22 16:47:27.525882: step 37340, loss = 0.73 (261.8 examples/sec; 0.489 sec/batch)
2018-03-22 16:47:33.095389: step 37350, loss = 0.82 (229.8 examples/sec; 0.557 sec/batch)
2018-03-22 16:47:37.673809: step 37360, loss = 0.78 (279.6 examples/sec; 0.458 sec/batch)
2018-03-22 16:47:42.910359: step 37370, loss = 0.88 (244.4 examples/sec; 0.524 sec/batch)
2018-03-22 16:47:47.984352: step 37380, loss = 0.70 (252.3 examples/sec; 0.507 sec/batch)
2018-03-22 16:47:53.206380: step 37390, loss = 0.87 (245.1 examples/sec; 0.522 sec/batch)
2018-03-22 16:47:58.141687: step 37400, loss = 0.69 (259.4 examples/sec; 0.494 sec/batch)
2018-03-22 16:48:02.647037: step 37410, loss = 0.84 (284.1 examples/sec; 0.451 sec/batch)
2018-03-22 16:48:07.595406: step 37420, loss = 0.86 (258.7 examples/sec; 0.495 sec/batch)
2018-03-22 16:48:12.329413: step 37430, loss = 0.70 (270.4 examples/sec; 0.473 sec/batch)
2018-03-22 16:48:17.408436: step 37440, loss = 0.76 (252.0 examples/sec; 0.508 sec/batch)
2018-03-22 16:48:22.879196: step 37450, loss = 0.62 (234.0 examples/sec; 0.547 sec/batch)
2018-03-22 16:48:27.568392: step 37460, loss = 0.58 (273.0 examples/sec; 0.469 sec/batch)
2018-03-22 16:48:32.677453: step 37470, loss = 0.70 (250.5 examples/sec; 0.511 sec/batch)
2018-03-22 16:48:37.622387: step 37480, loss = 0.80 (258.9 examples/sec; 0.494 sec/batch)
2018-03-22 16:48:42.890358: step 37490, loss = 0.92 (243.0 examples/sec; 0.527 sec/batch)
2018-03-22 16:48:48.248110: step 37500, loss = 0.60 (238.9 examples/sec; 0.536 sec/batch)
2018-03-22 16:48:53.859471: step 37510, loss = 0.78 (228.1 examples/sec; 0.561 sec/batch)
2018-03-22 16:48:58.539207: step 37520, loss = 0.76 (273.5 examples/sec; 0.468 sec/batch)
2018-03-22 16:49:03.820242: step 37530, loss = 0.80 (242.4 examples/sec; 0.528 sec/batch)
2018-03-22 16:49:09.327313: step 37540, loss = 0.70 (232.4 examples/sec; 0.551 sec/batch)
2018-03-22 16:49:14.228374: step 37550, loss = 0.77 (261.2 examples/sec; 0.490 sec/batch)
2018-03-22 16:49:19.419198: step 37560, loss = 0.69 (246.6 examples/sec; 0.519 sec/batch)
2018-03-22 16:49:24.566472: step 37570, loss = 0.72 (248.7 examples/sec; 0.515 sec/batch)
2018-03-22 16:49:29.829422: step 37580, loss = 0.72 (243.2 examples/sec; 0.526 sec/batch)
2018-03-22 16:49:35.195513: step 37590, loss = 0.81 (238.5 examples/sec; 0.537 sec/batch)
2018-03-22 16:49:40.264244: step 37600, loss = 0.66 (252.5 examples/sec; 0.507 sec/batch)
2018-03-22 16:49:44.891191: step 37610, loss = 0.86 (276.6 examples/sec; 0.463 sec/batch)
2018-03-22 16:49:50.008978: step 37620, loss = 0.88 (250.1 examples/sec; 0.512 sec/batch)
2018-03-22 16:49:55.256119: step 37630, loss = 0.73 (243.9 examples/sec; 0.525 sec/batch)
2018-03-22 16:50:00.788564: step 37640, loss = 0.85 (231.4 examples/sec; 0.553 sec/batch)
2018-03-22 16:50:05.762409: step 37650, loss = 0.81 (257.3 examples/sec; 0.497 sec/batch)
2018-03-22 16:50:11.071876: step 37660, loss = 0.82 (241.1 examples/sec; 0.531 sec/batch)
2018-03-22 16:50:15.975388: step 37670, loss = 0.76 (261.0 examples/sec; 0.490 sec/batch)
2018-03-22 16:50:21.450312: step 37680, loss = 0.90 (233.8 examples/sec; 0.547 sec/batch)
2018-03-22 16:50:26.438010: step 37690, loss = 0.84 (256.6 examples/sec; 0.499 sec/batch)
2018-03-22 16:50:32.360167: step 37700, loss = 0.73 (216.1 examples/sec; 0.592 sec/batch)
2018-03-22 16:50:37.049693: step 37710, loss = 0.76 (272.9 examples/sec; 0.469 sec/batch)
2018-03-22 16:50:42.097430: step 37720, loss = 0.87 (253.6 examples/sec; 0.505 sec/batch)
2018-03-22 16:50:47.070495: step 37730, loss = 0.73 (257.4 examples/sec; 0.497 sec/batch)
2018-03-22 16:50:52.115521: step 37740, loss = 1.04 (253.7 examples/sec; 0.505 sec/batch)
2018-03-22 16:50:57.497423: step 37750, loss = 0.84 (237.8 examples/sec; 0.538 sec/batch)
2018-03-22 16:51:01.721407: step 37760, loss = 0.68 (303.0 examples/sec; 0.422 sec/batch)
2018-03-22 16:51:06.188883: step 37770, loss = 0.94 (286.5 examples/sec; 0.447 sec/batch)
2018-03-22 16:51:11.244178: step 37780, loss = 0.72 (253.2 examples/sec; 0.506 sec/batch)
2018-03-22 16:51:16.379976: step 37790, loss = 0.63 (249.2 examples/sec; 0.514 sec/batch)
2018-03-22 16:51:21.266057: step 37800, loss = 0.69 (262.0 examples/sec; 0.489 sec/batch)
2018-03-22 16:51:26.394413: step 37810, loss = 0.74 (249.6 examples/sec; 0.513 sec/batch)
2018-03-22 16:51:31.908435: step 37820, loss = 0.71 (232.1 examples/sec; 0.551 sec/batch)
2018-03-22 16:51:36.719299: step 37830, loss = 0.92 (266.1 examples/sec; 0.481 sec/batch)
2018-03-22 16:51:41.452424: step 37840, loss = 0.73 (270.4 examples/sec; 0.473 sec/batch)
2018-03-22 16:51:46.107508: step 37850, loss = 0.83 (275.0 examples/sec; 0.466 sec/batch)
2018-03-22 16:51:50.933376: step 37860, loss = 0.71 (265.2 examples/sec; 0.483 sec/batch)
2018-03-22 16:51:55.608865: step 37870, loss = 0.74 (273.8 examples/sec; 0.468 sec/batch)
2018-03-22 16:52:00.809587: step 37880, loss = 0.81 (246.1 examples/sec; 0.520 sec/batch)
2018-03-22 16:52:06.259405: step 37890, loss = 0.66 (234.9 examples/sec; 0.545 sec/batch)
2018-03-22 16:52:11.129824: step 37900, loss = 0.80 (262.8 examples/sec; 0.487 sec/batch)
2018-03-22 16:52:16.304364: step 37910, loss = 0.73 (247.4 examples/sec; 0.517 sec/batch)
2018-03-22 16:52:21.356284: step 37920, loss = 0.71 (253.4 examples/sec; 0.505 sec/batch)
2018-03-22 16:52:26.300053: step 37930, loss = 0.76 (258.9 examples/sec; 0.494 sec/batch)
2018-03-22 16:52:31.318586: step 37940, loss = 0.76 (255.1 examples/sec; 0.502 sec/batch)
2018-03-22 16:52:35.915386: step 37950, loss = 0.81 (278.5 examples/sec; 0.460 sec/batch)
2018-03-22 16:52:41.205410: step 37960, loss = 0.80 (242.0 examples/sec; 0.529 sec/batch)
2018-03-22 16:52:46.655176: step 37970, loss = 0.82 (234.9 examples/sec; 0.545 sec/batch)
2018-03-22 16:52:51.421796: step 37980, loss = 0.78 (268.5 examples/sec; 0.477 sec/batch)
2018-03-22 16:52:56.679428: step 37990, loss = 0.69 (243.5 examples/sec; 0.526 sec/batch)
2018-03-22 16:53:01.866525: step 38000, loss = 0.63 (246.8 examples/sec; 0.519 sec/batch)
2018-03-22 16:53:06.905354: step 38010, loss = 0.78 (254.0 examples/sec; 0.504 sec/batch)
2018-03-22 16:53:12.359439: step 38020, loss = 0.98 (234.7 examples/sec; 0.545 sec/batch)
2018-03-22 16:53:17.519465: step 38030, loss = 0.71 (248.1 examples/sec; 0.516 sec/batch)
2018-03-22 16:53:22.459025: step 38040, loss = 0.67 (259.1 examples/sec; 0.494 sec/batch)
2018-03-22 16:53:27.309375: step 38050, loss = 0.86 (263.9 examples/sec; 0.485 sec/batch)
2018-03-22 16:53:32.559400: step 38060, loss = 0.74 (243.8 examples/sec; 0.525 sec/batch)
2018-03-22 16:53:37.788480: step 38070, loss = 0.68 (244.8 examples/sec; 0.523 sec/batch)
2018-03-22 16:53:43.069992: step 38080, loss = 0.79 (242.4 examples/sec; 0.528 sec/batch)
2018-03-22 16:53:48.430479: step 38090, loss = 0.75 (238.8 examples/sec; 0.536 sec/batch)
2018-03-22 16:53:54.346445: step 38100, loss = 0.69 (216.4 examples/sec; 0.592 sec/batch)
2018-03-22 16:53:59.198936: step 38110, loss = 0.79 (263.8 examples/sec; 0.485 sec/batch)
2018-03-22 16:54:04.089356: step 38120, loss = 0.83 (261.7 examples/sec; 0.489 sec/batch)
2018-03-22 16:54:09.973400: step 38130, loss = 0.75 (217.5 examples/sec; 0.588 sec/batch)
2018-03-22 16:54:14.997247: step 38140, loss = 0.83 (254.8 examples/sec; 0.502 sec/batch)
2018-03-22 16:54:20.119417: step 38150, loss = 0.85 (249.9 examples/sec; 0.512 sec/batch)
2018-03-22 16:54:25.504419: step 38160, loss = 0.78 (237.7 examples/sec; 0.539 sec/batch)
2018-03-22 16:54:30.463697: step 38170, loss = 0.58 (258.1 examples/sec; 0.496 sec/batch)
2018-03-22 16:54:35.225098: step 38180, loss = 0.74 (268.8 examples/sec; 0.476 sec/batch)
2018-03-22 16:54:40.463371: step 38190, loss = 0.86 (244.4 examples/sec; 0.524 sec/batch)
2018-03-22 16:54:45.834096: step 38200, loss = 0.78 (238.3 examples/sec; 0.537 sec/batch)
2018-03-22 16:54:50.945442: step 38210, loss = 0.65 (250.4 examples/sec; 0.511 sec/batch)
2018-03-22 16:54:55.429327: step 38220, loss = 0.96 (285.5 examples/sec; 0.448 sec/batch)
2018-03-22 16:55:00.999521: step 38230, loss = 0.69 (229.8 examples/sec; 0.557 sec/batch)
2018-03-22 16:55:05.854407: step 38240, loss = 0.80 (263.7 examples/sec; 0.485 sec/batch)
2018-03-22 16:55:11.290928: step 38250, loss = 0.75 (235.4 examples/sec; 0.544 sec/batch)
2018-03-22 16:55:16.218977: step 38260, loss = 0.68 (259.7 examples/sec; 0.493 sec/batch)
2018-03-22 16:55:21.329244: step 38270, loss = 0.81 (250.5 examples/sec; 0.511 sec/batch)
2018-03-22 16:55:26.594584: step 38280, loss = 0.80 (243.1 examples/sec; 0.527 sec/batch)
2018-03-22 16:55:31.400455: step 38290, loss = 0.75 (266.3 examples/sec; 0.481 sec/batch)
2018-03-22 16:55:36.026467: step 38300, loss = 0.65 (276.7 examples/sec; 0.463 sec/batch)
2018-03-22 16:55:40.623750: step 38310, loss = 1.09 (278.4 examples/sec; 0.460 sec/batch)
2018-03-22 16:55:45.386349: step 38320, loss = 0.70 (268.8 examples/sec; 0.476 sec/batch)
2018-03-22 16:55:49.851266: step 38330, loss = 0.73 (286.7 examples/sec; 0.446 sec/batch)
2018-03-22 16:55:54.422608: step 38340, loss = 0.82 (280.0 examples/sec; 0.457 sec/batch)
2018-03-22 16:55:59.933134: step 38350, loss = 0.77 (232.3 examples/sec; 0.551 sec/batch)
2018-03-22 16:56:05.171345: step 38360, loss = 0.79 (244.4 examples/sec; 0.524 sec/batch)
2018-03-22 16:56:10.077523: step 38370, loss = 0.73 (260.9 examples/sec; 0.491 sec/batch)
2018-03-22 16:56:15.142880: step 38380, loss = 0.79 (252.7 examples/sec; 0.507 sec/batch)
2018-03-22 16:56:20.803455: step 38390, loss = 0.73 (226.1 examples/sec; 0.566 sec/batch)
2018-03-22 16:56:26.198768: step 38400, loss = 0.79 (237.2 examples/sec; 0.540 sec/batch)
2018-03-22 16:56:31.253230: step 38410, loss = 0.89 (253.2 examples/sec; 0.505 sec/batch)
2018-03-22 16:56:36.250651: step 38420, loss = 0.66 (256.1 examples/sec; 0.500 sec/batch)
2018-03-22 16:56:41.678553: step 38430, loss = 0.66 (235.8 examples/sec; 0.543 sec/batch)
2018-03-22 16:56:46.768441: step 38440, loss = 0.70 (251.5 examples/sec; 0.509 sec/batch)
2018-03-22 16:56:51.637867: step 38450, loss = 0.70 (262.9 examples/sec; 0.487 sec/batch)
2018-03-22 16:56:56.664332: step 38460, loss = 0.84 (254.7 examples/sec; 0.503 sec/batch)
2018-03-22 16:57:02.113410: step 38470, loss = 0.80 (234.9 examples/sec; 0.545 sec/batch)
2018-03-22 16:57:06.933426: step 38480, loss = 0.83 (265.6 examples/sec; 0.482 sec/batch)
2018-03-22 16:57:12.462855: step 38490, loss = 0.73 (231.5 examples/sec; 0.553 sec/batch)
2018-03-22 16:57:17.922594: step 38500, loss = 0.80 (234.4 examples/sec; 0.546 sec/batch)
2018-03-22 16:57:22.774393: step 38510, loss = 0.68 (263.8 examples/sec; 0.485 sec/batch)
2018-03-22 16:57:27.970351: step 38520, loss = 0.86 (246.3 examples/sec; 0.520 sec/batch)
2018-03-22 16:57:32.571384: step 38530, loss = 0.69 (278.2 examples/sec; 0.460 sec/batch)
2018-03-22 16:57:37.638419: step 38540, loss = 0.81 (252.6 examples/sec; 0.507 sec/batch)
2018-03-22 16:57:43.148359: step 38550, loss = 0.83 (232.3 examples/sec; 0.551 sec/batch)
2018-03-22 16:57:48.177387: step 38560, loss = 0.75 (254.5 examples/sec; 0.503 sec/batch)
2018-03-22 16:57:53.667377: step 38570, loss = 0.82 (233.2 examples/sec; 0.549 sec/batch)
2018-03-22 16:57:58.803955: step 38580, loss = 0.82 (249.2 examples/sec; 0.514 sec/batch)
2018-03-22 16:58:03.960471: step 38590, loss = 0.73 (248.2 examples/sec; 0.516 sec/batch)
2018-03-22 16:58:08.793232: step 38600, loss = 0.76 (264.9 examples/sec; 0.483 sec/batch)
2018-03-22 16:58:14.020112: step 38610, loss = 0.76 (244.9 examples/sec; 0.523 sec/batch)
2018-03-22 16:58:19.060557: step 38620, loss = 0.95 (253.9 examples/sec; 0.504 sec/batch)
2018-03-22 16:58:24.387479: step 38630, loss = 0.73 (240.3 examples/sec; 0.533 sec/batch)
2018-03-22 16:58:28.663741: step 38640, loss = 0.79 (299.3 examples/sec; 0.428 sec/batch)
2018-03-22 16:58:33.428457: step 38650, loss = 0.94 (268.6 examples/sec; 0.476 sec/batch)
2018-03-22 16:58:38.270437: step 38660, loss = 0.69 (264.4 examples/sec; 0.484 sec/batch)
2018-03-22 16:58:43.664983: step 38670, loss = 0.65 (237.3 examples/sec; 0.539 sec/batch)
2018-03-22 16:58:48.066416: step 38680, loss = 0.75 (290.8 examples/sec; 0.440 sec/batch)
2018-03-22 16:58:52.848827: step 38690, loss = 0.78 (267.6 examples/sec; 0.478 sec/batch)
2018-03-22 16:58:57.535211: step 38700, loss = 0.64 (273.1 examples/sec; 0.469 sec/batch)
2018-03-22 16:59:03.398717: step 38710, loss = 0.70 (218.3 examples/sec; 0.586 sec/batch)
2018-03-22 16:59:08.266677: step 38720, loss = 0.79 (262.9 examples/sec; 0.487 sec/batch)
2018-03-22 16:59:13.226460: step 38730, loss = 0.75 (258.1 examples/sec; 0.496 sec/batch)
2018-03-22 16:59:18.573320: step 38740, loss = 0.74 (239.4 examples/sec; 0.535 sec/batch)
2018-03-22 16:59:23.744324: step 38750, loss = 0.79 (247.5 examples/sec; 0.517 sec/batch)
2018-03-22 16:59:28.121216: step 38760, loss = 0.70 (292.4 examples/sec; 0.438 sec/batch)
2018-03-22 16:59:32.752108: step 38770, loss = 0.80 (276.4 examples/sec; 0.463 sec/batch)
2018-03-22 16:59:37.395212: step 38780, loss = 0.84 (275.7 examples/sec; 0.464 sec/batch)
2018-03-22 16:59:42.173345: step 38790, loss = 0.74 (267.9 examples/sec; 0.478 sec/batch)
2018-03-22 16:59:47.524523: step 38800, loss = 0.84 (239.2 examples/sec; 0.535 sec/batch)
2018-03-22 16:59:52.721341: step 38810, loss = 0.63 (246.3 examples/sec; 0.520 sec/batch)
2018-03-22 16:59:58.025376: step 38820, loss = 0.93 (241.3 examples/sec; 0.530 sec/batch)
2018-03-22 17:00:03.553340: step 38830, loss = 0.69 (231.6 examples/sec; 0.553 sec/batch)
2018-03-22 17:00:08.002566: step 38840, loss = 0.79 (287.7 examples/sec; 0.445 sec/batch)
2018-03-22 17:00:13.088426: step 38850, loss = 0.90 (251.7 examples/sec; 0.509 sec/batch)
2018-03-22 17:00:18.150466: step 38860, loss = 0.72 (252.9 examples/sec; 0.506 sec/batch)
2018-03-22 17:00:23.632331: step 38870, loss = 0.83 (233.5 examples/sec; 0.548 sec/batch)
2018-03-22 17:00:28.814319: step 38880, loss = 0.67 (247.0 examples/sec; 0.518 sec/batch)
2018-03-22 17:00:33.898388: step 38890, loss = 0.80 (251.8 examples/sec; 0.508 sec/batch)
2018-03-22 17:00:40.054894: step 38900, loss = 0.75 (207.9 examples/sec; 0.616 sec/batch)
2018-03-22 17:00:45.297181: step 38910, loss = 0.75 (244.2 examples/sec; 0.524 sec/batch)
2018-03-22 17:00:50.527959: step 38920, loss = 0.80 (244.7 examples/sec; 0.523 sec/batch)
2018-03-22 17:00:55.195689: step 38930, loss = 0.75 (274.2 examples/sec; 0.467 sec/batch)
2018-03-22 17:01:00.287380: step 38940, loss = 0.72 (251.4 examples/sec; 0.509 sec/batch)
2018-03-22 17:01:05.663839: step 38950, loss = 0.68 (238.1 examples/sec; 0.538 sec/batch)
2018-03-22 17:01:11.093405: step 38960, loss = 0.88 (235.7 examples/sec; 0.543 sec/batch)
2018-03-22 17:01:16.383083: step 38970, loss = 0.76 (242.0 examples/sec; 0.529 sec/batch)
2018-03-22 17:01:21.326468: step 38980, loss = 0.63 (258.9 examples/sec; 0.494 sec/batch)
2018-03-22 17:01:26.325370: step 38990, loss = 0.76 (256.1 examples/sec; 0.500 sec/batch)
2018-03-22 17:01:31.837802: step 39000, loss = 0.64 (232.2 examples/sec; 0.551 sec/batch)
2018-03-22 17:01:36.284346: step 39010, loss = 0.86 (287.9 examples/sec; 0.445 sec/batch)
2018-03-22 17:01:41.896309: step 39020, loss = 0.74 (228.1 examples/sec; 0.561 sec/batch)
2018-03-22 17:01:47.762352: step 39030, loss = 0.83 (218.2 examples/sec; 0.587 sec/batch)
2018-03-22 17:01:52.726651: step 39040, loss = 0.74 (257.8 examples/sec; 0.496 sec/batch)
2018-03-22 17:01:57.653240: step 39050, loss = 0.95 (259.8 examples/sec; 0.493 sec/batch)
2018-03-22 17:02:02.482999: step 39060, loss = 0.79 (265.0 examples/sec; 0.483 sec/batch)
2018-03-22 17:02:07.685426: step 39070, loss = 0.91 (246.0 examples/sec; 0.520 sec/batch)
2018-03-22 17:02:12.753911: step 39080, loss = 0.86 (252.5 examples/sec; 0.507 sec/batch)
2018-03-22 17:02:17.767741: step 39090, loss = 0.65 (255.3 examples/sec; 0.501 sec/batch)
2018-03-22 17:02:23.099488: step 39100, loss = 0.77 (240.1 examples/sec; 0.533 sec/batch)
2018-03-22 17:02:27.569821: step 39110, loss = 0.79 (286.3 examples/sec; 0.447 sec/batch)
2018-03-22 17:02:32.378309: step 39120, loss = 0.67 (266.2 examples/sec; 0.481 sec/batch)
2018-03-22 17:02:37.146161: step 39130, loss = 0.70 (268.5 examples/sec; 0.477 sec/batch)
2018-03-22 17:02:42.133012: step 39140, loss = 0.69 (256.7 examples/sec; 0.499 sec/batch)
2018-03-22 17:02:46.944370: step 39150, loss = 0.85 (266.0 examples/sec; 0.481 sec/batch)
2018-03-22 17:02:52.143379: step 39160, loss = 0.81 (246.2 examples/sec; 0.520 sec/batch)
2018-03-22 17:02:57.346074: step 39170, loss = 0.75 (246.0 examples/sec; 0.520 sec/batch)
2018-03-22 17:03:02.257485: step 39180, loss = 0.79 (260.6 examples/sec; 0.491 sec/batch)
2018-03-22 17:03:06.545519: step 39190, loss = 0.76 (298.5 examples/sec; 0.429 sec/batch)
2018-03-22 17:03:11.725769: step 39200, loss = 0.76 (247.1 examples/sec; 0.518 sec/batch)
2018-03-22 17:03:16.563420: step 39210, loss = 0.71 (264.6 examples/sec; 0.484 sec/batch)
2018-03-22 17:03:21.287317: step 39220, loss = 0.66 (271.0 examples/sec; 0.472 sec/batch)
2018-03-22 17:03:25.631696: step 39230, loss = 0.67 (294.6 examples/sec; 0.434 sec/batch)
2018-03-22 17:03:30.756958: step 39240, loss = 0.79 (249.7 examples/sec; 0.513 sec/batch)
2018-03-22 17:03:35.902371: step 39250, loss = 0.73 (248.8 examples/sec; 0.515 sec/batch)
2018-03-22 17:03:40.921380: step 39260, loss = 0.87 (255.0 examples/sec; 0.502 sec/batch)
2018-03-22 17:03:45.646451: step 39270, loss = 0.70 (270.9 examples/sec; 0.473 sec/batch)
2018-03-22 17:03:50.921877: step 39280, loss = 0.82 (242.6 examples/sec; 0.528 sec/batch)
2018-03-22 17:03:55.873378: step 39290, loss = 0.68 (258.5 examples/sec; 0.495 sec/batch)
2018-03-22 17:04:00.962863: step 39300, loss = 0.73 (251.5 examples/sec; 0.509 sec/batch)
2018-03-22 17:04:05.764637: step 39310, loss = 0.83 (266.6 examples/sec; 0.480 sec/batch)
2018-03-22 17:04:11.221947: step 39320, loss = 0.70 (234.5 examples/sec; 0.546 sec/batch)
2018-03-22 17:04:16.407358: step 39330, loss = 0.80 (246.8 examples/sec; 0.519 sec/batch)
2018-03-22 17:04:21.724430: step 39340, loss = 0.72 (240.7 examples/sec; 0.532 sec/batch)
2018-03-22 17:04:26.690329: step 39350, loss = 0.75 (257.8 examples/sec; 0.497 sec/batch)
2018-03-22 17:04:31.587436: step 39360, loss = 0.95 (261.4 examples/sec; 0.490 sec/batch)
2018-03-22 17:04:36.023432: step 39370, loss = 0.76 (288.5 examples/sec; 0.444 sec/batch)
2018-03-22 17:04:41.413455: step 39380, loss = 0.70 (237.5 examples/sec; 0.539 sec/batch)
2018-03-22 17:04:46.806915: step 39390, loss = 0.75 (237.3 examples/sec; 0.539 sec/batch)
2018-03-22 17:04:51.985232: step 39400, loss = 0.68 (247.2 examples/sec; 0.518 sec/batch)
2018-03-22 17:04:56.620463: step 39410, loss = 0.71 (276.1 examples/sec; 0.464 sec/batch)
2018-03-22 17:05:02.080587: step 39420, loss = 0.75 (234.4 examples/sec; 0.546 sec/batch)
2018-03-22 17:05:06.891723: step 39430, loss = 0.80 (266.0 examples/sec; 0.481 sec/batch)
2018-03-22 17:05:12.175424: step 39440, loss = 0.59 (242.3 examples/sec; 0.528 sec/batch)
2018-03-22 17:05:16.785450: step 39450, loss = 0.70 (277.7 examples/sec; 0.461 sec/batch)
2018-03-22 17:05:21.909388: step 39460, loss = 0.69 (249.8 examples/sec; 0.512 sec/batch)
2018-03-22 17:05:26.871586: step 39470, loss = 0.72 (258.0 examples/sec; 0.496 sec/batch)
2018-03-22 17:05:32.502153: step 39480, loss = 0.79 (227.3 examples/sec; 0.563 sec/batch)
2018-03-22 17:05:37.817017: step 39490, loss = 0.85 (240.8 examples/sec; 0.531 sec/batch)
2018-03-22 17:05:42.833182: step 39500, loss = 0.92 (255.2 examples/sec; 0.502 sec/batch)
2018-03-22 17:05:47.814361: step 39510, loss = 0.82 (257.0 examples/sec; 0.498 sec/batch)
2018-03-22 17:05:53.262618: step 39520, loss = 0.80 (234.9 examples/sec; 0.545 sec/batch)
2018-03-22 17:05:58.391398: step 39530, loss = 0.79 (249.6 examples/sec; 0.513 sec/batch)
2018-03-22 17:06:03.910957: step 39540, loss = 1.04 (231.9 examples/sec; 0.552 sec/batch)
2018-03-22 17:06:08.553444: step 39550, loss = 0.81 (275.7 examples/sec; 0.464 sec/batch)
2018-03-22 17:06:13.615952: step 39560, loss = 0.66 (252.8 examples/sec; 0.506 sec/batch)
2018-03-22 17:06:18.657421: step 39570, loss = 0.91 (253.9 examples/sec; 0.504 sec/batch)
2018-03-22 17:06:24.598415: step 39580, loss = 0.84 (215.5 examples/sec; 0.594 sec/batch)
2018-03-22 17:06:28.662366: step 39590, loss = 0.84 (315.0 examples/sec; 0.406 sec/batch)
2018-03-22 17:06:33.564142: step 39600, loss = 0.73 (261.1 examples/sec; 0.490 sec/batch)
2018-03-22 17:06:38.270400: step 39610, loss = 0.78 (272.0 examples/sec; 0.471 sec/batch)
2018-03-22 17:06:43.234415: step 39620, loss = 0.66 (257.9 examples/sec; 0.496 sec/batch)
2018-03-22 17:06:48.097383: step 39630, loss = 0.73 (263.2 examples/sec; 0.486 sec/batch)
2018-03-22 17:06:53.209457: step 39640, loss = 0.82 (250.4 examples/sec; 0.511 sec/batch)
2018-03-22 17:06:58.557684: step 39650, loss = 0.73 (239.3 examples/sec; 0.535 sec/batch)
2018-03-22 17:07:03.070271: step 39660, loss = 0.79 (283.7 examples/sec; 0.451 sec/batch)
2018-03-22 17:07:07.845267: step 39670, loss = 0.86 (268.1 examples/sec; 0.477 sec/batch)
2018-03-22 17:07:12.164251: step 39680, loss = 0.62 (296.4 examples/sec; 0.432 sec/batch)
2018-03-22 17:07:16.645743: step 39690, loss = 0.65 (285.6 examples/sec; 0.448 sec/batch)
2018-03-22 17:07:22.062850: step 39700, loss = 0.67 (236.3 examples/sec; 0.542 sec/batch)
2018-03-22 17:07:27.261383: step 39710, loss = 0.82 (246.2 examples/sec; 0.520 sec/batch)
2018-03-22 17:07:32.456363: step 39720, loss = 0.69 (246.4 examples/sec; 0.519 sec/batch)
2018-03-22 17:07:37.300361: step 39730, loss = 0.92 (264.2 examples/sec; 0.484 sec/batch)
2018-03-22 17:07:42.457395: step 39740, loss = 0.69 (248.2 examples/sec; 0.516 sec/batch)
2018-03-22 17:07:47.688328: step 39750, loss = 0.98 (244.7 examples/sec; 0.523 sec/batch)
2018-03-22 17:07:52.268315: step 39760, loss = 0.82 (279.5 examples/sec; 0.458 sec/batch)
2018-03-22 17:07:57.279439: step 39770, loss = 0.80 (255.4 examples/sec; 0.501 sec/batch)
2018-03-22 17:08:02.268373: step 39780, loss = 0.61 (256.6 examples/sec; 0.499 sec/batch)
2018-03-22 17:08:07.623907: step 39790, loss = 0.78 (239.0 examples/sec; 0.536 sec/batch)
2018-03-22 17:08:13.053202: step 39800, loss = 0.71 (235.8 examples/sec; 0.543 sec/batch)
2018-03-22 17:08:18.086414: step 39810, loss = 0.74 (254.3 examples/sec; 0.503 sec/batch)
2018-03-22 17:08:23.358378: step 39820, loss = 0.85 (242.8 examples/sec; 0.527 sec/batch)
2018-03-22 17:08:28.641362: step 39830, loss = 0.88 (242.3 examples/sec; 0.528 sec/batch)
2018-03-22 17:08:33.882374: step 39840, loss = 0.81 (244.2 examples/sec; 0.524 sec/batch)
2018-03-22 17:08:38.941876: step 39850, loss = 0.73 (253.0 examples/sec; 0.506 sec/batch)
2018-03-22 17:08:44.581463: step 39860, loss = 0.72 (227.0 examples/sec; 0.564 sec/batch)
2018-03-22 17:08:49.458381: step 39870, loss = 0.75 (262.5 examples/sec; 0.488 sec/batch)
2018-03-22 17:08:54.229352: step 39880, loss = 0.77 (268.3 examples/sec; 0.477 sec/batch)
2018-03-22 17:08:59.222598: step 39890, loss = 0.74 (256.3 examples/sec; 0.499 sec/batch)
2018-03-22 17:09:04.652156: step 39900, loss = 0.69 (235.7 examples/sec; 0.543 sec/batch)
2018-03-22 17:09:09.236412: step 39910, loss = 0.92 (279.2 examples/sec; 0.458 sec/batch)
2018-03-22 17:09:13.976347: step 39920, loss = 0.73 (270.0 examples/sec; 0.474 sec/batch)
2018-03-22 17:09:18.957137: step 39930, loss = 0.74 (257.0 examples/sec; 0.498 sec/batch)
2018-03-22 17:09:24.051772: step 39940, loss = 0.94 (251.2 examples/sec; 0.509 sec/batch)
2018-03-22 17:09:28.954374: step 39950, loss = 0.68 (261.1 examples/sec; 0.490 sec/batch)
2018-03-22 17:09:33.941253: step 39960, loss = 0.91 (256.7 examples/sec; 0.499 sec/batch)
2018-03-22 17:09:38.993412: step 39970, loss = 0.64 (253.4 examples/sec; 0.505 sec/batch)
2018-03-22 17:09:43.685507: step 39980, loss = 0.87 (272.8 examples/sec; 0.469 sec/batch)
2018-03-22 17:09:49.367783: step 39990, loss = 0.75 (225.3 examples/sec; 0.568 sec/batch)
2018-03-22 17:09:54.903264: step 40000, loss = 0.91 (231.2 examples/sec; 0.554 sec/batch)
2018-03-22 17:09:59.870364: step 40010, loss = 1.09 (257.7 examples/sec; 0.497 sec/batch)
2018-03-22 17:10:04.788075: step 40020, loss = 0.81 (260.3 examples/sec; 0.492 sec/batch)
2018-03-22 17:10:09.568422: step 40030, loss = 0.77 (267.8 examples/sec; 0.478 sec/batch)
2018-03-22 17:10:14.429366: step 40040, loss = 0.92 (263.3 examples/sec; 0.486 sec/batch)
2018-03-22 17:10:19.719389: step 40050, loss = 0.68 (242.0 examples/sec; 0.529 sec/batch)
2018-03-22 17:10:24.443234: step 40060, loss = 0.80 (271.0 examples/sec; 0.472 sec/batch)
2018-03-22 17:10:28.615397: step 40070, loss = 0.75 (306.8 examples/sec; 0.417 sec/batch)
2018-03-22 17:10:33.480321: step 40080, loss = 0.77 (263.1 examples/sec; 0.486 sec/batch)
2018-03-22 17:10:38.623834: step 40090, loss = 0.70 (248.9 examples/sec; 0.514 sec/batch)
2018-03-22 17:10:43.522334: step 40100, loss = 0.76 (261.3 examples/sec; 0.490 sec/batch)
2018-03-22 17:10:47.921400: step 40110, loss = 0.71 (291.0 examples/sec; 0.440 sec/batch)
2018-03-22 17:10:53.440253: step 40120, loss = 0.70 (231.9 examples/sec; 0.552 sec/batch)
2018-03-22 17:10:57.642366: step 40130, loss = 0.86 (304.6 examples/sec; 0.420 sec/batch)
2018-03-22 17:11:02.192857: step 40140, loss = 0.58 (281.3 examples/sec; 0.455 sec/batch)
2018-03-22 17:11:06.682553: step 40150, loss = 0.73 (285.1 examples/sec; 0.449 sec/batch)
2018-03-22 17:11:11.182430: step 40160, loss = 0.78 (284.5 examples/sec; 0.450 sec/batch)
2018-03-22 17:11:15.999454: step 40170, loss = 0.79 (265.7 examples/sec; 0.482 sec/batch)
2018-03-22 17:11:20.980426: step 40180, loss = 0.93 (257.0 examples/sec; 0.498 sec/batch)
2018-03-22 17:11:25.627409: step 40190, loss = 0.89 (275.4 examples/sec; 0.465 sec/batch)
2018-03-22 17:11:30.968054: step 40200, loss = 0.82 (239.7 examples/sec; 0.534 sec/batch)
2018-03-22 17:11:36.042800: step 40210, loss = 0.66 (252.2 examples/sec; 0.507 sec/batch)
2018-03-22 17:11:40.971137: step 40220, loss = 0.83 (259.7 examples/sec; 0.493 sec/batch)
2018-03-22 17:11:45.314432: step 40230, loss = 0.91 (294.7 examples/sec; 0.434 sec/batch)
2018-03-22 17:11:50.472398: step 40240, loss = 0.74 (248.2 examples/sec; 0.516 sec/batch)
2018-03-22 17:11:55.072465: step 40250, loss = 0.85 (278.3 examples/sec; 0.460 sec/batch)
2018-03-22 17:12:00.357430: step 40260, loss = 0.70 (242.2 examples/sec; 0.528 sec/batch)
2018-03-22 17:12:05.482428: step 40270, loss = 0.75 (249.8 examples/sec; 0.512 sec/batch)
2018-03-22 17:12:10.347183: step 40280, loss = 0.70 (263.1 examples/sec; 0.486 sec/batch)
2018-03-22 17:12:16.129321: step 40290, loss = 0.78 (221.4 examples/sec; 0.578 sec/batch)
2018-03-22 17:12:22.336425: step 40300, loss = 0.85 (206.2 examples/sec; 0.621 sec/batch)
2018-03-22 17:12:27.039434: step 40310, loss = 0.75 (272.2 examples/sec; 0.470 sec/batch)
2018-03-22 17:12:31.810478: step 40320, loss = 0.69 (268.3 examples/sec; 0.477 sec/batch)
2018-03-22 17:12:36.670449: step 40330, loss = 0.84 (263.4 examples/sec; 0.486 sec/batch)
2018-03-22 17:12:41.564673: step 40340, loss = 0.72 (261.5 examples/sec; 0.489 sec/batch)
2018-03-22 17:12:46.626685: step 40350, loss = 0.78 (252.9 examples/sec; 0.506 sec/batch)
2018-03-22 17:12:51.956427: step 40360, loss = 0.76 (240.2 examples/sec; 0.533 sec/batch)
2018-03-22 17:12:56.768100: step 40370, loss = 0.71 (266.0 examples/sec; 0.481 sec/batch)
2018-03-22 17:13:01.801375: step 40380, loss = 0.87 (254.3 examples/sec; 0.503 sec/batch)
2018-03-22 17:13:06.961360: step 40390, loss = 0.60 (248.1 examples/sec; 0.516 sec/batch)
2018-03-22 17:13:12.503962: step 40400, loss = 0.79 (230.9 examples/sec; 0.554 sec/batch)
2018-03-22 17:13:17.355413: step 40410, loss = 0.81 (263.8 examples/sec; 0.485 sec/batch)
2018-03-22 17:13:22.676550: step 40420, loss = 0.58 (240.6 examples/sec; 0.532 sec/batch)
2018-03-22 17:13:27.775038: step 40430, loss = 0.75 (251.1 examples/sec; 0.510 sec/batch)
2018-03-22 17:13:32.715452: step 40440, loss = 0.68 (259.1 examples/sec; 0.494 sec/batch)
2018-03-22 17:13:37.948383: step 40450, loss = 0.67 (244.6 examples/sec; 0.523 sec/batch)
2018-03-22 17:13:42.965459: step 40460, loss = 0.68 (255.1 examples/sec; 0.502 sec/batch)
2018-03-22 17:13:47.795393: step 40470, loss = 0.77 (265.0 examples/sec; 0.483 sec/batch)
2018-03-22 17:13:52.538392: step 40480, loss = 0.64 (269.9 examples/sec; 0.474 sec/batch)
2018-03-22 17:13:57.180905: step 40490, loss = 0.69 (275.7 examples/sec; 0.464 sec/batch)
2018-03-22 17:14:02.903241: step 40500, loss = 0.83 (223.7 examples/sec; 0.572 sec/batch)
2018-03-22 17:14:07.222147: step 40510, loss = 0.63 (296.4 examples/sec; 0.432 sec/batch)
2018-03-22 17:14:12.721436: step 40520, loss = 0.91 (232.8 examples/sec; 0.550 sec/batch)
2018-03-22 17:14:17.556065: step 40530, loss = 0.63 (264.8 examples/sec; 0.483 sec/batch)
2018-03-22 17:14:22.088345: step 40540, loss = 0.78 (282.4 examples/sec; 0.453 sec/batch)
2018-03-22 17:14:27.048352: step 40550, loss = 0.63 (258.1 examples/sec; 0.496 sec/batch)
2018-03-22 17:14:32.357307: step 40560, loss = 0.87 (241.1 examples/sec; 0.531 sec/batch)
2018-03-22 17:14:36.889762: step 40570, loss = 0.76 (282.4 examples/sec; 0.453 sec/batch)
2018-03-22 17:14:41.816375: step 40580, loss = 0.77 (259.8 examples/sec; 0.493 sec/batch)
2018-03-22 17:14:46.796108: step 40590, loss = 0.72 (257.0 examples/sec; 0.498 sec/batch)
2018-03-22 17:14:51.572739: step 40600, loss = 0.71 (268.0 examples/sec; 0.478 sec/batch)
2018-03-22 17:14:55.903362: step 40610, loss = 0.80 (295.6 examples/sec; 0.433 sec/batch)
2018-03-22 17:15:00.899440: step 40620, loss = 0.79 (256.2 examples/sec; 0.500 sec/batch)
2018-03-22 17:15:06.217851: step 40630, loss = 0.71 (240.7 examples/sec; 0.532 sec/batch)
2018-03-22 17:15:11.222854: step 40640, loss = 0.90 (255.7 examples/sec; 0.501 sec/batch)
2018-03-22 17:15:15.814972: step 40650, loss = 0.68 (278.7 examples/sec; 0.459 sec/batch)
2018-03-22 17:15:20.801557: step 40660, loss = 0.74 (256.7 examples/sec; 0.499 sec/batch)
2018-03-22 17:15:26.350419: step 40670, loss = 0.77 (230.7 examples/sec; 0.555 sec/batch)
2018-03-22 17:15:31.295375: step 40680, loss = 0.87 (258.8 examples/sec; 0.494 sec/batch)
2018-03-22 17:15:36.215387: step 40690, loss = 0.78 (260.2 examples/sec; 0.492 sec/batch)
2018-03-22 17:15:41.782196: step 40700, loss = 0.65 (229.9 examples/sec; 0.557 sec/batch)
2018-03-22 17:15:46.336476: step 40710, loss = 0.84 (281.1 examples/sec; 0.455 sec/batch)
2018-03-22 17:15:51.525393: step 40720, loss = 0.70 (246.7 examples/sec; 0.519 sec/batch)
2018-03-22 17:15:56.348884: step 40730, loss = 0.76 (265.4 examples/sec; 0.482 sec/batch)
2018-03-22 17:16:01.654422: step 40740, loss = 0.84 (241.3 examples/sec; 0.531 sec/batch)
2018-03-22 17:16:07.262383: step 40750, loss = 0.74 (228.2 examples/sec; 0.561 sec/batch)
2018-03-22 17:16:12.528592: step 40760, loss = 0.82 (243.1 examples/sec; 0.527 sec/batch)
2018-03-22 17:16:17.556373: step 40770, loss = 0.75 (254.6 examples/sec; 0.503 sec/batch)
2018-03-22 17:16:22.129396: step 40780, loss = 0.78 (279.9 examples/sec; 0.457 sec/batch)
2018-03-22 17:16:27.623962: step 40790, loss = 0.78 (233.0 examples/sec; 0.549 sec/batch)
2018-03-22 17:16:33.007488: step 40800, loss = 0.63 (237.8 examples/sec; 0.538 sec/batch)
2018-03-22 17:16:37.873760: step 40810, loss = 0.66 (263.0 examples/sec; 0.487 sec/batch)
2018-03-22 17:16:42.858993: step 40820, loss = 0.84 (256.8 examples/sec; 0.499 sec/batch)
2018-03-22 17:16:48.185454: step 40830, loss = 0.72 (240.3 examples/sec; 0.533 sec/batch)
2018-03-22 17:16:53.177361: step 40840, loss = 0.74 (256.4 examples/sec; 0.499 sec/batch)
2018-03-22 17:16:58.395817: step 40850, loss = 0.84 (245.3 examples/sec; 0.522 sec/batch)
2018-03-22 17:17:03.583955: step 40860, loss = 0.81 (246.7 examples/sec; 0.519 sec/batch)
2018-03-22 17:17:08.737940: step 40870, loss = 0.65 (248.4 examples/sec; 0.515 sec/batch)
2018-03-22 17:17:14.053012: step 40880, loss = 0.74 (240.8 examples/sec; 0.532 sec/batch)
2018-03-22 17:17:18.929389: step 40890, loss = 0.68 (262.5 examples/sec; 0.488 sec/batch)
2018-03-22 17:17:24.282585: step 40900, loss = 0.80 (239.1 examples/sec; 0.535 sec/batch)
2018-03-22 17:17:29.616983: step 40910, loss = 0.83 (240.0 examples/sec; 0.533 sec/batch)
2018-03-22 17:17:34.450438: step 40920, loss = 0.78 (264.8 examples/sec; 0.483 sec/batch)
2018-03-22 17:17:39.289410: step 40930, loss = 0.76 (264.5 examples/sec; 0.484 sec/batch)
2018-03-22 17:17:44.764406: step 40940, loss = 0.69 (233.8 examples/sec; 0.547 sec/batch)
2018-03-22 17:17:50.070097: step 40950, loss = 0.73 (241.3 examples/sec; 0.531 sec/batch)
2018-03-22 17:17:55.463065: step 40960, loss = 0.67 (237.3 examples/sec; 0.539 sec/batch)
2018-03-22 17:18:00.046831: step 40970, loss = 0.70 (279.2 examples/sec; 0.458 sec/batch)
2018-03-22 17:18:04.354089: step 40980, loss = 0.68 (297.2 examples/sec; 0.431 sec/batch)
2018-03-22 17:18:09.502324: step 40990, loss = 0.72 (248.6 examples/sec; 0.515 sec/batch)
2018-03-22 17:18:14.765315: step 41000, loss = 0.86 (243.2 examples/sec; 0.526 sec/batch)
2018-03-22 17:18:19.038371: step 41010, loss = 0.99 (299.6 examples/sec; 0.427 sec/batch)
2018-03-22 17:18:23.949401: step 41020, loss = 0.77 (260.6 examples/sec; 0.491 sec/batch)
2018-03-22 17:18:29.731330: step 41030, loss = 0.82 (221.4 examples/sec; 0.578 sec/batch)
2018-03-22 17:18:34.626497: step 41040, loss = 0.77 (261.5 examples/sec; 0.490 sec/batch)
2018-03-22 17:18:40.156388: step 41050, loss = 0.62 (231.5 examples/sec; 0.553 sec/batch)
2018-03-22 17:18:44.995260: step 41060, loss = 0.76 (264.5 examples/sec; 0.484 sec/batch)
2018-03-22 17:18:49.412272: step 41070, loss = 0.89 (289.8 examples/sec; 0.442 sec/batch)
2018-03-22 17:18:54.708375: step 41080, loss = 0.70 (241.7 examples/sec; 0.530 sec/batch)
2018-03-22 17:18:59.840384: step 41090, loss = 0.59 (249.4 examples/sec; 0.513 sec/batch)
2018-03-22 17:19:05.328086: step 41100, loss = 0.79 (233.2 examples/sec; 0.549 sec/batch)
2018-03-22 17:19:10.235211: step 41110, loss = 0.66 (260.8 examples/sec; 0.491 sec/batch)
2018-03-22 17:19:14.760886: step 41120, loss = 0.80 (282.8 examples/sec; 0.453 sec/batch)
2018-03-22 17:19:19.520430: step 41130, loss = 0.67 (268.9 examples/sec; 0.476 sec/batch)
2018-03-22 17:19:24.503340: step 41140, loss = 0.79 (256.9 examples/sec; 0.498 sec/batch)
2018-03-22 17:19:30.437892: step 41150, loss = 0.65 (215.7 examples/sec; 0.593 sec/batch)
2018-03-22 17:19:35.277401: step 41160, loss = 0.67 (264.5 examples/sec; 0.484 sec/batch)
2018-03-22 17:19:40.794353: step 41170, loss = 0.82 (232.0 examples/sec; 0.552 sec/batch)
2018-03-22 17:19:45.501454: step 41180, loss = 0.73 (271.9 examples/sec; 0.471 sec/batch)
2018-03-22 17:19:50.083431: step 41190, loss = 0.82 (279.4 examples/sec; 0.458 sec/batch)
2018-03-22 17:19:55.428813: step 41200, loss = 0.87 (239.5 examples/sec; 0.535 sec/batch)
2018-03-22 17:20:00.306354: step 41210, loss = 0.90 (262.4 examples/sec; 0.488 sec/batch)
2018-03-22 17:20:05.003142: step 41220, loss = 0.69 (272.5 examples/sec; 0.470 sec/batch)
2018-03-22 17:20:09.745437: step 41230, loss = 0.71 (269.9 examples/sec; 0.474 sec/batch)
2018-03-22 17:20:14.760349: step 41240, loss = 0.88 (255.2 examples/sec; 0.501 sec/batch)
2018-03-22 17:20:19.946746: step 41250, loss = 0.70 (246.8 examples/sec; 0.519 sec/batch)
2018-03-22 17:20:25.463438: step 41260, loss = 0.78 (232.0 examples/sec; 0.552 sec/batch)
2018-03-22 17:20:30.327123: step 41270, loss = 0.83 (263.2 examples/sec; 0.486 sec/batch)
2018-03-22 17:20:35.096630: step 41280, loss = 0.79 (268.4 examples/sec; 0.477 sec/batch)
2018-03-22 17:20:40.440307: step 41290, loss = 0.72 (239.5 examples/sec; 0.534 sec/batch)
2018-03-22 17:20:46.025360: step 41300, loss = 0.88 (229.2 examples/sec; 0.559 sec/batch)
2018-03-22 17:20:51.094419: step 41310, loss = 0.58 (252.5 examples/sec; 0.507 sec/batch)
2018-03-22 17:20:55.904304: step 41320, loss = 0.68 (266.1 examples/sec; 0.481 sec/batch)
2018-03-22 17:21:01.040933: step 41330, loss = 0.85 (249.2 examples/sec; 0.514 sec/batch)
2018-03-22 17:21:05.733679: step 41340, loss = 0.87 (272.8 examples/sec; 0.469 sec/batch)
2018-03-22 17:21:11.026482: step 41350, loss = 0.87 (241.8 examples/sec; 0.529 sec/batch)
2018-03-22 17:21:15.747115: step 41360, loss = 0.77 (271.1 examples/sec; 0.472 sec/batch)
2018-03-22 17:21:21.375334: step 41370, loss = 0.84 (227.4 examples/sec; 0.563 sec/batch)
2018-03-22 17:21:26.371350: step 41380, loss = 0.73 (256.2 examples/sec; 0.500 sec/batch)
2018-03-22 17:21:31.191329: step 41390, loss = 0.89 (265.6 examples/sec; 0.482 sec/batch)
2018-03-22 17:21:35.882928: step 41400, loss = 0.69 (272.8 examples/sec; 0.469 sec/batch)
2018-03-22 17:21:40.721380: step 41410, loss = 0.69 (264.5 examples/sec; 0.484 sec/batch)
2018-03-22 17:21:45.755557: step 41420, loss = 0.77 (254.3 examples/sec; 0.503 sec/batch)
2018-03-22 17:21:50.648919: step 41430, loss = 0.73 (261.6 examples/sec; 0.489 sec/batch)
2018-03-22 17:21:55.217026: step 41440, loss = 0.79 (280.2 examples/sec; 0.457 sec/batch)
2018-03-22 17:21:59.952402: step 41450, loss = 0.71 (270.3 examples/sec; 0.474 sec/batch)
2018-03-22 17:22:04.226554: step 41460, loss = 0.66 (299.5 examples/sec; 0.427 sec/batch)
2018-03-22 17:22:08.406883: step 41470, loss = 1.02 (306.2 examples/sec; 0.418 sec/batch)
2018-03-22 17:22:13.166172: step 41480, loss = 0.59 (268.9 examples/sec; 0.476 sec/batch)
2018-03-22 17:22:17.802398: step 41490, loss = 0.85 (276.1 examples/sec; 0.464 sec/batch)
2018-03-22 17:22:23.627349: step 41500, loss = 0.71 (219.7 examples/sec; 0.582 sec/batch)
2018-03-22 17:22:28.648277: step 41510, loss = 0.83 (254.9 examples/sec; 0.502 sec/batch)
2018-03-22 17:22:33.952315: step 41520, loss = 0.75 (241.3 examples/sec; 0.530 sec/batch)
2018-03-22 17:22:38.955152: step 41530, loss = 0.73 (255.9 examples/sec; 0.500 sec/batch)
2018-03-22 17:22:44.434439: step 41540, loss = 0.63 (233.6 examples/sec; 0.548 sec/batch)
2018-03-22 17:22:49.724141: step 41550, loss = 0.88 (242.0 examples/sec; 0.529 sec/batch)
2018-03-22 17:22:54.522430: step 41560, loss = 0.80 (266.8 examples/sec; 0.480 sec/batch)
2018-03-22 17:23:00.080449: step 41570, loss = 0.80 (230.3 examples/sec; 0.556 sec/batch)
2018-03-22 17:23:04.731802: step 41580, loss = 0.84 (275.2 examples/sec; 0.465 sec/batch)
2018-03-22 17:23:09.721337: step 41590, loss = 0.52 (256.5 examples/sec; 0.499 sec/batch)
2018-03-22 17:23:14.891433: step 41600, loss = 0.83 (247.6 examples/sec; 0.517 sec/batch)
2018-03-22 17:23:20.256351: step 41610, loss = 0.89 (238.6 examples/sec; 0.536 sec/batch)
2018-03-22 17:23:24.925759: step 41620, loss = 0.82 (274.1 examples/sec; 0.467 sec/batch)
2018-03-22 17:23:30.546323: step 41630, loss = 0.91 (227.7 examples/sec; 0.562 sec/batch)
2018-03-22 17:23:35.788082: step 41640, loss = 0.83 (244.2 examples/sec; 0.524 sec/batch)
2018-03-22 17:23:40.873370: step 41650, loss = 0.58 (251.7 examples/sec; 0.509 sec/batch)
2018-03-22 17:23:45.740406: step 41660, loss = 0.69 (263.0 examples/sec; 0.487 sec/batch)
2018-03-22 17:23:50.603650: step 41670, loss = 0.79 (263.2 examples/sec; 0.486 sec/batch)
2018-03-22 17:23:55.088219: step 41680, loss = 0.75 (285.4 examples/sec; 0.448 sec/batch)
2018-03-22 17:24:00.844402: step 41690, loss = 0.73 (222.4 examples/sec; 0.576 sec/batch)
2018-03-22 17:24:05.456343: step 41700, loss = 0.70 (277.5 examples/sec; 0.461 sec/batch)
2018-03-22 17:24:11.343664: step 41710, loss = 0.74 (217.4 examples/sec; 0.589 sec/batch)
2018-03-22 17:24:16.284420: step 41720, loss = 0.68 (259.1 examples/sec; 0.494 sec/batch)
2018-03-22 17:24:21.335347: step 41730, loss = 0.77 (253.4 examples/sec; 0.505 sec/batch)
2018-03-22 17:24:26.165369: step 41740, loss = 0.80 (265.0 examples/sec; 0.483 sec/batch)
2018-03-22 17:24:31.461528: step 41750, loss = 0.78 (241.7 examples/sec; 0.530 sec/batch)
2018-03-22 17:24:36.555686: step 41760, loss = 0.61 (251.3 examples/sec; 0.509 sec/batch)
2018-03-22 17:24:41.689797: step 41770, loss = 0.84 (249.3 examples/sec; 0.513 sec/batch)
2018-03-22 17:24:46.404438: step 41780, loss = 0.93 (271.5 examples/sec; 0.471 sec/batch)
2018-03-22 17:24:51.681348: step 41790, loss = 0.73 (242.6 examples/sec; 0.528 sec/batch)
2018-03-22 17:24:56.820769: step 41800, loss = 0.68 (249.1 examples/sec; 0.514 sec/batch)
2018-03-22 17:25:01.940881: step 41810, loss = 0.83 (250.0 examples/sec; 0.512 sec/batch)
2018-03-22 17:25:06.674384: step 41820, loss = 0.86 (270.4 examples/sec; 0.473 sec/batch)
2018-03-22 17:25:12.334325: step 41830, loss = 0.73 (226.2 examples/sec; 0.566 sec/batch)
2018-03-22 17:25:17.468736: step 41840, loss = 0.71 (249.3 examples/sec; 0.513 sec/batch)
2018-03-22 17:25:22.478425: step 41850, loss = 0.77 (255.5 examples/sec; 0.501 sec/batch)
2018-03-22 17:25:27.049021: step 41860, loss = 0.84 (280.1 examples/sec; 0.457 sec/batch)
2018-03-22 17:25:31.861944: step 41870, loss = 0.72 (266.0 examples/sec; 0.481 sec/batch)
2018-03-22 17:25:36.620906: step 41880, loss = 0.62 (269.0 examples/sec; 0.476 sec/batch)
2018-03-22 17:25:41.835289: step 41890, loss = 0.78 (245.5 examples/sec; 0.521 sec/batch)
2018-03-22 17:25:46.415972: step 41900, loss = 0.71 (279.4 examples/sec; 0.458 sec/batch)
2018-03-22 17:25:51.564342: step 41910, loss = 0.61 (248.6 examples/sec; 0.515 sec/batch)
2018-03-22 17:25:56.020040: step 41920, loss = 0.74 (287.3 examples/sec; 0.446 sec/batch)
2018-03-22 17:26:00.843194: step 41930, loss = 0.73 (265.4 examples/sec; 0.482 sec/batch)
2018-03-22 17:26:05.007367: step 41940, loss = 0.76 (307.4 examples/sec; 0.416 sec/batch)
2018-03-22 17:26:10.424451: step 41950, loss = 0.80 (236.3 examples/sec; 0.542 sec/batch)
2018-03-22 17:26:15.280272: step 41960, loss = 0.83 (263.6 examples/sec; 0.486 sec/batch)
2018-03-22 17:26:20.468328: step 41970, loss = 0.69 (246.7 examples/sec; 0.519 sec/batch)
2018-03-22 17:26:25.466398: step 41980, loss = 0.74 (256.1 examples/sec; 0.500 sec/batch)
2018-03-22 17:26:30.361741: step 41990, loss = 0.76 (261.5 examples/sec; 0.490 sec/batch)
2018-03-22 17:26:35.338077: step 42000, loss = 0.79 (257.2 examples/sec; 0.498 sec/batch)
2018-03-22 17:26:39.920450: step 42010, loss = 0.77 (279.3 examples/sec; 0.458 sec/batch)
2018-03-22 17:26:45.244445: step 42020, loss = 0.75 (240.4 examples/sec; 0.532 sec/batch)
2018-03-22 17:26:50.319376: step 42030, loss = 0.61 (252.2 examples/sec; 0.507 sec/batch)
2018-03-22 17:26:55.241621: step 42040, loss = 0.67 (260.0 examples/sec; 0.492 sec/batch)
2018-03-22 17:27:01.215414: step 42050, loss = 0.89 (214.3 examples/sec; 0.597 sec/batch)
2018-03-22 17:27:06.256965: step 42060, loss = 0.79 (253.9 examples/sec; 0.504 sec/batch)
2018-03-22 17:27:11.318383: step 42070, loss = 0.79 (252.9 examples/sec; 0.506 sec/batch)
2018-03-22 17:27:16.768359: step 42080, loss = 0.77 (234.9 examples/sec; 0.545 sec/batch)
2018-03-22 17:27:22.025447: step 42090, loss = 0.79 (243.5 examples/sec; 0.526 sec/batch)
2018-03-22 17:27:26.967649: step 42100, loss = 0.76 (259.0 examples/sec; 0.494 sec/batch)
2018-03-22 17:27:31.920332: step 42110, loss = 0.61 (258.4 examples/sec; 0.495 sec/batch)
2018-03-22 17:27:36.598616: step 42120, loss = 0.72 (273.6 examples/sec; 0.468 sec/batch)
2018-03-22 17:27:41.818917: step 42130, loss = 0.78 (245.3 examples/sec; 0.522 sec/batch)
2018-03-22 17:27:46.479492: step 42140, loss = 0.87 (274.5 examples/sec; 0.466 sec/batch)
2018-03-22 17:27:51.846699: step 42150, loss = 0.90 (238.5 examples/sec; 0.537 sec/batch)
2018-03-22 17:27:56.572296: step 42160, loss = 0.66 (270.9 examples/sec; 0.473 sec/batch)
2018-03-22 17:28:01.826789: step 42170, loss = 0.89 (243.6 examples/sec; 0.525 sec/batch)
2018-03-22 17:28:06.653371: step 42180, loss = 0.77 (265.2 examples/sec; 0.483 sec/batch)
2018-03-22 17:28:12.089393: step 42190, loss = 0.64 (235.5 examples/sec; 0.544 sec/batch)
2018-03-22 17:28:17.383630: step 42200, loss = 0.85 (241.8 examples/sec; 0.529 sec/batch)
2018-03-22 17:28:22.428325: step 42210, loss = 0.70 (253.7 examples/sec; 0.504 sec/batch)
2018-03-22 17:28:26.763175: step 42220, loss = 0.72 (295.3 examples/sec; 0.433 sec/batch)
2018-03-22 17:28:31.909262: step 42230, loss = 0.76 (248.7 examples/sec; 0.515 sec/batch)
2018-03-22 17:28:37.120810: step 42240, loss = 0.76 (245.6 examples/sec; 0.521 sec/batch)
2018-03-22 17:28:42.067810: step 42250, loss = 0.78 (258.7 examples/sec; 0.495 sec/batch)
2018-03-22 17:28:47.202437: step 42260, loss = 0.65 (249.3 examples/sec; 0.513 sec/batch)
2018-03-22 17:28:52.702433: step 42270, loss = 0.87 (232.7 examples/sec; 0.550 sec/batch)
2018-03-22 17:28:58.042410: step 42280, loss = 0.70 (239.7 examples/sec; 0.534 sec/batch)
2018-03-22 17:29:03.404822: step 42290, loss = 0.81 (238.7 examples/sec; 0.536 sec/batch)
2018-03-22 17:29:08.689519: step 42300, loss = 0.83 (242.2 examples/sec; 0.528 sec/batch)
2018-03-22 17:29:13.514699: step 42310, loss = 0.75 (265.3 examples/sec; 0.483 sec/batch)
2018-03-22 17:29:18.232878: step 42320, loss = 1.09 (271.3 examples/sec; 0.472 sec/batch)
2018-03-22 17:29:23.087305: step 42330, loss = 0.72 (263.7 examples/sec; 0.485 sec/batch)
2018-03-22 17:29:28.062351: step 42340, loss = 0.67 (257.3 examples/sec; 0.498 sec/batch)
2018-03-22 17:29:32.950404: step 42350, loss = 1.00 (261.9 examples/sec; 0.489 sec/batch)
2018-03-22 17:29:38.069360: step 42360, loss = 0.72 (250.1 examples/sec; 0.512 sec/batch)
2018-03-22 17:29:42.780697: step 42370, loss = 0.81 (271.7 examples/sec; 0.471 sec/batch)
2018-03-22 17:29:46.974338: step 42380, loss = 0.64 (305.2 examples/sec; 0.419 sec/batch)
2018-03-22 17:29:51.683461: step 42390, loss = 0.76 (271.8 examples/sec; 0.471 sec/batch)
2018-03-22 17:29:56.009581: step 42400, loss = 0.73 (295.9 examples/sec; 0.433 sec/batch)
2018-03-22 17:30:00.293230: step 42410, loss = 0.84 (298.8 examples/sec; 0.428 sec/batch)
2018-03-22 17:30:05.564518: step 42420, loss = 0.67 (242.8 examples/sec; 0.527 sec/batch)
2018-03-22 17:30:10.771441: step 42430, loss = 0.65 (245.8 examples/sec; 0.521 sec/batch)
2018-03-22 17:30:15.622329: step 42440, loss = 0.72 (263.9 examples/sec; 0.485 sec/batch)
2018-03-22 17:30:20.951758: step 42450, loss = 0.65 (240.2 examples/sec; 0.533 sec/batch)
2018-03-22 17:30:26.030240: step 42460, loss = 0.92 (252.0 examples/sec; 0.508 sec/batch)
2018-03-22 17:30:30.914314: step 42470, loss = 0.69 (262.1 examples/sec; 0.488 sec/batch)
2018-03-22 17:30:36.102446: step 42480, loss = 0.88 (246.7 examples/sec; 0.519 sec/batch)
2018-03-22 17:30:41.217341: step 42490, loss = 0.77 (250.2 examples/sec; 0.511 sec/batch)
2018-03-22 17:30:46.345594: step 42500, loss = 0.75 (249.6 examples/sec; 0.513 sec/batch)
2018-03-22 17:30:52.052523: step 42510, loss = 0.69 (224.3 examples/sec; 0.571 sec/batch)
2018-03-22 17:30:57.151373: step 42520, loss = 0.87 (251.0 examples/sec; 0.510 sec/batch)
2018-03-22 17:31:01.733551: step 42530, loss = 0.66 (279.3 examples/sec; 0.458 sec/batch)
2018-03-22 17:31:06.994967: step 42540, loss = 0.71 (243.3 examples/sec; 0.526 sec/batch)
2018-03-22 17:31:12.046460: step 42550, loss = 0.69 (253.4 examples/sec; 0.505 sec/batch)
2018-03-22 17:31:17.003563: step 42560, loss = 0.68 (258.2 examples/sec; 0.496 sec/batch)
2018-03-22 17:31:22.298320: step 42570, loss = 0.70 (241.7 examples/sec; 0.529 sec/batch)
2018-03-22 17:31:27.223647: step 42580, loss = 0.81 (259.9 examples/sec; 0.493 sec/batch)
2018-03-22 17:31:32.434412: step 42590, loss = 0.71 (245.6 examples/sec; 0.521 sec/batch)
2018-03-22 17:31:37.894287: step 42600, loss = 0.70 (234.4 examples/sec; 0.546 sec/batch)
2018-03-22 17:31:43.083372: step 42610, loss = 0.70 (246.7 examples/sec; 0.519 sec/batch)
2018-03-22 17:31:47.843759: step 42620, loss = 0.66 (268.9 examples/sec; 0.476 sec/batch)
2018-03-22 17:31:52.740417: step 42630, loss = 0.73 (261.4 examples/sec; 0.490 sec/batch)
2018-03-22 17:31:57.562402: step 42640, loss = 0.77 (265.5 examples/sec; 0.482 sec/batch)
2018-03-22 17:32:02.915276: step 42650, loss = 0.63 (239.1 examples/sec; 0.535 sec/batch)
2018-03-22 17:32:07.986387: step 42660, loss = 0.83 (252.4 examples/sec; 0.507 sec/batch)
2018-03-22 17:32:13.143830: step 42670, loss = 0.72 (248.2 examples/sec; 0.516 sec/batch)
2018-03-22 17:32:17.733877: step 42680, loss = 0.81 (278.9 examples/sec; 0.459 sec/batch)
2018-03-22 17:32:22.791361: step 42690, loss = 0.79 (253.1 examples/sec; 0.506 sec/batch)
2018-03-22 17:32:28.610907: step 42700, loss = 0.72 (219.9 examples/sec; 0.582 sec/batch)
2018-03-22 17:32:33.683546: step 42710, loss = 0.72 (252.3 examples/sec; 0.507 sec/batch)
2018-03-22 17:32:38.472764: step 42720, loss = 0.84 (267.3 examples/sec; 0.479 sec/batch)
2018-03-22 17:32:43.725517: step 42730, loss = 0.80 (243.7 examples/sec; 0.525 sec/batch)
2018-03-22 17:32:48.641453: step 42740, loss = 0.87 (260.4 examples/sec; 0.492 sec/batch)
2018-03-22 17:32:53.507171: step 42750, loss = 0.78 (263.1 examples/sec; 0.487 sec/batch)
2018-03-22 17:32:58.866317: step 42760, loss = 0.82 (238.8 examples/sec; 0.536 sec/batch)
2018-03-22 17:33:03.802310: step 42770, loss = 0.68 (259.3 examples/sec; 0.494 sec/batch)
2018-03-22 17:33:09.261462: step 42780, loss = 0.85 (234.5 examples/sec; 0.546 sec/batch)
2018-03-22 17:33:14.527167: step 42790, loss = 0.76 (243.1 examples/sec; 0.527 sec/batch)
2018-03-22 17:33:19.784000: step 42800, loss = 0.84 (243.5 examples/sec; 0.526 sec/batch)
2018-03-22 17:33:24.621348: step 42810, loss = 0.74 (264.6 examples/sec; 0.484 sec/batch)
2018-03-22 17:33:29.725364: step 42820, loss = 0.79 (250.8 examples/sec; 0.510 sec/batch)
2018-03-22 17:33:34.302176: step 42830, loss = 0.67 (279.7 examples/sec; 0.458 sec/batch)
2018-03-22 17:33:38.723961: step 42840, loss = 0.79 (289.5 examples/sec; 0.442 sec/batch)
2018-03-22 17:33:43.864054: step 42850, loss = 0.78 (249.0 examples/sec; 0.514 sec/batch)
2018-03-22 17:33:48.277444: step 42860, loss = 0.81 (290.0 examples/sec; 0.441 sec/batch)
2018-03-22 17:33:52.970393: step 42870, loss = 0.75 (272.7 examples/sec; 0.469 sec/batch)
2018-03-22 17:33:57.822987: step 42880, loss = 0.63 (263.8 examples/sec; 0.485 sec/batch)
2018-03-22 17:34:03.044005: step 42890, loss = 0.79 (245.2 examples/sec; 0.522 sec/batch)
2018-03-22 17:34:08.942724: step 42900, loss = 0.71 (217.0 examples/sec; 0.590 sec/batch)
2018-03-22 17:34:14.010480: step 42910, loss = 0.92 (252.6 examples/sec; 0.507 sec/batch)
2018-03-22 17:34:18.391432: step 42920, loss = 0.73 (292.2 examples/sec; 0.438 sec/batch)
2018-03-22 17:34:23.302383: step 42930, loss = 0.72 (260.6 examples/sec; 0.491 sec/batch)
2018-03-22 17:34:28.566113: step 42940, loss = 0.69 (243.2 examples/sec; 0.526 sec/batch)
2018-03-22 17:34:33.684323: step 42950, loss = 0.73 (250.1 examples/sec; 0.512 sec/batch)
2018-03-22 17:34:38.690540: step 42960, loss = 0.93 (255.7 examples/sec; 0.501 sec/batch)
2018-03-22 17:34:43.736272: step 42970, loss = 0.65 (253.7 examples/sec; 0.505 sec/batch)
2018-03-22 17:34:48.491366: step 42980, loss = 0.79 (269.2 examples/sec; 0.476 sec/batch)
2018-03-22 17:34:54.197376: step 42990, loss = 0.90 (224.3 examples/sec; 0.571 sec/batch)
2018-03-22 17:34:59.626860: step 43000, loss = 0.73 (235.7 examples/sec; 0.543 sec/batch)
2018-03-22 17:35:04.373359: step 43010, loss = 0.72 (269.7 examples/sec; 0.475 sec/batch)
2018-03-22 17:35:09.540431: step 43020, loss = 0.71 (247.7 examples/sec; 0.517 sec/batch)
2018-03-22 17:35:14.774343: step 43030, loss = 0.73 (244.6 examples/sec; 0.523 sec/batch)
2018-03-22 17:35:19.593295: step 43040, loss = 0.69 (265.6 examples/sec; 0.482 sec/batch)
2018-03-22 17:35:24.566301: step 43050, loss = 0.68 (257.4 examples/sec; 0.497 sec/batch)
2018-03-22 17:35:30.250320: step 43060, loss = 0.76 (225.2 examples/sec; 0.568 sec/batch)
2018-03-22 17:35:35.138263: step 43070, loss = 0.67 (261.9 examples/sec; 0.489 sec/batch)
2018-03-22 17:35:40.317223: step 43080, loss = 0.65 (247.2 examples/sec; 0.518 sec/batch)
2018-03-22 17:35:45.086351: step 43090, loss = 0.76 (268.4 examples/sec; 0.477 sec/batch)
2018-03-22 17:35:50.418192: step 43100, loss = 0.83 (240.1 examples/sec; 0.533 sec/batch)
2018-03-22 17:35:55.561676: step 43110, loss = 0.64 (248.9 examples/sec; 0.514 sec/batch)
2018-03-22 17:36:00.467484: step 43120, loss = 0.88 (260.9 examples/sec; 0.491 sec/batch)
2018-03-22 17:36:05.527223: step 43130, loss = 0.83 (253.0 examples/sec; 0.506 sec/batch)
2018-03-22 17:36:10.865741: step 43140, loss = 0.66 (239.8 examples/sec; 0.534 sec/batch)
2018-03-22 17:36:15.840369: step 43150, loss = 0.73 (257.3 examples/sec; 0.497 sec/batch)
2018-03-22 17:36:20.983394: step 43160, loss = 0.85 (248.9 examples/sec; 0.514 sec/batch)
2018-03-22 17:36:25.805071: step 43170, loss = 0.82 (265.5 examples/sec; 0.482 sec/batch)
2018-03-22 17:36:31.323084: step 43180, loss = 0.84 (232.0 examples/sec; 0.552 sec/batch)
2018-03-22 17:36:35.948780: step 43190, loss = 0.64 (276.7 examples/sec; 0.463 sec/batch)
2018-03-22 17:36:41.007842: step 43200, loss = 0.82 (253.0 examples/sec; 0.506 sec/batch)
2018-03-22 17:36:45.899458: step 43210, loss = 0.77 (261.7 examples/sec; 0.489 sec/batch)
2018-03-22 17:36:51.559149: step 43220, loss = 0.61 (226.2 examples/sec; 0.566 sec/batch)
2018-03-22 17:36:56.207497: step 43230, loss = 0.71 (275.4 examples/sec; 0.465 sec/batch)
2018-03-22 17:37:01.323487: step 43240, loss = 0.72 (250.2 examples/sec; 0.512 sec/batch)
2018-03-22 17:37:06.521454: step 43250, loss = 0.61 (246.3 examples/sec; 0.520 sec/batch)
2018-03-22 17:37:11.690644: step 43260, loss = 0.83 (247.6 examples/sec; 0.517 sec/batch)
2018-03-22 17:37:16.440757: step 43270, loss = 0.78 (269.5 examples/sec; 0.475 sec/batch)
2018-03-22 17:37:21.181419: step 43280, loss = 0.78 (270.0 examples/sec; 0.474 sec/batch)
2018-03-22 17:37:26.136186: step 43290, loss = 0.74 (258.3 examples/sec; 0.495 sec/batch)
2018-03-22 17:37:31.341866: step 43300, loss = 0.67 (245.9 examples/sec; 0.521 sec/batch)
2018-03-22 17:37:35.713350: step 43310, loss = 0.86 (292.8 examples/sec; 0.437 sec/batch)
2018-03-22 17:37:40.130418: step 43320, loss = 0.66 (289.8 examples/sec; 0.442 sec/batch)
2018-03-22 17:37:44.249252: step 43330, loss = 0.81 (310.8 examples/sec; 0.412 sec/batch)
2018-03-22 17:37:49.401489: step 43340, loss = 0.86 (248.4 examples/sec; 0.515 sec/batch)
2018-03-22 17:37:54.654391: step 43350, loss = 0.62 (243.7 examples/sec; 0.525 sec/batch)
2018-03-22 17:37:59.464501: step 43360, loss = 0.71 (266.1 examples/sec; 0.481 sec/batch)
2018-03-22 17:38:04.649368: step 43370, loss = 0.80 (246.9 examples/sec; 0.518 sec/batch)
2018-03-22 17:38:10.022349: step 43380, loss = 0.69 (238.2 examples/sec; 0.537 sec/batch)
2018-03-22 17:38:14.652503: step 43390, loss = 0.89 (276.4 examples/sec; 0.463 sec/batch)
2018-03-22 17:38:19.518466: step 43400, loss = 0.82 (263.1 examples/sec; 0.487 sec/batch)
2018-03-22 17:38:24.129376: step 43410, loss = 0.75 (277.6 examples/sec; 0.461 sec/batch)
2018-03-22 17:38:29.586381: step 43420, loss = 0.66 (234.6 examples/sec; 0.546 sec/batch)
2018-03-22 17:38:34.354314: step 43430, loss = 0.79 (268.5 examples/sec; 0.477 sec/batch)
2018-03-22 17:38:39.419337: step 43440, loss = 0.75 (252.7 examples/sec; 0.507 sec/batch)
2018-03-22 17:38:44.457207: step 43450, loss = 0.69 (254.1 examples/sec; 0.504 sec/batch)
2018-03-22 17:38:49.596528: step 43460, loss = 0.67 (249.1 examples/sec; 0.514 sec/batch)
2018-03-22 17:38:54.717396: step 43470, loss = 0.67 (250.0 examples/sec; 0.512 sec/batch)
2018-03-22 17:39:00.207398: step 43480, loss = 0.68 (233.2 examples/sec; 0.549 sec/batch)
2018-03-22 17:39:05.458347: step 43490, loss = 0.70 (243.8 examples/sec; 0.525 sec/batch)
2018-03-22 17:39:10.519831: step 43500, loss = 0.76 (252.9 examples/sec; 0.506 sec/batch)
2018-03-22 17:39:15.208782: step 43510, loss = 0.83 (273.0 examples/sec; 0.469 sec/batch)
2018-03-22 17:39:20.702420: step 43520, loss = 0.76 (233.0 examples/sec; 0.549 sec/batch)
2018-03-22 17:39:25.456202: step 43530, loss = 0.74 (269.3 examples/sec; 0.475 sec/batch)
2018-03-22 17:39:30.582399: step 43540, loss = 0.69 (249.7 examples/sec; 0.513 sec/batch)
2018-03-22 17:39:34.976178: step 43550, loss = 0.88 (291.3 examples/sec; 0.439 sec/batch)
2018-03-22 17:39:40.017613: step 43560, loss = 0.86 (253.9 examples/sec; 0.504 sec/batch)
2018-03-22 17:39:44.962373: step 43570, loss = 0.70 (258.9 examples/sec; 0.494 sec/batch)
2018-03-22 17:39:50.110331: step 43580, loss = 0.61 (248.6 examples/sec; 0.515 sec/batch)
2018-03-22 17:39:55.128178: step 43590, loss = 0.77 (255.1 examples/sec; 0.502 sec/batch)
2018-03-22 17:40:00.692766: step 43600, loss = 0.73 (230.0 examples/sec; 0.556 sec/batch)
2018-03-22 17:40:05.490813: step 43610, loss = 0.75 (266.8 examples/sec; 0.480 sec/batch)
2018-03-22 17:40:10.329632: step 43620, loss = 0.73 (264.5 examples/sec; 0.484 sec/batch)
2018-03-22 17:40:14.929184: step 43630, loss = 0.90 (278.3 examples/sec; 0.460 sec/batch)
2018-03-22 17:40:20.569386: step 43640, loss = 0.81 (226.9 examples/sec; 0.564 sec/batch)
2018-03-22 17:40:25.997684: step 43650, loss = 0.72 (235.8 examples/sec; 0.543 sec/batch)
2018-03-22 17:40:30.676405: step 43660, loss = 0.83 (273.6 examples/sec; 0.468 sec/batch)
2018-03-22 17:40:35.660261: step 43670, loss = 0.80 (256.8 examples/sec; 0.498 sec/batch)
2018-03-22 17:40:40.838484: step 43680, loss = 0.67 (247.2 examples/sec; 0.518 sec/batch)
2018-03-22 17:40:45.755375: step 43690, loss = 0.64 (260.3 examples/sec; 0.492 sec/batch)
2018-03-22 17:40:51.319411: step 43700, loss = 0.81 (230.0 examples/sec; 0.556 sec/batch)
2018-03-22 17:40:55.687829: step 43710, loss = 0.74 (293.0 examples/sec; 0.437 sec/batch)
2018-03-22 17:41:00.783167: step 43720, loss = 0.78 (251.2 examples/sec; 0.510 sec/batch)
2018-03-22 17:41:05.928416: step 43730, loss = 0.73 (248.8 examples/sec; 0.515 sec/batch)
2018-03-22 17:41:10.788408: step 43740, loss = 0.62 (263.4 examples/sec; 0.486 sec/batch)
2018-03-22 17:41:15.623466: step 43750, loss = 0.79 (264.7 examples/sec; 0.484 sec/batch)
2018-03-22 17:41:21.517374: step 43760, loss = 0.91 (217.2 examples/sec; 0.589 sec/batch)
2018-03-22 17:41:26.308465: step 43770, loss = 0.73 (267.2 examples/sec; 0.479 sec/batch)
2018-03-22 17:41:30.926451: step 43780, loss = 0.82 (277.2 examples/sec; 0.462 sec/batch)
2018-03-22 17:41:35.343534: step 43790, loss = 0.72 (289.8 examples/sec; 0.442 sec/batch)
2018-03-22 17:41:39.700403: step 43800, loss = 0.84 (293.8 examples/sec; 0.436 sec/batch)
2018-03-22 17:41:44.764283: step 43810, loss = 0.71 (252.8 examples/sec; 0.506 sec/batch)
2018-03-22 17:41:49.909336: step 43820, loss = 0.82 (248.8 examples/sec; 0.515 sec/batch)
2018-03-22 17:41:55.443571: step 43830, loss = 0.77 (231.3 examples/sec; 0.553 sec/batch)
2018-03-22 17:42:00.566339: step 43840, loss = 0.76 (249.9 examples/sec; 0.512 sec/batch)
2018-03-22 17:42:05.276474: step 43850, loss = 0.77 (271.8 examples/sec; 0.471 sec/batch)
2018-03-22 17:42:10.488366: step 43860, loss = 0.76 (245.6 examples/sec; 0.521 sec/batch)
2018-03-22 17:42:15.388408: step 43870, loss = 0.70 (261.2 examples/sec; 0.490 sec/batch)
2018-03-22 17:42:20.904424: step 43880, loss = 0.86 (232.1 examples/sec; 0.552 sec/batch)
2018-03-22 17:42:25.491422: step 43890, loss = 0.81 (279.0 examples/sec; 0.459 sec/batch)
2018-03-22 17:42:31.408376: step 43900, loss = 0.78 (216.3 examples/sec; 0.592 sec/batch)
2018-03-22 17:42:36.146566: step 43910, loss = 0.75 (270.1 examples/sec; 0.474 sec/batch)
2018-03-22 17:42:41.432000: step 43920, loss = 0.77 (242.2 examples/sec; 0.529 sec/batch)
2018-03-22 17:42:46.219383: step 43930, loss = 0.94 (267.4 examples/sec; 0.479 sec/batch)
2018-03-22 17:42:51.976373: step 43940, loss = 0.65 (222.3 examples/sec; 0.576 sec/batch)
2018-03-22 17:42:57.140238: step 43950, loss = 0.88 (247.9 examples/sec; 0.516 sec/batch)
2018-03-22 17:43:01.859028: step 43960, loss = 0.90 (271.3 examples/sec; 0.472 sec/batch)
2018-03-22 17:43:06.708385: step 43970, loss = 0.66 (264.0 examples/sec; 0.485 sec/batch)
2018-03-22 17:43:12.005442: step 43980, loss = 0.73 (241.6 examples/sec; 0.530 sec/batch)
2018-03-22 17:43:17.647291: step 43990, loss = 0.68 (226.9 examples/sec; 0.564 sec/batch)
2018-03-22 17:43:23.348096: step 44000, loss = 0.85 (224.5 examples/sec; 0.570 sec/batch)
2018-03-22 17:43:28.036052: step 44010, loss = 0.79 (273.0 examples/sec; 0.469 sec/batch)
2018-03-22 17:43:33.222486: step 44020, loss = 0.69 (246.8 examples/sec; 0.519 sec/batch)
2018-03-22 17:43:38.390429: step 44030, loss = 0.92 (247.7 examples/sec; 0.517 sec/batch)
2018-03-22 17:43:43.569484: step 44040, loss = 0.86 (247.1 examples/sec; 0.518 sec/batch)
2018-03-22 17:43:48.849427: step 44050, loss = 0.78 (242.4 examples/sec; 0.528 sec/batch)
2018-03-22 17:43:54.322851: step 44060, loss = 0.68 (233.9 examples/sec; 0.547 sec/batch)
2018-03-22 17:43:59.962364: step 44070, loss = 0.73 (227.0 examples/sec; 0.564 sec/batch)
2018-03-22 17:44:05.381000: step 44080, loss = 0.77 (236.2 examples/sec; 0.542 sec/batch)
2018-03-22 17:44:10.544360: step 44090, loss = 0.68 (247.9 examples/sec; 0.516 sec/batch)
2018-03-22 17:44:15.749328: step 44100, loss = 0.88 (245.9 examples/sec; 0.520 sec/batch)
2018-03-22 17:44:21.101332: step 44110, loss = 0.72 (239.2 examples/sec; 0.535 sec/batch)
2018-03-22 17:44:26.247353: step 44120, loss = 0.79 (248.7 examples/sec; 0.515 sec/batch)
2018-03-22 17:44:31.538644: step 44130, loss = 0.88 (241.9 examples/sec; 0.529 sec/batch)
2018-03-22 17:44:36.431049: step 44140, loss = 0.82 (261.6 examples/sec; 0.489 sec/batch)
2018-03-22 17:44:41.901468: step 44150, loss = 0.73 (234.0 examples/sec; 0.547 sec/batch)
2018-03-22 17:44:46.654387: step 44160, loss = 0.86 (269.3 examples/sec; 0.475 sec/batch)
2018-03-22 17:44:51.596484: step 44170, loss = 0.82 (259.0 examples/sec; 0.494 sec/batch)
2018-03-22 17:44:56.118957: step 44180, loss = 0.93 (283.0 examples/sec; 0.452 sec/batch)
2018-03-22 17:45:01.388545: step 44190, loss = 0.75 (242.9 examples/sec; 0.527 sec/batch)
2018-03-22 17:45:06.444434: step 44200, loss = 0.79 (253.2 examples/sec; 0.506 sec/batch)
2018-03-22 17:45:11.201437: step 44210, loss = 0.78 (269.1 examples/sec; 0.476 sec/batch)
2018-03-22 17:45:15.578632: step 44220, loss = 0.83 (292.4 examples/sec; 0.438 sec/batch)
2018-03-22 17:45:20.078875: step 44230, loss = 0.62 (284.4 examples/sec; 0.450 sec/batch)
2018-03-22 17:45:24.181552: step 44240, loss = 0.85 (312.0 examples/sec; 0.410 sec/batch)
2018-03-22 17:45:28.210329: step 44250, loss = 0.89 (317.7 examples/sec; 0.403 sec/batch)
2018-03-22 17:45:33.062440: step 44260, loss = 0.89 (263.8 examples/sec; 0.485 sec/batch)
2018-03-22 17:45:37.702355: step 44270, loss = 0.75 (275.9 examples/sec; 0.464 sec/batch)
2018-03-22 17:45:42.633388: step 44280, loss = 0.71 (259.6 examples/sec; 0.493 sec/batch)
2018-03-22 17:45:47.862431: step 44290, loss = 0.74 (244.8 examples/sec; 0.523 sec/batch)
2018-03-22 17:45:53.145801: step 44300, loss = 0.80 (242.3 examples/sec; 0.528 sec/batch)
2018-03-22 17:45:57.545072: step 44310, loss = 0.66 (291.0 examples/sec; 0.440 sec/batch)
2018-03-22 17:46:02.639043: step 44320, loss = 0.77 (251.3 examples/sec; 0.509 sec/batch)
2018-03-22 17:46:07.734406: step 44330, loss = 0.82 (251.2 examples/sec; 0.510 sec/batch)
2018-03-22 17:46:12.564205: step 44340, loss = 0.75 (265.0 examples/sec; 0.483 sec/batch)
2018-03-22 17:46:17.390512: step 44350, loss = 0.73 (265.2 examples/sec; 0.483 sec/batch)
2018-03-22 17:46:22.111396: step 44360, loss = 0.58 (271.1 examples/sec; 0.472 sec/batch)
2018-03-22 17:46:27.413312: step 44370, loss = 0.63 (241.4 examples/sec; 0.530 sec/batch)
2018-03-22 17:46:32.837425: step 44380, loss = 0.76 (236.0 examples/sec; 0.542 sec/batch)
2018-03-22 17:46:38.174392: step 44390, loss = 0.83 (239.8 examples/sec; 0.534 sec/batch)
2018-03-22 17:46:44.100121: step 44400, loss = 0.80 (216.0 examples/sec; 0.593 sec/batch)
2018-03-22 17:46:48.804366: step 44410, loss = 0.68 (272.1 examples/sec; 0.470 sec/batch)
2018-03-22 17:46:53.902434: step 44420, loss = 0.68 (251.1 examples/sec; 0.510 sec/batch)
2018-03-22 17:46:58.480176: step 44430, loss = 0.80 (279.6 examples/sec; 0.458 sec/batch)
2018-03-22 17:47:03.637426: step 44440, loss = 0.74 (248.2 examples/sec; 0.516 sec/batch)
2018-03-22 17:47:08.816435: step 44450, loss = 0.69 (247.2 examples/sec; 0.518 sec/batch)
2018-03-22 17:47:13.722996: step 44460, loss = 0.69 (260.9 examples/sec; 0.491 sec/batch)
2018-03-22 17:47:19.201413: step 44470, loss = 0.74 (233.6 examples/sec; 0.548 sec/batch)
2018-03-22 17:47:24.471476: step 44480, loss = 0.69 (242.9 examples/sec; 0.527 sec/batch)
2018-03-22 17:47:29.971605: step 44490, loss = 0.74 (232.7 examples/sec; 0.550 sec/batch)
2018-03-22 17:47:35.093731: step 44500, loss = 0.72 (249.9 examples/sec; 0.512 sec/batch)
2018-03-22 17:47:40.169068: step 44510, loss = 0.73 (252.2 examples/sec; 0.508 sec/batch)
2018-03-22 17:47:45.334482: step 44520, loss = 0.70 (247.8 examples/sec; 0.517 sec/batch)
2018-03-22 17:47:50.151420: step 44530, loss = 0.70 (265.7 examples/sec; 0.482 sec/batch)
2018-03-22 17:47:55.393351: step 44540, loss = 0.81 (244.2 examples/sec; 0.524 sec/batch)
2018-03-22 17:48:00.760382: step 44550, loss = 0.76 (238.5 examples/sec; 0.537 sec/batch)
2018-03-22 17:48:05.767398: step 44560, loss = 0.75 (255.6 examples/sec; 0.501 sec/batch)
2018-03-22 17:48:10.504479: step 44570, loss = 0.80 (270.2 examples/sec; 0.474 sec/batch)
2018-03-22 17:48:15.584261: step 44580, loss = 0.72 (252.0 examples/sec; 0.508 sec/batch)
2018-03-22 17:48:20.671361: step 44590, loss = 0.78 (251.6 examples/sec; 0.509 sec/batch)
2018-03-22 17:48:26.073614: step 44600, loss = 0.78 (236.9 examples/sec; 0.540 sec/batch)
2018-03-22 17:48:31.831487: step 44610, loss = 0.89 (222.3 examples/sec; 0.576 sec/batch)
2018-03-22 17:48:36.837884: step 44620, loss = 0.82 (255.7 examples/sec; 0.501 sec/batch)
2018-03-22 17:48:42.203744: step 44630, loss = 0.83 (238.5 examples/sec; 0.537 sec/batch)
2018-03-22 17:48:47.135381: step 44640, loss = 0.78 (259.5 examples/sec; 0.493 sec/batch)
2018-03-22 17:48:52.390469: step 44650, loss = 0.74 (243.6 examples/sec; 0.526 sec/batch)
2018-03-22 17:48:57.492435: step 44660, loss = 0.79 (250.9 examples/sec; 0.510 sec/batch)
2018-03-22 17:49:02.163236: step 44670, loss = 0.74 (274.0 examples/sec; 0.467 sec/batch)
2018-03-22 17:49:06.659068: step 44680, loss = 0.67 (284.7 examples/sec; 0.450 sec/batch)
2018-03-22 17:49:11.584188: step 44690, loss = 0.70 (259.9 examples/sec; 0.493 sec/batch)
2018-03-22 17:49:16.531832: step 44700, loss = 0.86 (258.7 examples/sec; 0.495 sec/batch)
2018-03-22 17:49:21.595404: step 44710, loss = 0.77 (252.8 examples/sec; 0.506 sec/batch)
2018-03-22 17:49:26.820210: step 44720, loss = 0.76 (245.0 examples/sec; 0.522 sec/batch)
2018-03-22 17:49:32.324324: step 44730, loss = 0.68 (232.6 examples/sec; 0.550 sec/batch)
2018-03-22 17:49:37.079099: step 44740, loss = 0.75 (269.2 examples/sec; 0.475 sec/batch)
2018-03-22 17:49:41.902370: step 44750, loss = 0.78 (265.4 examples/sec; 0.482 sec/batch)
2018-03-22 17:49:46.762871: step 44760, loss = 0.67 (263.3 examples/sec; 0.486 sec/batch)
2018-03-22 17:49:52.339497: step 44770, loss = 0.79 (229.5 examples/sec; 0.558 sec/batch)
2018-03-22 17:49:57.549371: step 44780, loss = 0.73 (245.7 examples/sec; 0.521 sec/batch)
2018-03-22 17:50:02.595773: step 44790, loss = 0.69 (253.6 examples/sec; 0.505 sec/batch)
2018-03-22 17:50:07.535768: step 44800, loss = 0.79 (259.1 examples/sec; 0.494 sec/batch)
2018-03-22 17:50:12.958435: step 44810, loss = 0.85 (236.0 examples/sec; 0.542 sec/batch)
2018-03-22 17:50:18.002424: step 44820, loss = 0.71 (253.8 examples/sec; 0.504 sec/batch)
2018-03-22 17:50:23.330438: step 44830, loss = 0.85 (240.2 examples/sec; 0.533 sec/batch)
2018-03-22 17:50:28.545089: step 44840, loss = 0.79 (245.5 examples/sec; 0.521 sec/batch)
2018-03-22 17:50:33.543679: step 44850, loss = 0.76 (256.1 examples/sec; 0.500 sec/batch)
2018-03-22 17:50:38.250386: step 44860, loss = 0.76 (272.0 examples/sec; 0.471 sec/batch)
2018-03-22 17:50:43.539418: step 44870, loss = 0.62 (242.0 examples/sec; 0.529 sec/batch)
2018-03-22 17:50:48.448406: step 44880, loss = 0.70 (260.7 examples/sec; 0.491 sec/batch)
2018-03-22 17:50:53.232399: step 44890, loss = 0.67 (267.6 examples/sec; 0.478 sec/batch)
2018-03-22 17:50:58.591412: step 44900, loss = 0.70 (238.9 examples/sec; 0.536 sec/batch)
2018-03-22 17:51:03.989343: step 44910, loss = 0.94 (237.1 examples/sec; 0.540 sec/batch)
2018-03-22 17:51:08.586076: step 44920, loss = 0.76 (278.5 examples/sec; 0.460 sec/batch)
2018-03-22 17:51:13.544342: step 44930, loss = 0.77 (258.2 examples/sec; 0.496 sec/batch)
2018-03-22 17:51:19.432461: step 44940, loss = 0.78 (217.4 examples/sec; 0.589 sec/batch)
2018-03-22 17:51:24.756527: step 44950, loss = 0.68 (240.4 examples/sec; 0.532 sec/batch)
2018-03-22 17:51:29.605414: step 44960, loss = 0.70 (264.0 examples/sec; 0.485 sec/batch)
2018-03-22 17:51:34.644363: step 44970, loss = 0.65 (254.0 examples/sec; 0.504 sec/batch)
2018-03-22 17:51:40.397410: step 44980, loss = 0.69 (222.5 examples/sec; 0.575 sec/batch)
2018-03-22 17:51:45.848473: step 44990, loss = 0.91 (234.8 examples/sec; 0.545 sec/batch)
2018-03-22 17:51:50.928707: step 45000, loss = 0.83 (252.0 examples/sec; 0.508 sec/batch)
2018-03-22 17:51:55.858846: step 45010, loss = 0.70 (259.6 examples/sec; 0.493 sec/batch)
2018-03-22 17:52:01.089153: step 45020, loss = 0.90 (244.7 examples/sec; 0.523 sec/batch)
2018-03-22 17:52:06.225570: step 45030, loss = 0.65 (249.2 examples/sec; 0.514 sec/batch)
2018-03-22 17:52:11.533312: step 45040, loss = 0.76 (241.2 examples/sec; 0.531 sec/batch)
2018-03-22 17:52:16.336410: step 45050, loss = 0.66 (266.5 examples/sec; 0.480 sec/batch)
2018-03-22 17:52:21.891415: step 45060, loss = 0.78 (230.4 examples/sec; 0.556 sec/batch)
2018-03-22 17:52:26.670407: step 45070, loss = 0.82 (267.8 examples/sec; 0.478 sec/batch)
2018-03-22 17:52:31.831003: step 45080, loss = 0.89 (248.0 examples/sec; 0.516 sec/batch)
2018-03-22 17:52:37.008665: step 45090, loss = 0.93 (247.2 examples/sec; 0.518 sec/batch)
2018-03-22 17:52:42.376376: step 45100, loss = 0.73 (238.5 examples/sec; 0.537 sec/batch)
2018-03-22 17:52:47.262833: step 45110, loss = 0.75 (261.9 examples/sec; 0.489 sec/batch)
2018-03-22 17:52:51.582448: step 45120, loss = 0.72 (296.3 examples/sec; 0.432 sec/batch)
2018-03-22 17:52:55.978400: step 45130, loss = 0.86 (291.2 examples/sec; 0.440 sec/batch)
2018-03-22 17:53:00.530449: step 45140, loss = 0.65 (281.2 examples/sec; 0.455 sec/batch)
2018-03-22 17:53:05.640476: step 45150, loss = 0.75 (250.5 examples/sec; 0.511 sec/batch)
2018-03-22 17:53:11.042999: step 45160, loss = 0.88 (236.9 examples/sec; 0.540 sec/batch)
2018-03-22 17:53:15.985345: step 45170, loss = 0.84 (259.0 examples/sec; 0.494 sec/batch)
2018-03-22 17:53:20.956494: step 45180, loss = 0.71 (257.5 examples/sec; 0.497 sec/batch)
2018-03-22 17:53:25.204046: step 45190, loss = 0.84 (301.3 examples/sec; 0.425 sec/batch)
2018-03-22 17:53:30.466993: step 45200, loss = 0.64 (243.2 examples/sec; 0.526 sec/batch)
2018-03-22 17:53:35.270459: step 45210, loss = 0.74 (266.5 examples/sec; 0.480 sec/batch)
2018-03-22 17:53:40.376323: step 45220, loss = 0.81 (250.7 examples/sec; 0.511 sec/batch)
2018-03-22 17:53:45.645406: step 45230, loss = 0.70 (242.9 examples/sec; 0.527 sec/batch)
2018-03-22 17:53:50.750682: step 45240, loss = 0.70 (250.7 examples/sec; 0.511 sec/batch)
2018-03-22 17:53:55.875280: step 45250, loss = 0.82 (249.8 examples/sec; 0.512 sec/batch)
2018-03-22 17:54:00.705486: step 45260, loss = 0.77 (265.0 examples/sec; 0.483 sec/batch)
2018-03-22 17:54:05.488461: step 45270, loss = 0.77 (267.6 examples/sec; 0.478 sec/batch)
2018-03-22 17:54:10.607434: step 45280, loss = 0.84 (250.1 examples/sec; 0.512 sec/batch)
2018-03-22 17:54:15.660362: step 45290, loss = 0.69 (253.3 examples/sec; 0.505 sec/batch)
2018-03-22 17:54:20.871930: step 45300, loss = 0.68 (245.6 examples/sec; 0.521 sec/batch)
2018-03-22 17:54:25.594122: step 45310, loss = 0.70 (271.1 examples/sec; 0.472 sec/batch)
2018-03-22 17:54:31.268928: step 45320, loss = 0.75 (225.6 examples/sec; 0.567 sec/batch)
2018-03-22 17:54:36.316746: step 45330, loss = 0.76 (253.6 examples/sec; 0.505 sec/batch)
2018-03-22 17:54:40.945116: step 45340, loss = 1.01 (276.5 examples/sec; 0.463 sec/batch)
2018-03-22 17:54:45.822366: step 45350, loss = 0.74 (262.4 examples/sec; 0.488 sec/batch)
2018-03-22 17:54:51.089406: step 45360, loss = 0.77 (243.0 examples/sec; 0.527 sec/batch)
2018-03-22 17:54:56.213420: step 45370, loss = 0.88 (249.8 examples/sec; 0.512 sec/batch)
2018-03-22 17:55:01.363868: step 45380, loss = 0.74 (248.5 examples/sec; 0.515 sec/batch)
2018-03-22 17:55:06.266339: step 45390, loss = 0.75 (261.1 examples/sec; 0.490 sec/batch)
2018-03-22 17:55:11.717365: step 45400, loss = 0.85 (234.8 examples/sec; 0.545 sec/batch)
2018-03-22 17:55:16.620098: step 45410, loss = 0.77 (261.1 examples/sec; 0.490 sec/batch)
2018-03-22 17:55:21.512426: step 45420, loss = 0.72 (261.6 examples/sec; 0.489 sec/batch)
2018-03-22 17:55:26.304443: step 45430, loss = 0.79 (267.1 examples/sec; 0.479 sec/batch)
2018-03-22 17:55:31.946787: step 45440, loss = 0.82 (226.9 examples/sec; 0.564 sec/batch)
2018-03-22 17:55:36.747806: step 45450, loss = 0.73 (266.6 examples/sec; 0.480 sec/batch)
2018-03-22 17:55:41.156215: step 45460, loss = 0.60 (290.4 examples/sec; 0.441 sec/batch)
2018-03-22 17:55:45.783387: step 45470, loss = 0.82 (276.6 examples/sec; 0.463 sec/batch)
2018-03-22 17:55:51.190403: step 45480, loss = 0.97 (236.7 examples/sec; 0.541 sec/batch)
2018-03-22 17:55:56.382614: step 45490, loss = 0.80 (246.5 examples/sec; 0.519 sec/batch)
2018-03-22 17:56:01.228050: step 45500, loss = 0.83 (264.2 examples/sec; 0.485 sec/batch)
2018-03-22 17:56:05.909471: step 45510, loss = 0.90 (273.4 examples/sec; 0.468 sec/batch)
2018-03-22 17:56:10.679196: step 45520, loss = 0.66 (268.4 examples/sec; 0.477 sec/batch)
2018-03-22 17:56:15.863405: step 45530, loss = 0.79 (246.9 examples/sec; 0.518 sec/batch)
2018-03-22 17:56:21.404851: step 45540, loss = 0.67 (231.0 examples/sec; 0.554 sec/batch)
2018-03-22 17:56:26.085314: step 45550, loss = 0.85 (273.5 examples/sec; 0.468 sec/batch)
2018-03-22 17:56:31.274575: step 45560, loss = 0.98 (246.7 examples/sec; 0.519 sec/batch)
2018-03-22 17:56:36.395912: step 45570, loss = 0.62 (249.9 examples/sec; 0.512 sec/batch)
2018-03-22 17:56:40.977311: step 45580, loss = 0.79 (279.4 examples/sec; 0.458 sec/batch)
2018-03-22 17:56:44.990535: step 45590, loss = 0.67 (318.9 examples/sec; 0.401 sec/batch)
2018-03-22 17:56:50.779744: step 45600, loss = 0.78 (221.1 examples/sec; 0.579 sec/batch)
2018-03-22 17:56:56.137609: step 45610, loss = 0.79 (238.9 examples/sec; 0.536 sec/batch)
2018-03-22 17:57:01.397467: step 45620, loss = 0.89 (243.4 examples/sec; 0.526 sec/batch)
2018-03-22 17:57:06.229443: step 45630, loss = 0.74 (264.9 examples/sec; 0.483 sec/batch)
2018-03-22 17:57:11.068745: step 45640, loss = 0.65 (264.5 examples/sec; 0.484 sec/batch)
2018-03-22 17:57:15.965260: step 45650, loss = 0.82 (261.4 examples/sec; 0.490 sec/batch)
2018-03-22 17:57:20.964969: step 45660, loss = 0.85 (256.0 examples/sec; 0.500 sec/batch)
2018-03-22 17:57:26.406526: step 45670, loss = 0.68 (235.2 examples/sec; 0.544 sec/batch)
2018-03-22 17:57:31.522459: step 45680, loss = 0.82 (250.2 examples/sec; 0.512 sec/batch)
2018-03-22 17:57:36.902334: step 45690, loss = 0.76 (237.9 examples/sec; 0.538 sec/batch)
2018-03-22 17:57:42.216040: step 45700, loss = 0.69 (240.9 examples/sec; 0.531 sec/batch)
2018-03-22 17:57:46.964429: step 45710, loss = 0.85 (269.6 examples/sec; 0.475 sec/batch)
2018-03-22 17:57:51.845433: step 45720, loss = 0.82 (262.2 examples/sec; 0.488 sec/batch)
2018-03-22 17:57:57.181382: step 45730, loss = 0.66 (239.9 examples/sec; 0.534 sec/batch)
2018-03-22 17:58:02.410393: step 45740, loss = 0.58 (244.8 examples/sec; 0.523 sec/batch)
2018-03-22 17:58:07.134212: step 45750, loss = 0.68 (271.0 examples/sec; 0.472 sec/batch)
2018-03-22 17:58:11.989746: step 45760, loss = 0.72 (263.6 examples/sec; 0.486 sec/batch)
2018-03-22 17:58:17.163434: step 45770, loss = 0.73 (247.4 examples/sec; 0.517 sec/batch)
2018-03-22 17:58:22.559340: step 45780, loss = 0.87 (237.2 examples/sec; 0.540 sec/batch)
2018-03-22 17:58:27.611409: step 45790, loss = 0.92 (253.4 examples/sec; 0.505 sec/batch)
2018-03-22 17:58:33.079652: step 45800, loss = 0.81 (234.1 examples/sec; 0.547 sec/batch)
2018-03-22 17:58:37.778400: step 45810, loss = 1.11 (272.4 examples/sec; 0.470 sec/batch)
2018-03-22 17:58:43.754778: step 45820, loss = 0.88 (214.2 examples/sec; 0.598 sec/batch)
2018-03-22 17:58:48.540002: step 45830, loss = 0.61 (267.5 examples/sec; 0.479 sec/batch)
2018-03-22 17:58:53.937380: step 45840, loss = 0.76 (237.2 examples/sec; 0.540 sec/batch)
2018-03-22 17:58:58.997574: step 45850, loss = 0.85 (253.0 examples/sec; 0.506 sec/batch)
2018-03-22 17:59:04.357911: step 45860, loss = 0.79 (238.8 examples/sec; 0.536 sec/batch)
2018-03-22 17:59:09.616619: step 45870, loss = 0.71 (243.4 examples/sec; 0.526 sec/batch)
2018-03-22 17:59:14.322345: step 45880, loss = 0.81 (272.0 examples/sec; 0.471 sec/batch)
2018-03-22 17:59:19.431419: step 45890, loss = 0.64 (250.5 examples/sec; 0.511 sec/batch)
2018-03-22 17:59:25.145492: step 45900, loss = 0.64 (224.0 examples/sec; 0.571 sec/batch)
2018-03-22 17:59:29.818869: step 45910, loss = 0.79 (273.9 examples/sec; 0.467 sec/batch)
2018-03-22 17:59:34.684390: step 45920, loss = 0.87 (263.1 examples/sec; 0.487 sec/batch)
2018-03-22 17:59:40.075563: step 45930, loss = 0.73 (237.4 examples/sec; 0.539 sec/batch)
2018-03-22 17:59:45.653398: step 45940, loss = 0.90 (229.5 examples/sec; 0.558 sec/batch)
2018-03-22 17:59:51.127635: step 45950, loss = 0.63 (233.8 examples/sec; 0.547 sec/batch)
2018-03-22 17:59:56.449864: step 45960, loss = 0.69 (240.5 examples/sec; 0.532 sec/batch)
2018-03-22 18:00:01.439445: step 45970, loss = 0.98 (256.5 examples/sec; 0.499 sec/batch)
2018-03-22 18:00:06.483402: step 45980, loss = 0.57 (253.8 examples/sec; 0.504 sec/batch)
2018-03-22 18:00:11.651429: step 45990, loss = 0.68 (247.7 examples/sec; 0.517 sec/batch)
2018-03-22 18:00:16.310550: step 46000, loss = 0.83 (274.7 examples/sec; 0.466 sec/batch)
2018-03-22 18:00:21.167411: step 46010, loss = 0.65 (263.5 examples/sec; 0.486 sec/batch)
2018-03-22 18:00:25.951276: step 46020, loss = 0.69 (267.6 examples/sec; 0.478 sec/batch)
2018-03-22 18:00:30.264556: step 46030, loss = 0.64 (296.8 examples/sec; 0.431 sec/batch)
2018-03-22 18:00:34.616809: step 46040, loss = 0.81 (294.1 examples/sec; 0.435 sec/batch)
2018-03-22 18:00:39.821595: step 46050, loss = 0.76 (245.9 examples/sec; 0.520 sec/batch)
2018-03-22 18:00:45.106878: step 46060, loss = 0.90 (242.2 examples/sec; 0.529 sec/batch)
2018-03-22 18:00:50.225510: step 46070, loss = 0.56 (250.1 examples/sec; 0.512 sec/batch)
2018-03-22 18:00:54.864413: step 46080, loss = 0.61 (275.9 examples/sec; 0.464 sec/batch)
2018-03-22 18:01:00.390534: step 46090, loss = 0.72 (231.6 examples/sec; 0.553 sec/batch)
2018-03-22 18:01:05.312292: step 46100, loss = 0.87 (260.1 examples/sec; 0.492 sec/batch)
2018-03-22 18:01:10.194898: step 46110, loss = 0.63 (262.2 examples/sec; 0.488 sec/batch)
2018-03-22 18:01:15.247382: step 46120, loss = 0.74 (253.3 examples/sec; 0.505 sec/batch)
2018-03-22 18:01:21.168414: step 46130, loss = 0.78 (216.2 examples/sec; 0.592 sec/batch)
2018-03-22 18:01:26.374253: step 46140, loss = 0.82 (245.9 examples/sec; 0.521 sec/batch)
2018-03-22 18:01:30.724078: step 46150, loss = 0.84 (294.3 examples/sec; 0.435 sec/batch)
2018-03-22 18:01:35.211343: step 46160, loss = 0.67 (285.3 examples/sec; 0.449 sec/batch)
2018-03-22 18:01:40.210768: step 46170, loss = 0.58 (256.0 examples/sec; 0.500 sec/batch)
2018-03-22 18:01:45.291444: step 46180, loss = 0.75 (251.9 examples/sec; 0.508 sec/batch)
2018-03-22 18:01:50.340066: step 46190, loss = 0.87 (253.5 examples/sec; 0.505 sec/batch)
2018-03-22 18:01:55.271418: step 46200, loss = 0.61 (259.6 examples/sec; 0.493 sec/batch)
2018-03-22 18:02:00.793372: step 46210, loss = 0.71 (231.8 examples/sec; 0.552 sec/batch)
2018-03-22 18:02:05.561683: step 46220, loss = 0.96 (268.4 examples/sec; 0.477 sec/batch)
2018-03-22 18:02:10.618465: step 46230, loss = 0.70 (253.1 examples/sec; 0.506 sec/batch)
2018-03-22 18:02:15.259393: step 46240, loss = 0.60 (275.8 examples/sec; 0.464 sec/batch)
2018-03-22 18:02:20.863418: step 46250, loss = 0.77 (228.4 examples/sec; 0.560 sec/batch)
2018-03-22 18:02:26.140188: step 46260, loss = 0.72 (242.6 examples/sec; 0.528 sec/batch)
2018-03-22 18:02:30.962438: step 46270, loss = 0.64 (265.4 examples/sec; 0.482 sec/batch)
2018-03-22 18:02:35.798355: step 46280, loss = 0.66 (264.7 examples/sec; 0.484 sec/batch)
2018-03-22 18:02:41.079708: step 46290, loss = 0.82 (242.4 examples/sec; 0.528 sec/batch)
2018-03-22 18:02:46.031970: step 46300, loss = 0.78 (258.5 examples/sec; 0.495 sec/batch)
2018-03-22 18:02:51.189003: step 46310, loss = 0.73 (248.2 examples/sec; 0.516 sec/batch)
2018-03-22 18:02:56.677970: step 46320, loss = 0.75 (233.2 examples/sec; 0.549 sec/batch)
2018-03-22 18:03:02.127494: step 46330, loss = 0.72 (234.9 examples/sec; 0.545 sec/batch)
2018-03-22 18:03:07.035559: step 46340, loss = 0.78 (260.8 examples/sec; 0.491 sec/batch)
2018-03-22 18:03:12.348530: step 46350, loss = 0.74 (240.9 examples/sec; 0.531 sec/batch)
2018-03-22 18:03:17.680946: step 46360, loss = 0.72 (240.0 examples/sec; 0.533 sec/batch)
2018-03-22 18:03:22.980422: step 46370, loss = 0.84 (241.5 examples/sec; 0.530 sec/batch)
2018-03-22 18:03:27.431061: step 46380, loss = 0.69 (287.6 examples/sec; 0.445 sec/batch)
2018-03-22 18:03:32.150822: step 46390, loss = 0.78 (271.2 examples/sec; 0.472 sec/batch)
2018-03-22 18:03:37.394623: step 46400, loss = 0.80 (244.1 examples/sec; 0.524 sec/batch)
2018-03-22 18:03:43.037868: step 46410, loss = 0.89 (226.8 examples/sec; 0.564 sec/batch)
2018-03-22 18:03:47.945681: step 46420, loss = 0.71 (260.8 examples/sec; 0.491 sec/batch)
2018-03-22 18:03:52.336012: step 46430, loss = 0.83 (291.5 examples/sec; 0.439 sec/batch)
2018-03-22 18:03:57.431480: step 46440, loss = 0.72 (251.2 examples/sec; 0.510 sec/batch)
2018-03-22 18:04:02.715398: step 46450, loss = 0.78 (242.2 examples/sec; 0.528 sec/batch)
2018-03-22 18:04:07.585418: step 46460, loss = 0.82 (262.8 examples/sec; 0.487 sec/batch)
2018-03-22 18:04:12.408868: step 46470, loss = 0.77 (265.4 examples/sec; 0.482 sec/batch)
2018-03-22 18:04:16.948184: step 46480, loss = 0.78 (282.0 examples/sec; 0.454 sec/batch)
2018-03-22 18:04:21.495806: step 46490, loss = 0.69 (281.5 examples/sec; 0.455 sec/batch)
2018-03-22 18:04:25.927143: step 46500, loss = 0.73 (288.9 examples/sec; 0.443 sec/batch)
2018-03-22 18:04:30.772428: step 46510, loss = 0.67 (264.2 examples/sec; 0.485 sec/batch)
2018-03-22 18:04:36.056432: step 46520, loss = 0.81 (242.2 examples/sec; 0.528 sec/batch)
2018-03-22 18:04:41.224011: step 46530, loss = 0.76 (247.7 examples/sec; 0.517 sec/batch)
2018-03-22 18:04:46.045582: step 46540, loss = 0.66 (265.5 examples/sec; 0.482 sec/batch)
2018-03-22 18:04:51.147872: step 46550, loss = 0.80 (250.9 examples/sec; 0.510 sec/batch)
2018-03-22 18:04:56.065282: step 46560, loss = 0.75 (260.3 examples/sec; 0.492 sec/batch)
2018-03-22 18:05:00.788379: step 46570, loss = 0.66 (271.0 examples/sec; 0.472 sec/batch)
2018-03-22 18:05:05.355929: step 46580, loss = 0.81 (280.2 examples/sec; 0.457 sec/batch)
2018-03-22 18:05:11.036109: step 46590, loss = 0.83 (225.3 examples/sec; 0.568 sec/batch)
2018-03-22 18:05:16.421431: step 46600, loss = 0.78 (237.7 examples/sec; 0.539 sec/batch)
2018-03-22 18:05:21.570318: step 46610, loss = 0.67 (248.6 examples/sec; 0.515 sec/batch)
2018-03-22 18:05:26.070514: step 46620, loss = 0.73 (284.4 examples/sec; 0.450 sec/batch)
2018-03-22 18:05:31.540383: step 46630, loss = 0.76 (234.0 examples/sec; 0.547 sec/batch)
2018-03-22 18:05:36.938567: step 46640, loss = 0.73 (237.1 examples/sec; 0.540 sec/batch)
2018-03-22 18:05:42.167888: step 46650, loss = 0.78 (244.8 examples/sec; 0.523 sec/batch)
2018-03-22 18:05:47.306531: step 46660, loss = 0.68 (249.1 examples/sec; 0.514 sec/batch)
2018-03-22 18:05:52.225421: step 46670, loss = 0.68 (260.2 examples/sec; 0.492 sec/batch)
2018-03-22 18:05:57.299992: step 46680, loss = 0.77 (252.2 examples/sec; 0.507 sec/batch)
2018-03-22 18:06:02.136426: step 46690, loss = 0.81 (264.7 examples/sec; 0.484 sec/batch)
2018-03-22 18:06:07.185501: step 46700, loss = 0.79 (253.5 examples/sec; 0.505 sec/batch)
2018-03-22 18:06:12.131228: step 46710, loss = 0.76 (258.8 examples/sec; 0.495 sec/batch)
2018-03-22 18:06:17.117022: step 46720, loss = 0.71 (256.7 examples/sec; 0.499 sec/batch)
2018-03-22 18:06:22.164168: step 46730, loss = 0.84 (253.6 examples/sec; 0.505 sec/batch)
2018-03-22 18:06:26.644919: step 46740, loss = 0.94 (285.7 examples/sec; 0.448 sec/batch)
2018-03-22 18:06:31.835429: step 46750, loss = 0.66 (246.6 examples/sec; 0.519 sec/batch)
2018-03-22 18:06:37.091904: step 46760, loss = 0.80 (243.5 examples/sec; 0.526 sec/batch)
2018-03-22 18:06:42.026418: step 46770, loss = 0.70 (259.4 examples/sec; 0.493 sec/batch)
2018-03-22 18:06:47.084393: step 46780, loss = 0.84 (253.1 examples/sec; 0.506 sec/batch)
2018-03-22 18:06:52.370374: step 46790, loss = 0.78 (242.1 examples/sec; 0.529 sec/batch)
2018-03-22 18:06:58.082656: step 46800, loss = 0.68 (224.1 examples/sec; 0.571 sec/batch)
2018-03-22 18:07:02.857963: step 46810, loss = 0.68 (268.0 examples/sec; 0.478 sec/batch)
2018-03-22 18:07:07.545425: step 46820, loss = 0.80 (273.1 examples/sec; 0.469 sec/batch)
2018-03-22 18:07:12.573800: step 46830, loss = 0.81 (254.6 examples/sec; 0.503 sec/batch)
2018-03-22 18:07:17.792315: step 46840, loss = 0.75 (245.3 examples/sec; 0.522 sec/batch)
2018-03-22 18:07:23.289813: step 46850, loss = 0.62 (232.8 examples/sec; 0.550 sec/batch)
2018-03-22 18:07:28.243757: step 46860, loss = 0.75 (258.4 examples/sec; 0.495 sec/batch)
2018-03-22 18:07:33.035008: step 46870, loss = 0.76 (267.2 examples/sec; 0.479 sec/batch)
2018-03-22 18:07:38.218441: step 46880, loss = 0.65 (246.9 examples/sec; 0.518 sec/batch)
2018-03-22 18:07:43.286649: step 46890, loss = 0.74 (252.6 examples/sec; 0.507 sec/batch)
2018-03-22 18:07:48.739399: step 46900, loss = 0.78 (234.7 examples/sec; 0.545 sec/batch)
2018-03-22 18:07:54.012382: step 46910, loss = 0.57 (242.7 examples/sec; 0.527 sec/batch)
2018-03-22 18:07:58.458429: step 46920, loss = 0.80 (287.9 examples/sec; 0.445 sec/batch)
2018-03-22 18:08:03.434297: step 46930, loss = 0.77 (257.2 examples/sec; 0.498 sec/batch)
2018-03-22 18:08:08.215661: step 46940, loss = 0.67 (267.7 examples/sec; 0.478 sec/batch)
2018-03-22 18:08:13.694351: step 46950, loss = 0.91 (233.6 examples/sec; 0.548 sec/batch)
2018-03-22 18:08:17.962637: step 46960, loss = 0.60 (299.9 examples/sec; 0.427 sec/batch)
2018-03-22 18:08:23.166399: step 46970, loss = 0.70 (246.0 examples/sec; 0.520 sec/batch)
2018-03-22 18:08:28.362420: step 46980, loss = 0.83 (246.3 examples/sec; 0.520 sec/batch)
2018-03-22 18:08:34.021410: step 46990, loss = 0.59 (226.2 examples/sec; 0.566 sec/batch)
2018-03-22 18:08:39.338500: step 47000, loss = 0.86 (240.7 examples/sec; 0.532 sec/batch)
2018-03-22 18:08:43.478037: step 47010, loss = 0.92 (309.2 examples/sec; 0.414 sec/batch)
2018-03-22 18:08:48.944551: step 47020, loss = 0.66 (234.2 examples/sec; 0.547 sec/batch)
2018-03-22 18:08:54.357416: step 47030, loss = 0.85 (236.5 examples/sec; 0.541 sec/batch)
2018-03-22 18:08:59.397420: step 47040, loss = 0.82 (254.0 examples/sec; 0.504 sec/batch)
2018-03-22 18:09:04.209957: step 47050, loss = 0.86 (266.0 examples/sec; 0.481 sec/batch)
2018-03-22 18:09:09.127672: step 47060, loss = 0.70 (260.3 examples/sec; 0.492 sec/batch)
2018-03-22 18:09:13.715350: step 47070, loss = 0.68 (279.0 examples/sec; 0.459 sec/batch)
2018-03-22 18:09:18.870335: step 47080, loss = 0.73 (248.3 examples/sec; 0.515 sec/batch)
2018-03-22 18:09:23.793469: step 47090, loss = 0.77 (260.0 examples/sec; 0.492 sec/batch)
2018-03-22 18:09:29.051051: step 47100, loss = 0.71 (243.5 examples/sec; 0.526 sec/batch)
2018-03-22 18:09:34.681517: step 47110, loss = 0.76 (227.3 examples/sec; 0.563 sec/batch)
2018-03-22 18:09:39.932543: step 47120, loss = 0.83 (243.8 examples/sec; 0.525 sec/batch)
2018-03-22 18:09:44.953480: step 47130, loss = 0.92 (254.9 examples/sec; 0.502 sec/batch)
2018-03-22 18:09:50.600454: step 47140, loss = 0.75 (226.7 examples/sec; 0.565 sec/batch)
2018-03-22 18:09:55.390451: step 47150, loss = 0.73 (267.2 examples/sec; 0.479 sec/batch)
2018-03-22 18:10:00.395518: step 47160, loss = 0.71 (255.7 examples/sec; 0.501 sec/batch)
2018-03-22 18:10:05.335529: step 47170, loss = 0.66 (259.1 examples/sec; 0.494 sec/batch)
2018-03-22 18:10:10.644453: step 47180, loss = 0.86 (241.1 examples/sec; 0.531 sec/batch)
2018-03-22 18:10:15.317460: step 47190, loss = 0.81 (273.9 examples/sec; 0.467 sec/batch)
2018-03-22 18:10:20.666157: step 47200, loss = 0.80 (239.3 examples/sec; 0.535 sec/batch)
2018-03-22 18:10:25.920461: step 47210, loss = 0.74 (243.6 examples/sec; 0.525 sec/batch)
2018-03-22 18:10:30.903418: step 47220, loss = 0.96 (256.9 examples/sec; 0.498 sec/batch)
2018-03-22 18:10:35.619612: step 47230, loss = 0.61 (271.4 examples/sec; 0.472 sec/batch)
2018-03-22 18:10:40.838451: step 47240, loss = 0.81 (245.3 examples/sec; 0.522 sec/batch)
2018-03-22 18:10:46.235369: step 47250, loss = 0.87 (237.2 examples/sec; 0.540 sec/batch)
2018-03-22 18:10:51.199145: step 47260, loss = 0.85 (257.9 examples/sec; 0.496 sec/batch)
2018-03-22 18:10:56.198742: step 47270, loss = 0.85 (256.0 examples/sec; 0.500 sec/batch)
2018-03-22 18:11:01.434845: step 47280, loss = 0.86 (244.5 examples/sec; 0.524 sec/batch)
2018-03-22 18:11:06.214394: step 47290, loss = 0.65 (267.8 examples/sec; 0.478 sec/batch)
2018-03-22 18:11:11.774626: step 47300, loss = 0.71 (230.2 examples/sec; 0.556 sec/batch)
2018-03-22 18:11:16.768848: step 47310, loss = 0.74 (256.3 examples/sec; 0.499 sec/batch)
2018-03-22 18:11:21.787360: step 47320, loss = 0.72 (255.1 examples/sec; 0.502 sec/batch)
2018-03-22 18:11:26.859833: step 47330, loss = 0.81 (252.3 examples/sec; 0.507 sec/batch)
2018-03-22 18:11:31.833381: step 47340, loss = 0.89 (257.4 examples/sec; 0.497 sec/batch)
2018-03-22 18:11:36.558382: step 47350, loss = 0.79 (270.9 examples/sec; 0.473 sec/batch)
2018-03-22 18:11:41.486417: step 47360, loss = 0.73 (259.7 examples/sec; 0.493 sec/batch)
2018-03-22 18:11:46.170374: step 47370, loss = 0.68 (273.3 examples/sec; 0.468 sec/batch)
2018-03-22 18:11:51.075351: step 47380, loss = 0.81 (261.0 examples/sec; 0.490 sec/batch)
2018-03-22 18:11:55.736057: step 47390, loss = 0.87 (274.6 examples/sec; 0.466 sec/batch)
2018-03-22 18:12:00.568514: step 47400, loss = 0.74 (264.9 examples/sec; 0.483 sec/batch)
2018-03-22 18:12:05.375433: step 47410, loss = 0.78 (266.3 examples/sec; 0.481 sec/batch)
2018-03-22 18:12:09.950242: step 47420, loss = 0.74 (279.8 examples/sec; 0.457 sec/batch)
2018-03-22 18:12:14.494316: step 47430, loss = 0.78 (281.7 examples/sec; 0.454 sec/batch)
2018-03-22 18:12:19.514522: step 47440, loss = 0.89 (255.0 examples/sec; 0.502 sec/batch)
2018-03-22 18:12:24.190814: step 47450, loss = 0.65 (273.7 examples/sec; 0.468 sec/batch)
2018-03-22 18:12:29.732381: step 47460, loss = 0.89 (231.0 examples/sec; 0.554 sec/batch)
2018-03-22 18:12:34.363414: step 47470, loss = 0.95 (276.4 examples/sec; 0.463 sec/batch)
2018-03-22 18:12:39.317061: step 47480, loss = 0.74 (258.4 examples/sec; 0.495 sec/batch)
2018-03-22 18:12:43.826427: step 47490, loss = 0.77 (283.9 examples/sec; 0.451 sec/batch)
2018-03-22 18:12:49.167525: step 47500, loss = 0.69 (239.7 examples/sec; 0.534 sec/batch)
2018-03-22 18:12:54.459433: step 47510, loss = 0.85 (241.9 examples/sec; 0.529 sec/batch)
2018-03-22 18:12:59.582437: step 47520, loss = 0.70 (249.9 examples/sec; 0.512 sec/batch)
2018-03-22 18:13:04.264464: step 47530, loss = 0.58 (273.4 examples/sec; 0.468 sec/batch)
2018-03-22 18:13:09.180161: step 47540, loss = 0.81 (260.4 examples/sec; 0.492 sec/batch)
2018-03-22 18:13:14.142381: step 47550, loss = 0.77 (257.9 examples/sec; 0.496 sec/batch)
2018-03-22 18:13:19.490393: step 47560, loss = 0.86 (239.3 examples/sec; 0.535 sec/batch)
2018-03-22 18:13:25.057639: step 47570, loss = 0.80 (229.9 examples/sec; 0.557 sec/batch)
2018-03-22 18:13:30.426405: step 47580, loss = 0.68 (238.4 examples/sec; 0.537 sec/batch)
2018-03-22 18:13:35.112430: step 47590, loss = 0.69 (273.2 examples/sec; 0.469 sec/batch)
2018-03-22 18:13:40.787618: step 47600, loss = 0.61 (225.5 examples/sec; 0.568 sec/batch)
2018-03-22 18:13:45.758386: step 47610, loss = 0.76 (257.5 examples/sec; 0.497 sec/batch)
2018-03-22 18:13:50.878522: step 47620, loss = 0.74 (250.0 examples/sec; 0.512 sec/batch)
2018-03-22 18:13:55.641863: step 47630, loss = 0.63 (268.7 examples/sec; 0.476 sec/batch)
2018-03-22 18:14:00.564371: step 47640, loss = 0.75 (260.0 examples/sec; 0.492 sec/batch)
2018-03-22 18:14:05.672506: step 47650, loss = 0.83 (250.6 examples/sec; 0.511 sec/batch)
2018-03-22 18:14:10.430495: step 47660, loss = 0.78 (269.0 examples/sec; 0.476 sec/batch)
2018-03-22 18:14:15.461374: step 47670, loss = 0.70 (254.4 examples/sec; 0.503 sec/batch)
2018-03-22 18:14:20.594416: step 47680, loss = 0.73 (249.4 examples/sec; 0.513 sec/batch)
2018-03-22 18:14:25.718366: step 47690, loss = 0.69 (249.8 examples/sec; 0.512 sec/batch)
2018-03-22 18:14:30.825876: step 47700, loss = 0.88 (250.6 examples/sec; 0.511 sec/batch)
2018-03-22 18:14:36.184507: step 47710, loss = 0.84 (238.9 examples/sec; 0.536 sec/batch)
2018-03-22 18:14:41.123362: step 47720, loss = 0.74 (259.2 examples/sec; 0.494 sec/batch)
2018-03-22 18:14:45.718415: step 47730, loss = 0.69 (278.6 examples/sec; 0.460 sec/batch)
2018-03-22 18:14:51.029437: step 47740, loss = 0.74 (241.0 examples/sec; 0.531 sec/batch)
2018-03-22 18:14:55.689420: step 47750, loss = 0.72 (274.7 examples/sec; 0.466 sec/batch)
2018-03-22 18:15:00.625415: step 47760, loss = 0.88 (259.3 examples/sec; 0.494 sec/batch)
2018-03-22 18:15:05.634385: step 47770, loss = 0.74 (255.5 examples/sec; 0.501 sec/batch)
2018-03-22 18:15:10.701541: step 47780, loss = 0.78 (252.6 examples/sec; 0.507 sec/batch)
2018-03-22 18:15:16.141442: step 47790, loss = 0.84 (235.3 examples/sec; 0.544 sec/batch)
2018-03-22 18:15:21.195907: step 47800, loss = 0.81 (253.2 examples/sec; 0.505 sec/batch)
2018-03-22 18:15:26.207860: step 47810, loss = 0.75 (255.4 examples/sec; 0.501 sec/batch)
2018-03-22 18:15:31.056818: step 47820, loss = 0.72 (264.0 examples/sec; 0.485 sec/batch)
2018-03-22 18:15:35.686658: step 47830, loss = 0.65 (276.5 examples/sec; 0.463 sec/batch)
2018-03-22 18:15:40.492356: step 47840, loss = 0.87 (266.4 examples/sec; 0.481 sec/batch)
2018-03-22 18:15:45.221797: step 47850, loss = 0.69 (270.6 examples/sec; 0.473 sec/batch)
2018-03-22 18:15:49.992376: step 47860, loss = 0.66 (268.3 examples/sec; 0.477 sec/batch)
2018-03-22 18:15:54.836308: step 47870, loss = 0.75 (264.2 examples/sec; 0.484 sec/batch)
2018-03-22 18:15:59.369855: step 47880, loss = 0.91 (282.3 examples/sec; 0.453 sec/batch)
2018-03-22 18:16:03.744593: step 47890, loss = 0.75 (292.6 examples/sec; 0.437 sec/batch)
2018-03-22 18:16:09.223012: step 47900, loss = 0.85 (233.6 examples/sec; 0.548 sec/batch)
2018-03-22 18:16:14.556466: step 47910, loss = 0.65 (240.0 examples/sec; 0.533 sec/batch)
2018-03-22 18:16:19.740411: step 47920, loss = 0.75 (246.9 examples/sec; 0.518 sec/batch)
2018-03-22 18:16:24.893747: step 47930, loss = 0.69 (248.4 examples/sec; 0.515 sec/batch)
2018-03-22 18:16:29.603095: step 47940, loss = 0.73 (271.8 examples/sec; 0.471 sec/batch)
2018-03-22 18:16:33.921430: step 47950, loss = 0.68 (296.4 examples/sec; 0.432 sec/batch)
2018-03-22 18:16:38.600419: step 47960, loss = 0.81 (273.6 examples/sec; 0.468 sec/batch)
2018-03-22 18:16:43.237644: step 47970, loss = 0.65 (276.0 examples/sec; 0.464 sec/batch)
2018-03-22 18:16:47.852994: step 47980, loss = 0.74 (277.3 examples/sec; 0.462 sec/batch)
2018-03-22 18:16:52.734300: step 47990, loss = 0.85 (262.2 examples/sec; 0.488 sec/batch)
2018-03-22 18:16:57.633286: step 48000, loss = 0.82 (261.3 examples/sec; 0.490 sec/batch)
2018-03-22 18:17:02.527331: step 48010, loss = 0.90 (261.5 examples/sec; 0.489 sec/batch)
2018-03-22 18:17:07.341486: step 48020, loss = 0.67 (265.9 examples/sec; 0.481 sec/batch)
2018-03-22 18:17:12.243386: step 48030, loss = 0.67 (261.1 examples/sec; 0.490 sec/batch)
2018-03-22 18:17:17.046475: step 48040, loss = 0.85 (266.5 examples/sec; 0.480 sec/batch)
2018-03-22 18:17:22.116479: step 48050, loss = 0.73 (252.5 examples/sec; 0.507 sec/batch)
2018-03-22 18:17:26.980358: step 48060, loss = 0.82 (263.2 examples/sec; 0.486 sec/batch)
2018-03-22 18:17:31.843003: step 48070, loss = 0.52 (263.2 examples/sec; 0.486 sec/batch)
2018-03-22 18:17:37.183406: step 48080, loss = 0.74 (239.7 examples/sec; 0.534 sec/batch)
2018-03-22 18:17:42.458784: step 48090, loss = 0.60 (242.6 examples/sec; 0.528 sec/batch)
2018-03-22 18:17:47.574226: step 48100, loss = 0.82 (250.2 examples/sec; 0.512 sec/batch)
2018-03-22 18:17:52.887447: step 48110, loss = 0.65 (240.9 examples/sec; 0.531 sec/batch)
2018-03-22 18:17:57.768034: step 48120, loss = 0.76 (262.3 examples/sec; 0.488 sec/batch)
2018-03-22 18:18:02.993372: step 48130, loss = 0.81 (245.0 examples/sec; 0.523 sec/batch)
2018-03-22 18:18:07.999417: step 48140, loss = 0.84 (255.7 examples/sec; 0.501 sec/batch)
2018-03-22 18:18:13.293207: step 48150, loss = 0.69 (241.8 examples/sec; 0.529 sec/batch)
2018-03-22 18:18:17.652368: step 48160, loss = 0.62 (293.6 examples/sec; 0.436 sec/batch)
2018-03-22 18:18:22.492455: step 48170, loss = 0.78 (264.5 examples/sec; 0.484 sec/batch)
2018-03-22 18:18:27.205507: step 48180, loss = 0.65 (271.6 examples/sec; 0.471 sec/batch)
2018-03-22 18:18:32.284546: step 48190, loss = 0.63 (252.0 examples/sec; 0.508 sec/batch)
2018-03-22 18:18:37.320032: step 48200, loss = 0.64 (254.2 examples/sec; 0.504 sec/batch)
2018-03-22 18:18:42.207396: step 48210, loss = 0.78 (261.9 examples/sec; 0.489 sec/batch)
2018-03-22 18:18:47.497240: step 48220, loss = 0.83 (242.0 examples/sec; 0.529 sec/batch)
2018-03-22 18:18:52.960586: step 48230, loss = 0.86 (234.3 examples/sec; 0.546 sec/batch)
2018-03-22 18:18:58.009447: step 48240, loss = 0.75 (253.5 examples/sec; 0.505 sec/batch)
2018-03-22 18:19:03.577395: step 48250, loss = 0.79 (229.9 examples/sec; 0.557 sec/batch)
2018-03-22 18:19:08.523978: step 48260, loss = 1.00 (258.8 examples/sec; 0.495 sec/batch)
2018-03-22 18:19:13.787442: step 48270, loss = 0.80 (243.2 examples/sec; 0.526 sec/batch)
2018-03-22 18:19:18.266756: step 48280, loss = 0.61 (285.8 examples/sec; 0.448 sec/batch)
2018-03-22 18:19:22.835363: step 48290, loss = 0.59 (280.2 examples/sec; 0.457 sec/batch)
2018-03-22 18:19:27.631873: step 48300, loss = 0.63 (266.9 examples/sec; 0.480 sec/batch)
2018-03-22 18:19:33.115539: step 48310, loss = 0.78 (233.4 examples/sec; 0.548 sec/batch)
2018-03-22 18:19:37.677336: step 48320, loss = 0.69 (280.6 examples/sec; 0.456 sec/batch)
2018-03-22 18:19:42.642637: step 48330, loss = 0.77 (257.8 examples/sec; 0.497 sec/batch)
2018-03-22 18:19:47.317285: step 48340, loss = 0.79 (273.8 examples/sec; 0.467 sec/batch)
2018-03-22 18:19:52.384987: step 48350, loss = 0.73 (252.6 examples/sec; 0.507 sec/batch)
2018-03-22 18:19:57.034361: step 48360, loss = 0.83 (275.3 examples/sec; 0.465 sec/batch)
2018-03-22 18:20:02.322876: step 48370, loss = 0.72 (242.0 examples/sec; 0.529 sec/batch)
2018-03-22 18:20:07.552177: step 48380, loss = 0.62 (244.8 examples/sec; 0.523 sec/batch)
2018-03-22 18:20:12.661413: step 48390, loss = 0.76 (250.5 examples/sec; 0.511 sec/batch)
2018-03-22 18:20:18.281766: step 48400, loss = 0.60 (227.7 examples/sec; 0.562 sec/batch)
2018-03-22 18:20:23.220652: step 48410, loss = 0.92 (259.2 examples/sec; 0.494 sec/batch)
2018-03-22 18:20:27.900443: step 48420, loss = 0.66 (273.5 examples/sec; 0.468 sec/batch)
2018-03-22 18:20:33.468229: step 48430, loss = 0.65 (229.9 examples/sec; 0.557 sec/batch)
2018-03-22 18:20:38.463175: step 48440, loss = 0.74 (256.3 examples/sec; 0.499 sec/batch)
2018-03-22 18:20:43.675647: step 48450, loss = 0.56 (245.6 examples/sec; 0.521 sec/batch)
2018-03-22 18:20:48.280478: step 48460, loss = 0.71 (278.0 examples/sec; 0.460 sec/batch)
2018-03-22 18:20:53.505402: step 48470, loss = 0.71 (245.0 examples/sec; 0.522 sec/batch)
2018-03-22 18:20:58.762454: step 48480, loss = 0.78 (243.5 examples/sec; 0.526 sec/batch)
2018-03-22 18:21:04.611392: step 48490, loss = 0.76 (218.8 examples/sec; 0.585 sec/batch)
2018-03-22 18:21:09.643249: step 48500, loss = 0.72 (254.4 examples/sec; 0.503 sec/batch)
2018-03-22 18:21:14.123332: step 48510, loss = 0.61 (285.7 examples/sec; 0.448 sec/batch)
2018-03-22 18:21:18.917753: step 48520, loss = 0.78 (267.0 examples/sec; 0.479 sec/batch)
2018-03-22 18:21:23.871475: step 48530, loss = 0.76 (258.4 examples/sec; 0.495 sec/batch)
2018-03-22 18:21:28.571426: step 48540, loss = 0.73 (272.3 examples/sec; 0.470 sec/batch)
2018-03-22 18:21:33.824996: step 48550, loss = 0.65 (243.6 examples/sec; 0.525 sec/batch)
2018-03-22 18:21:38.927354: step 48560, loss = 0.78 (250.9 examples/sec; 0.510 sec/batch)
2018-03-22 18:21:44.086473: step 48570, loss = 0.68 (248.1 examples/sec; 0.516 sec/batch)
2018-03-22 18:21:48.882561: step 48580, loss = 0.73 (266.9 examples/sec; 0.480 sec/batch)
2018-03-22 18:21:54.239984: step 48590, loss = 0.70 (238.9 examples/sec; 0.536 sec/batch)
2018-03-22 18:21:59.603831: step 48600, loss = 0.85 (238.6 examples/sec; 0.536 sec/batch)
2018-03-22 18:22:04.215616: step 48610, loss = 0.82 (277.5 examples/sec; 0.461 sec/batch)
2018-03-22 18:22:09.159367: step 48620, loss = 1.01 (258.9 examples/sec; 0.494 sec/batch)
2018-03-22 18:22:14.387110: step 48630, loss = 0.76 (244.8 examples/sec; 0.523 sec/batch)
2018-03-22 18:22:20.032376: step 48640, loss = 0.78 (226.7 examples/sec; 0.565 sec/batch)
2018-03-22 18:22:25.408887: step 48650, loss = 0.74 (238.1 examples/sec; 0.538 sec/batch)
2018-03-22 18:22:30.402439: step 48660, loss = 0.68 (256.3 examples/sec; 0.499 sec/batch)
2018-03-22 18:22:34.951349: step 48670, loss = 0.71 (281.4 examples/sec; 0.455 sec/batch)
2018-03-22 18:22:40.198518: step 48680, loss = 0.85 (243.9 examples/sec; 0.525 sec/batch)
2018-03-22 18:22:45.252466: step 48690, loss = 0.69 (253.3 examples/sec; 0.505 sec/batch)
2018-03-22 18:22:50.837788: step 48700, loss = 0.91 (229.2 examples/sec; 0.559 sec/batch)
2018-03-22 18:22:55.827242: step 48710, loss = 0.73 (256.5 examples/sec; 0.499 sec/batch)
2018-03-22 18:23:01.272594: step 48720, loss = 0.66 (235.1 examples/sec; 0.545 sec/batch)
2018-03-22 18:23:06.558409: step 48730, loss = 0.72 (242.2 examples/sec; 0.529 sec/batch)
2018-03-22 18:23:11.626087: step 48740, loss = 0.75 (252.6 examples/sec; 0.507 sec/batch)
2018-03-22 18:23:16.110391: step 48750, loss = 0.73 (285.4 examples/sec; 0.448 sec/batch)
2018-03-22 18:23:20.657349: step 48760, loss = 0.75 (281.5 examples/sec; 0.455 sec/batch)
2018-03-22 18:23:25.493807: step 48770, loss = 0.81 (264.7 examples/sec; 0.484 sec/batch)
2018-03-22 18:23:30.152017: step 48780, loss = 0.78 (274.8 examples/sec; 0.466 sec/batch)
2018-03-22 18:23:34.979235: step 48790, loss = 0.76 (265.2 examples/sec; 0.483 sec/batch)
2018-03-22 18:23:40.256618: step 48800, loss = 0.78 (242.5 examples/sec; 0.528 sec/batch)
2018-03-22 18:23:45.277361: step 48810, loss = 0.93 (254.9 examples/sec; 0.502 sec/batch)
2018-03-22 18:23:49.858473: step 48820, loss = 0.99 (279.4 examples/sec; 0.458 sec/batch)
2018-03-22 18:23:54.364478: step 48830, loss = 0.72 (284.1 examples/sec; 0.451 sec/batch)
2018-03-22 18:23:59.687384: step 48840, loss = 0.63 (240.5 examples/sec; 0.532 sec/batch)
2018-03-22 18:24:05.236704: step 48850, loss = 0.77 (230.7 examples/sec; 0.555 sec/batch)
2018-03-22 18:24:10.288459: step 48860, loss = 0.74 (253.4 examples/sec; 0.505 sec/batch)
2018-03-22 18:24:14.831580: step 48870, loss = 0.72 (281.7 examples/sec; 0.454 sec/batch)
2018-03-22 18:24:20.272210: step 48880, loss = 0.75 (235.3 examples/sec; 0.544 sec/batch)
2018-03-22 18:24:25.700385: step 48890, loss = 0.80 (235.8 examples/sec; 0.543 sec/batch)
2018-03-22 18:24:31.340650: step 48900, loss = 0.67 (226.9 examples/sec; 0.564 sec/batch)
2018-03-22 18:24:35.823414: step 48910, loss = 0.78 (285.5 examples/sec; 0.448 sec/batch)
2018-03-22 18:24:41.042002: step 48920, loss = 0.87 (245.3 examples/sec; 0.522 sec/batch)
2018-03-22 18:24:45.739332: step 48930, loss = 0.88 (272.5 examples/sec; 0.470 sec/batch)
2018-03-22 18:24:50.862793: step 48940, loss = 0.76 (249.8 examples/sec; 0.512 sec/batch)
2018-03-22 18:24:55.878219: step 48950, loss = 0.85 (255.2 examples/sec; 0.502 sec/batch)
2018-03-22 18:25:01.207438: step 48960, loss = 0.68 (240.2 examples/sec; 0.533 sec/batch)
2018-03-22 18:25:06.131999: step 48970, loss = 0.84 (259.9 examples/sec; 0.492 sec/batch)
2018-03-22 18:25:11.035332: step 48980, loss = 0.79 (261.0 examples/sec; 0.490 sec/batch)
2018-03-22 18:25:15.708408: step 48990, loss = 0.79 (273.9 examples/sec; 0.467 sec/batch)
2018-03-22 18:25:21.579340: step 49000, loss = 0.88 (218.0 examples/sec; 0.587 sec/batch)
2018-03-22 18:25:26.848439: step 49010, loss = 0.78 (242.9 examples/sec; 0.527 sec/batch)
2018-03-22 18:25:31.763439: step 49020, loss = 0.66 (260.4 examples/sec; 0.492 sec/batch)
2018-03-22 18:25:36.449706: step 49030, loss = 0.71 (273.2 examples/sec; 0.469 sec/batch)
2018-03-22 18:25:41.667381: step 49040, loss = 0.70 (245.3 examples/sec; 0.522 sec/batch)
2018-03-22 18:25:46.568444: step 49050, loss = 0.83 (261.2 examples/sec; 0.490 sec/batch)
2018-03-22 18:25:51.867383: step 49060, loss = 0.68 (241.6 examples/sec; 0.530 sec/batch)
2018-03-22 18:25:56.904867: step 49070, loss = 0.84 (254.1 examples/sec; 0.504 sec/batch)
2018-03-22 18:26:01.827649: step 49080, loss = 0.85 (260.0 examples/sec; 0.492 sec/batch)
2018-03-22 18:26:06.582419: step 49090, loss = 0.64 (269.2 examples/sec; 0.475 sec/batch)
2018-03-22 18:26:11.742462: step 49100, loss = 1.04 (248.1 examples/sec; 0.516 sec/batch)
2018-03-22 18:26:16.677362: step 49110, loss = 0.78 (259.4 examples/sec; 0.493 sec/batch)
2018-03-22 18:26:22.228804: step 49120, loss = 0.94 (230.6 examples/sec; 0.555 sec/batch)
2018-03-22 18:26:27.124139: step 49130, loss = 0.68 (261.5 examples/sec; 0.490 sec/batch)
2018-03-22 18:26:32.131080: step 49140, loss = 0.81 (255.6 examples/sec; 0.501 sec/batch)
2018-03-22 18:26:37.169412: step 49150, loss = 0.76 (254.1 examples/sec; 0.504 sec/batch)
2018-03-22 18:26:42.536394: step 49160, loss = 0.73 (238.5 examples/sec; 0.537 sec/batch)
2018-03-22 18:26:47.450416: step 49170, loss = 0.60 (260.5 examples/sec; 0.491 sec/batch)
2018-03-22 18:26:52.974461: step 49180, loss = 0.74 (231.7 examples/sec; 0.552 sec/batch)
2018-03-22 18:26:58.168537: step 49190, loss = 0.65 (246.4 examples/sec; 0.519 sec/batch)
2018-03-22 18:27:03.060486: step 49200, loss = 0.87 (261.7 examples/sec; 0.489 sec/batch)
2018-03-22 18:27:07.495011: step 49210, loss = 0.71 (288.6 examples/sec; 0.443 sec/batch)
2018-03-22 18:27:12.365465: step 49220, loss = 0.73 (262.8 examples/sec; 0.487 sec/batch)
2018-03-22 18:27:17.582911: step 49230, loss = 0.63 (245.3 examples/sec; 0.522 sec/batch)
2018-03-22 18:27:22.906021: step 49240, loss = 0.63 (240.5 examples/sec; 0.532 sec/batch)
2018-03-22 18:27:27.824320: step 49250, loss = 0.82 (260.3 examples/sec; 0.492 sec/batch)
2018-03-22 18:27:32.606208: step 49260, loss = 0.72 (267.7 examples/sec; 0.478 sec/batch)
2018-03-22 18:27:37.332415: step 49270, loss = 0.81 (270.8 examples/sec; 0.473 sec/batch)
2018-03-22 18:27:42.703414: step 49280, loss = 0.71 (238.3 examples/sec; 0.537 sec/batch)
2018-03-22 18:27:47.732358: step 49290, loss = 0.82 (254.5 examples/sec; 0.503 sec/batch)
2018-03-22 18:27:53.500139: step 49300, loss = 0.75 (221.9 examples/sec; 0.577 sec/batch)
2018-03-22 18:27:58.019339: step 49310, loss = 0.62 (283.2 examples/sec; 0.452 sec/batch)
2018-03-22 18:28:02.518681: step 49320, loss = 0.76 (284.5 examples/sec; 0.450 sec/batch)
2018-03-22 18:28:07.603495: step 49330, loss = 0.64 (251.7 examples/sec; 0.508 sec/batch)
2018-03-22 18:28:12.895374: step 49340, loss = 0.67 (241.9 examples/sec; 0.529 sec/batch)
2018-03-22 18:28:17.328386: step 49350, loss = 0.72 (288.7 examples/sec; 0.443 sec/batch)
2018-03-22 18:28:22.633216: step 49360, loss = 0.72 (241.3 examples/sec; 0.530 sec/batch)
2018-03-22 18:28:27.727612: step 49370, loss = 0.70 (251.3 examples/sec; 0.509 sec/batch)
2018-03-22 18:28:32.570998: step 49380, loss = 0.78 (264.3 examples/sec; 0.484 sec/batch)
2018-03-22 18:28:37.372369: step 49390, loss = 0.82 (266.6 examples/sec; 0.480 sec/batch)
2018-03-22 18:28:43.267044: step 49400, loss = 0.63 (217.1 examples/sec; 0.589 sec/batch)
2018-03-22 18:28:48.233336: step 49410, loss = 0.77 (257.7 examples/sec; 0.497 sec/batch)
2018-03-22 18:28:53.500405: step 49420, loss = 0.76 (243.0 examples/sec; 0.527 sec/batch)
2018-03-22 18:28:58.045516: step 49430, loss = 0.62 (281.6 examples/sec; 0.455 sec/batch)
2018-03-22 18:29:03.314440: step 49440, loss = 0.75 (242.9 examples/sec; 0.527 sec/batch)
2018-03-22 18:29:08.391340: step 49450, loss = 0.61 (252.1 examples/sec; 0.508 sec/batch)
2018-03-22 18:29:14.209385: step 49460, loss = 0.75 (220.0 examples/sec; 0.582 sec/batch)
2018-03-22 18:29:19.007431: step 49470, loss = 0.64 (266.8 examples/sec; 0.480 sec/batch)
2018-03-22 18:29:23.814026: step 49480, loss = 0.61 (266.3 examples/sec; 0.481 sec/batch)
2018-03-22 18:29:28.840396: step 49490, loss = 0.86 (254.7 examples/sec; 0.503 sec/batch)
2018-03-22 18:29:34.175908: step 49500, loss = 0.81 (239.9 examples/sec; 0.534 sec/batch)
2018-03-22 18:29:39.439402: step 49510, loss = 0.60 (243.2 examples/sec; 0.526 sec/batch)
2018-03-22 18:29:44.599554: step 49520, loss = 0.88 (248.1 examples/sec; 0.516 sec/batch)
2018-03-22 18:29:49.483379: step 49530, loss = 0.71 (262.1 examples/sec; 0.488 sec/batch)
2018-03-22 18:29:54.318371: step 49540, loss = 0.76 (264.7 examples/sec; 0.483 sec/batch)
2018-03-22 18:29:59.639335: step 49550, loss = 0.68 (240.6 examples/sec; 0.532 sec/batch)
2018-03-22 18:30:04.153008: step 49560, loss = 0.69 (283.6 examples/sec; 0.451 sec/batch)
2018-03-22 18:30:08.715194: step 49570, loss = 0.77 (280.6 examples/sec; 0.456 sec/batch)
2018-03-22 18:30:13.828533: step 49580, loss = 0.72 (250.3 examples/sec; 0.511 sec/batch)
2018-03-22 18:30:18.960413: step 49590, loss = 0.74 (249.4 examples/sec; 0.513 sec/batch)
2018-03-22 18:30:23.931717: step 49600, loss = 0.74 (257.5 examples/sec; 0.497 sec/batch)
2018-03-22 18:30:28.868442: step 49610, loss = 0.71 (259.3 examples/sec; 0.494 sec/batch)
2018-03-22 18:30:34.645336: step 49620, loss = 0.74 (221.6 examples/sec; 0.578 sec/batch)
2018-03-22 18:30:39.876433: step 49630, loss = 0.93 (244.7 examples/sec; 0.523 sec/batch)
2018-03-22 18:30:44.513432: step 49640, loss = 0.82 (276.0 examples/sec; 0.464 sec/batch)
2018-03-22 18:30:49.428396: step 49650, loss = 0.66 (260.4 examples/sec; 0.491 sec/batch)
2018-03-22 18:30:54.347359: step 49660, loss = 0.62 (260.2 examples/sec; 0.492 sec/batch)
2018-03-22 18:30:58.894232: step 49670, loss = 0.68 (281.5 examples/sec; 0.455 sec/batch)
2018-03-22 18:31:03.770443: step 49680, loss = 0.88 (262.5 examples/sec; 0.488 sec/batch)
2018-03-22 18:31:08.900818: step 49690, loss = 0.75 (249.5 examples/sec; 0.513 sec/batch)
2018-03-22 18:31:14.341322: step 49700, loss = 0.81 (235.3 examples/sec; 0.544 sec/batch)
2018-03-22 18:31:18.829353: step 49710, loss = 0.72 (285.2 examples/sec; 0.449 sec/batch)
2018-03-22 18:31:23.968034: step 49720, loss = 0.66 (249.1 examples/sec; 0.514 sec/batch)
2018-03-22 18:31:28.705721: step 49730, loss = 0.72 (270.2 examples/sec; 0.474 sec/batch)
2018-03-22 18:31:33.421094: step 49740, loss = 0.76 (271.4 examples/sec; 0.472 sec/batch)
2018-03-22 18:31:38.545339: step 49750, loss = 0.88 (249.8 examples/sec; 0.512 sec/batch)
2018-03-22 18:31:44.057979: step 49760, loss = 0.66 (232.2 examples/sec; 0.551 sec/batch)
2018-03-22 18:31:49.092885: step 49770, loss = 0.70 (254.2 examples/sec; 0.503 sec/batch)
2018-03-22 18:31:53.461673: step 49780, loss = 0.75 (293.0 examples/sec; 0.437 sec/batch)
2018-03-22 18:31:58.032384: step 49790, loss = 0.64 (280.0 examples/sec; 0.457 sec/batch)
2018-03-22 18:32:04.031025: step 49800, loss = 0.72 (213.4 examples/sec; 0.600 sec/batch)
2018-03-22 18:32:08.767893: step 49810, loss = 0.71 (270.2 examples/sec; 0.474 sec/batch)
2018-03-22 18:32:13.875169: step 49820, loss = 0.62 (250.6 examples/sec; 0.511 sec/batch)
2018-03-22 18:32:18.806419: step 49830, loss = 0.75 (259.6 examples/sec; 0.493 sec/batch)
2018-03-22 18:32:24.318328: step 49840, loss = 0.63 (232.2 examples/sec; 0.551 sec/batch)
2018-03-22 18:32:29.093391: step 49850, loss = 0.69 (268.1 examples/sec; 0.478 sec/batch)
2018-03-22 18:32:33.974298: step 49860, loss = 0.80 (262.2 examples/sec; 0.488 sec/batch)
2018-03-22 18:32:38.879425: step 49870, loss = 0.99 (261.0 examples/sec; 0.491 sec/batch)
2018-03-22 18:32:44.372401: step 49880, loss = 0.69 (233.0 examples/sec; 0.549 sec/batch)
2018-03-22 18:32:49.218640: step 49890, loss = 0.85 (264.1 examples/sec; 0.485 sec/batch)
2018-03-22 18:32:55.085653: step 49900, loss = 0.66 (218.2 examples/sec; 0.587 sec/batch)
2018-03-22 18:33:00.461182: step 49910, loss = 0.81 (238.1 examples/sec; 0.538 sec/batch)
2018-03-22 18:33:05.760489: step 49920, loss = 0.89 (241.5 examples/sec; 0.530 sec/batch)
2018-03-22 18:33:10.788382: step 49930, loss = 0.69 (254.6 examples/sec; 0.503 sec/batch)
2018-03-22 18:33:15.844660: step 49940, loss = 0.89 (253.2 examples/sec; 0.506 sec/batch)
2018-03-22 18:33:21.191420: step 49950, loss = 0.72 (239.4 examples/sec; 0.535 sec/batch)
2018-03-22 18:33:25.985348: step 49960, loss = 0.81 (267.0 examples/sec; 0.479 sec/batch)
2018-03-22 18:33:30.933423: step 49970, loss = 0.94 (258.7 examples/sec; 0.495 sec/batch)
2018-03-22 18:33:35.675388: step 49980, loss = 0.69 (269.9 examples/sec; 0.474 sec/batch)
2018-03-22 18:33:40.434643: step 49990, loss = 0.69 (268.9 examples/sec; 0.476 sec/batch)
2018-03-22 18:33:45.494514: step 50000, loss = 0.72 (253.0 examples/sec; 0.506 sec/batch)
2018-03-22 18:33:50.968325: step 50010, loss = 0.86 (233.8 examples/sec; 0.547 sec/batch)
2018-03-22 18:33:55.873328: step 50020, loss = 0.74 (261.0 examples/sec; 0.491 sec/batch)
2018-03-22 18:34:00.879066: step 50030, loss = 0.75 (255.7 examples/sec; 0.501 sec/batch)
2018-03-22 18:34:06.270364: step 50040, loss = 0.65 (237.4 examples/sec; 0.539 sec/batch)
2018-03-22 18:34:11.247458: step 50050, loss = 0.74 (257.2 examples/sec; 0.498 sec/batch)
2018-03-22 18:34:16.311453: step 50060, loss = 0.66 (252.8 examples/sec; 0.506 sec/batch)
2018-03-22 18:34:21.214380: step 50070, loss = 0.69 (261.1 examples/sec; 0.490 sec/batch)
2018-03-22 18:34:26.083273: step 50080, loss = 0.65 (262.9 examples/sec; 0.487 sec/batch)
2018-03-22 18:34:31.127488: step 50090, loss = 0.71 (253.8 examples/sec; 0.504 sec/batch)
2018-03-22 18:34:35.993907: step 50100, loss = 0.72 (263.0 examples/sec; 0.487 sec/batch)
2018-03-22 18:34:40.568307: step 50110, loss = 0.74 (279.8 examples/sec; 0.457 sec/batch)
2018-03-22 18:34:46.037598: step 50120, loss = 0.63 (234.0 examples/sec; 0.547 sec/batch)
2018-03-22 18:34:51.416424: step 50130, loss = 0.90 (238.0 examples/sec; 0.538 sec/batch)
2018-03-22 18:34:56.508434: step 50140, loss = 0.68 (251.4 examples/sec; 0.509 sec/batch)
2018-03-22 18:35:01.573396: step 50150, loss = 0.82 (252.7 examples/sec; 0.506 sec/batch)
2018-03-22 18:35:05.960493: step 50160, loss = 0.82 (291.8 examples/sec; 0.439 sec/batch)
2018-03-22 18:35:10.771360: step 50170, loss = 0.67 (266.1 examples/sec; 0.481 sec/batch)
2018-03-22 18:35:15.595221: step 50180, loss = 0.64 (265.3 examples/sec; 0.482 sec/batch)
2018-03-22 18:35:21.025104: step 50190, loss = 0.85 (235.7 examples/sec; 0.543 sec/batch)
2018-03-22 18:35:26.293222: step 50200, loss = 0.85 (243.0 examples/sec; 0.527 sec/batch)
2018-03-22 18:35:31.375461: step 50210, loss = 0.64 (251.9 examples/sec; 0.508 sec/batch)
2018-03-22 18:35:35.996128: step 50220, loss = 0.70 (277.0 examples/sec; 0.462 sec/batch)
2018-03-22 18:35:41.309431: step 50230, loss = 0.70 (240.9 examples/sec; 0.531 sec/batch)
2018-03-22 18:35:46.179451: step 50240, loss = 0.85 (262.8 examples/sec; 0.487 sec/batch)
2018-03-22 18:35:51.706474: step 50250, loss = 0.75 (231.6 examples/sec; 0.553 sec/batch)
2018-03-22 18:35:56.254279: step 50260, loss = 0.52 (281.5 examples/sec; 0.455 sec/batch)
2018-03-22 18:36:01.888567: step 50270, loss = 0.72 (227.2 examples/sec; 0.563 sec/batch)
2018-03-22 18:36:07.034356: step 50280, loss = 0.62 (248.7 examples/sec; 0.515 sec/batch)
2018-03-22 18:36:12.244443: step 50290, loss = 0.71 (245.7 examples/sec; 0.521 sec/batch)
2018-03-22 18:36:16.936969: step 50300, loss = 0.73 (272.8 examples/sec; 0.469 sec/batch)
2018-03-22 18:36:22.489616: step 50310, loss = 0.78 (230.5 examples/sec; 0.555 sec/batch)
2018-03-22 18:36:27.533599: step 50320, loss = 0.60 (253.8 examples/sec; 0.504 sec/batch)
2018-03-22 18:36:32.842376: step 50330, loss = 0.77 (241.1 examples/sec; 0.531 sec/batch)
2018-03-22 18:36:37.256358: step 50340, loss = 0.68 (290.0 examples/sec; 0.441 sec/batch)
2018-03-22 18:36:42.403372: step 50350, loss = 0.76 (248.7 examples/sec; 0.515 sec/batch)
2018-03-22 18:36:47.381386: step 50360, loss = 0.71 (257.1 examples/sec; 0.498 sec/batch)
2018-03-22 18:36:52.619450: step 50370, loss = 0.75 (244.4 examples/sec; 0.524 sec/batch)
2018-03-22 18:36:57.399158: step 50380, loss = 0.67 (267.8 examples/sec; 0.478 sec/batch)
2018-03-22 18:37:02.617608: step 50390, loss = 0.69 (245.3 examples/sec; 0.522 sec/batch)
2018-03-22 18:37:08.119238: step 50400, loss = 0.79 (232.7 examples/sec; 0.550 sec/batch)
2018-03-22 18:37:13.595367: step 50410, loss = 0.81 (233.7 examples/sec; 0.548 sec/batch)
2018-03-22 18:37:18.178447: step 50420, loss = 0.80 (279.3 examples/sec; 0.458 sec/batch)
2018-03-22 18:37:23.508490: step 50430, loss = 0.74 (240.1 examples/sec; 0.533 sec/batch)
2018-03-22 18:37:28.981873: step 50440, loss = 0.75 (233.9 examples/sec; 0.547 sec/batch)
2018-03-22 18:37:34.028976: step 50450, loss = 0.58 (253.6 examples/sec; 0.505 sec/batch)
2018-03-22 18:37:38.941399: step 50460, loss = 0.80 (260.6 examples/sec; 0.491 sec/batch)
2018-03-22 18:37:43.933403: step 50470, loss = 0.79 (256.4 examples/sec; 0.499 sec/batch)
2018-03-22 18:37:49.296439: step 50480, loss = 0.70 (238.7 examples/sec; 0.536 sec/batch)
2018-03-22 18:37:54.324282: step 50490, loss = 0.67 (254.6 examples/sec; 0.503 sec/batch)
2018-03-22 18:37:59.696939: step 50500, loss = 0.70 (238.2 examples/sec; 0.537 sec/batch)
2018-03-22 18:38:04.309639: step 50510, loss = 0.82 (277.5 examples/sec; 0.461 sec/batch)
2018-03-22 18:38:09.547390: step 50520, loss = 1.06 (244.4 examples/sec; 0.524 sec/batch)
2018-03-22 18:38:14.793404: step 50530, loss = 0.83 (244.0 examples/sec; 0.525 sec/batch)
2018-03-22 18:38:19.069369: step 50540, loss = 0.62 (299.3 examples/sec; 0.428 sec/batch)
2018-03-22 18:38:23.217122: step 50550, loss = 0.69 (308.6 examples/sec; 0.415 sec/batch)
2018-03-22 18:38:27.940104: step 50560, loss = 0.77 (271.0 examples/sec; 0.472 sec/batch)
2018-03-22 18:38:33.494995: step 50570, loss = 0.70 (230.4 examples/sec; 0.555 sec/batch)
2018-03-22 18:38:38.038041: step 50580, loss = 0.54 (281.7 examples/sec; 0.454 sec/batch)
2018-03-22 18:38:42.956395: step 50590, loss = 0.79 (260.2 examples/sec; 0.492 sec/batch)
2018-03-22 18:38:47.856606: step 50600, loss = 0.73 (261.2 examples/sec; 0.490 sec/batch)
2018-03-22 18:38:52.882828: step 50610, loss = 0.66 (254.7 examples/sec; 0.503 sec/batch)
2018-03-22 18:38:57.334444: step 50620, loss = 0.75 (287.5 examples/sec; 0.445 sec/batch)
2018-03-22 18:39:02.457375: step 50630, loss = 0.82 (249.9 examples/sec; 0.512 sec/batch)
2018-03-22 18:39:07.239074: step 50640, loss = 0.66 (267.7 examples/sec; 0.478 sec/batch)
2018-03-22 18:39:12.775386: step 50650, loss = 0.65 (231.2 examples/sec; 0.554 sec/batch)
2018-03-22 18:39:17.616341: step 50660, loss = 0.73 (264.4 examples/sec; 0.484 sec/batch)
2018-03-22 18:39:22.910065: step 50670, loss = 0.67 (241.8 examples/sec; 0.529 sec/batch)
2018-03-22 18:39:27.714346: step 50680, loss = 0.54 (266.4 examples/sec; 0.480 sec/batch)
2018-03-22 18:39:32.909461: step 50690, loss = 0.76 (246.4 examples/sec; 0.520 sec/batch)
2018-03-22 18:39:38.545337: step 50700, loss = 0.74 (227.1 examples/sec; 0.564 sec/batch)
2018-03-22 18:39:43.407388: step 50710, loss = 0.73 (263.3 examples/sec; 0.486 sec/batch)
2018-03-22 18:39:48.417989: step 50720, loss = 0.81 (255.5 examples/sec; 0.501 sec/batch)
2018-03-22 18:39:53.089873: step 50730, loss = 0.85 (274.0 examples/sec; 0.467 sec/batch)
2018-03-22 18:39:57.820406: step 50740, loss = 0.74 (270.6 examples/sec; 0.473 sec/batch)
2018-03-22 18:40:02.637306: step 50750, loss = 0.69 (265.7 examples/sec; 0.482 sec/batch)
2018-03-22 18:40:07.460884: step 50760, loss = 0.68 (265.4 examples/sec; 0.482 sec/batch)
2018-03-22 18:40:12.245839: step 50770, loss = 0.74 (267.5 examples/sec; 0.478 sec/batch)
2018-03-22 18:40:16.831381: step 50780, loss = 0.75 (279.1 examples/sec; 0.459 sec/batch)
2018-03-22 18:40:21.277532: step 50790, loss = 0.67 (287.9 examples/sec; 0.445 sec/batch)
2018-03-22 18:40:26.203598: step 50800, loss = 0.75 (259.8 examples/sec; 0.493 sec/batch)
2018-03-22 18:40:31.381153: step 50810, loss = 0.85 (247.2 examples/sec; 0.518 sec/batch)
2018-03-22 18:40:36.207409: step 50820, loss = 0.69 (265.2 examples/sec; 0.483 sec/batch)
2018-03-22 18:40:41.159413: step 50830, loss = 0.73 (258.5 examples/sec; 0.495 sec/batch)
2018-03-22 18:40:45.558842: step 50840, loss = 0.68 (290.9 examples/sec; 0.440 sec/batch)
2018-03-22 18:40:50.736421: step 50850, loss = 0.65 (247.2 examples/sec; 0.518 sec/batch)
2018-03-22 18:40:55.581448: step 50860, loss = 0.68 (264.2 examples/sec; 0.485 sec/batch)
2018-03-22 18:41:00.519367: step 50870, loss = 0.66 (259.2 examples/sec; 0.494 sec/batch)
2018-03-22 18:41:05.454449: step 50880, loss = 0.70 (259.4 examples/sec; 0.494 sec/batch)
2018-03-22 18:41:10.313614: step 50890, loss = 0.63 (263.4 examples/sec; 0.486 sec/batch)
2018-03-22 18:41:15.153810: step 50900, loss = 0.67 (264.5 examples/sec; 0.484 sec/batch)
2018-03-22 18:41:20.321518: step 50910, loss = 0.75 (247.7 examples/sec; 0.517 sec/batch)
2018-03-22 18:41:24.836366: step 50920, loss = 0.99 (283.5 examples/sec; 0.451 sec/batch)
2018-03-22 18:41:29.409081: step 50930, loss = 0.83 (279.9 examples/sec; 0.457 sec/batch)
2018-03-22 18:41:34.274306: step 50940, loss = 0.67 (263.1 examples/sec; 0.487 sec/batch)
2018-03-22 18:41:38.945014: step 50950, loss = 0.71 (274.0 examples/sec; 0.467 sec/batch)
2018-03-22 18:41:43.721385: step 50960, loss = 0.89 (268.0 examples/sec; 0.478 sec/batch)
2018-03-22 18:41:48.235340: step 50970, loss = 0.73 (283.6 examples/sec; 0.451 sec/batch)
2018-03-22 18:41:53.471501: step 50980, loss = 0.61 (244.5 examples/sec; 0.524 sec/batch)
2018-03-22 18:41:58.349360: step 50990, loss = 0.72 (262.4 examples/sec; 0.488 sec/batch)
2018-03-22 18:42:03.398687: step 51000, loss = 0.75 (253.5 examples/sec; 0.505 sec/batch)
2018-03-22 18:42:07.978362: step 51010, loss = 0.70 (279.5 examples/sec; 0.458 sec/batch)
2018-03-22 18:42:12.934119: step 51020, loss = 0.87 (258.3 examples/sec; 0.496 sec/batch)
2018-03-22 18:42:17.490420: step 51030, loss = 0.70 (280.9 examples/sec; 0.456 sec/batch)
2018-03-22 18:42:22.693226: step 51040, loss = 0.87 (246.0 examples/sec; 0.520 sec/batch)
2018-03-22 18:42:27.305383: step 51050, loss = 0.87 (277.5 examples/sec; 0.461 sec/batch)
2018-03-22 18:42:32.197952: step 51060, loss = 0.69 (261.6 examples/sec; 0.489 sec/batch)
2018-03-22 18:42:36.549461: step 51070, loss = 0.58 (294.1 examples/sec; 0.435 sec/batch)
2018-03-22 18:42:40.933655: step 51080, loss = 0.97 (292.0 examples/sec; 0.438 sec/batch)
2018-03-22 18:42:45.976392: step 51090, loss = 0.76 (253.8 examples/sec; 0.504 sec/batch)
2018-03-22 18:42:51.416203: step 51100, loss = 0.80 (235.3 examples/sec; 0.544 sec/batch)
2018-03-22 18:42:56.138656: step 51110, loss = 0.76 (271.0 examples/sec; 0.472 sec/batch)
2018-03-22 18:43:01.518027: step 51120, loss = 0.72 (237.9 examples/sec; 0.538 sec/batch)
2018-03-22 18:43:05.781374: step 51130, loss = 0.81 (300.2 examples/sec; 0.426 sec/batch)
2018-03-22 18:43:10.468376: step 51140, loss = 0.72 (273.1 examples/sec; 0.469 sec/batch)
2018-03-22 18:43:15.268990: step 51150, loss = 0.69 (266.6 examples/sec; 0.480 sec/batch)
2018-03-22 18:43:20.549433: step 51160, loss = 0.64 (242.4 examples/sec; 0.528 sec/batch)
2018-03-22 18:43:25.004652: step 51170, loss = 0.80 (287.3 examples/sec; 0.446 sec/batch)
2018-03-22 18:43:29.999491: step 51180, loss = 0.74 (256.3 examples/sec; 0.499 sec/batch)
2018-03-22 18:43:34.664397: step 51190, loss = 0.63 (274.4 examples/sec; 0.466 sec/batch)
2018-03-22 18:43:40.183458: step 51200, loss = 0.79 (231.9 examples/sec; 0.552 sec/batch)
2018-03-22 18:43:45.187056: step 51210, loss = 0.89 (255.8 examples/sec; 0.500 sec/batch)
2018-03-22 18:43:49.941288: step 51220, loss = 0.79 (269.2 examples/sec; 0.475 sec/batch)
2018-03-22 18:43:54.593346: step 51230, loss = 0.86 (275.1 examples/sec; 0.465 sec/batch)
2018-03-22 18:43:59.201760: step 51240, loss = 0.67 (277.8 examples/sec; 0.461 sec/batch)
2018-03-22 18:44:04.048795: step 51250, loss = 0.72 (264.1 examples/sec; 0.485 sec/batch)
2018-03-22 18:44:08.773977: step 51260, loss = 0.63 (270.9 examples/sec; 0.473 sec/batch)
2018-03-22 18:44:13.802613: step 51270, loss = 0.65 (254.5 examples/sec; 0.503 sec/batch)
2018-03-22 18:44:18.498608: step 51280, loss = 0.74 (272.6 examples/sec; 0.470 sec/batch)
2018-03-22 18:44:23.550351: step 51290, loss = 0.86 (253.4 examples/sec; 0.505 sec/batch)
2018-03-22 18:44:28.271553: step 51300, loss = 0.72 (271.1 examples/sec; 0.472 sec/batch)
2018-03-22 18:44:33.392531: step 51310, loss = 0.75 (250.0 examples/sec; 0.512 sec/batch)
2018-03-22 18:44:37.640371: step 51320, loss = 0.90 (301.3 examples/sec; 0.425 sec/batch)
2018-03-22 18:44:43.232405: step 51330, loss = 0.79 (228.9 examples/sec; 0.559 sec/batch)
2018-03-22 18:44:47.986390: step 51340, loss = 0.70 (269.2 examples/sec; 0.475 sec/batch)
2018-03-22 18:44:52.935688: step 51350, loss = 0.74 (258.6 examples/sec; 0.495 sec/batch)
2018-03-22 18:44:57.528380: step 51360, loss = 0.72 (278.7 examples/sec; 0.459 sec/batch)
2018-03-22 18:45:02.617379: step 51370, loss = 0.69 (251.5 examples/sec; 0.509 sec/batch)
2018-03-22 18:45:07.549429: step 51380, loss = 0.80 (259.5 examples/sec; 0.493 sec/batch)
2018-03-22 18:45:12.474390: step 51390, loss = 0.65 (259.9 examples/sec; 0.492 sec/batch)
2018-03-22 18:45:17.627736: step 51400, loss = 0.77 (248.4 examples/sec; 0.515 sec/batch)
2018-03-22 18:45:22.310546: step 51410, loss = 0.76 (273.3 examples/sec; 0.468 sec/batch)
2018-03-22 18:45:26.926431: step 51420, loss = 0.64 (277.3 examples/sec; 0.462 sec/batch)
2018-03-22 18:45:32.233629: step 51430, loss = 0.78 (241.2 examples/sec; 0.531 sec/batch)
2018-03-22 18:45:37.217675: step 51440, loss = 0.73 (256.8 examples/sec; 0.498 sec/batch)
2018-03-22 18:45:42.596339: step 51450, loss = 0.70 (238.0 examples/sec; 0.538 sec/batch)
2018-03-22 18:45:47.252553: step 51460, loss = 0.82 (274.9 examples/sec; 0.466 sec/batch)
2018-03-22 18:45:51.989373: step 51470, loss = 0.69 (270.2 examples/sec; 0.474 sec/batch)
2018-03-22 18:45:56.587372: step 51480, loss = 0.61 (278.4 examples/sec; 0.460 sec/batch)
2018-03-22 18:46:01.746126: step 51490, loss = 0.78 (248.1 examples/sec; 0.516 sec/batch)
2018-03-22 18:46:06.564593: step 51500, loss = 0.66 (265.6 examples/sec; 0.482 sec/batch)
2018-03-22 18:46:11.725397: step 51510, loss = 0.81 (248.0 examples/sec; 0.516 sec/batch)
2018-03-22 18:46:16.118398: step 51520, loss = 0.73 (291.4 examples/sec; 0.439 sec/batch)
2018-03-22 18:46:20.791958: step 51530, loss = 0.71 (273.9 examples/sec; 0.467 sec/batch)
2018-03-22 18:46:24.945534: step 51540, loss = 0.70 (308.2 examples/sec; 0.415 sec/batch)
2018-03-22 18:46:29.405414: step 51550, loss = 0.76 (287.0 examples/sec; 0.446 sec/batch)
2018-03-22 18:46:34.419397: step 51560, loss = 0.68 (255.3 examples/sec; 0.501 sec/batch)
2018-03-22 18:46:38.901394: step 51570, loss = 0.75 (285.6 examples/sec; 0.448 sec/batch)
2018-03-22 18:46:44.113408: step 51580, loss = 0.89 (245.6 examples/sec; 0.521 sec/batch)
2018-03-22 18:46:48.781475: step 51590, loss = 0.70 (274.2 examples/sec; 0.467 sec/batch)
2018-03-22 18:46:54.036448: step 51600, loss = 0.68 (243.6 examples/sec; 0.525 sec/batch)
2018-03-22 18:46:58.370719: step 51610, loss = 0.68 (295.3 examples/sec; 0.433 sec/batch)
2018-03-22 18:47:03.266434: step 51620, loss = 0.77 (261.5 examples/sec; 0.490 sec/batch)
2018-03-22 18:47:08.122394: step 51630, loss = 0.65 (263.6 examples/sec; 0.486 sec/batch)
2018-03-22 18:47:13.399999: step 51640, loss = 0.78 (242.5 examples/sec; 0.528 sec/batch)
2018-03-22 18:47:18.424376: step 51650, loss = 0.84 (254.8 examples/sec; 0.502 sec/batch)
2018-03-22 18:47:22.979418: step 51660, loss = 0.77 (281.0 examples/sec; 0.456 sec/batch)
2018-03-22 18:47:27.676496: step 51670, loss = 0.81 (272.5 examples/sec; 0.470 sec/batch)
2018-03-22 18:47:32.838274: step 51680, loss = 0.66 (248.0 examples/sec; 0.516 sec/batch)
2018-03-22 18:47:37.203430: step 51690, loss = 0.76 (293.2 examples/sec; 0.437 sec/batch)
2018-03-22 18:47:42.946087: step 51700, loss = 0.71 (222.9 examples/sec; 0.574 sec/batch)
2018-03-22 18:47:47.789482: step 51710, loss = 0.67 (264.3 examples/sec; 0.484 sec/batch)
2018-03-22 18:47:52.916561: step 51720, loss = 0.71 (249.7 examples/sec; 0.513 sec/batch)
2018-03-22 18:47:57.945349: step 51730, loss = 0.76 (254.5 examples/sec; 0.503 sec/batch)
2018-03-22 18:48:03.383352: step 51740, loss = 0.77 (235.4 examples/sec; 0.544 sec/batch)
2018-03-22 18:48:08.192480: step 51750, loss = 0.83 (266.2 examples/sec; 0.481 sec/batch)
2018-03-22 18:48:13.077330: step 51760, loss = 1.00 (262.0 examples/sec; 0.488 sec/batch)
2018-03-22 18:48:18.056572: step 51770, loss = 0.74 (257.1 examples/sec; 0.498 sec/batch)
2018-03-22 18:48:23.125873: step 51780, loss = 0.77 (252.5 examples/sec; 0.507 sec/batch)
2018-03-22 18:48:27.943407: step 51790, loss = 0.64 (265.7 examples/sec; 0.482 sec/batch)
2018-03-22 18:48:33.386848: step 51800, loss = 0.71 (235.1 examples/sec; 0.544 sec/batch)
2018-03-22 18:48:38.135710: step 51810, loss = 0.85 (269.6 examples/sec; 0.475 sec/batch)
2018-03-22 18:48:43.020309: step 51820, loss = 0.79 (262.0 examples/sec; 0.488 sec/batch)
2018-03-22 18:48:47.529374: step 51830, loss = 0.79 (283.9 examples/sec; 0.451 sec/batch)
2018-03-22 18:48:52.587433: step 51840, loss = 0.70 (253.1 examples/sec; 0.506 sec/batch)
2018-03-22 18:48:57.359386: step 51850, loss = 0.81 (268.2 examples/sec; 0.477 sec/batch)
2018-03-22 18:49:02.430343: step 51860, loss = 0.72 (252.4 examples/sec; 0.507 sec/batch)
2018-03-22 18:49:07.110419: step 51870, loss = 0.86 (273.5 examples/sec; 0.468 sec/batch)
2018-03-22 18:49:12.423428: step 51880, loss = 0.66 (240.9 examples/sec; 0.531 sec/batch)
2018-03-22 18:49:17.152405: step 51890, loss = 0.89 (270.7 examples/sec; 0.473 sec/batch)
2018-03-22 18:49:22.157819: step 51900, loss = 0.60 (255.7 examples/sec; 0.501 sec/batch)
2018-03-22 18:49:26.868274: step 51910, loss = 0.75 (271.7 examples/sec; 0.471 sec/batch)
2018-03-22 18:49:31.590365: step 51920, loss = 0.82 (271.1 examples/sec; 0.472 sec/batch)
2018-03-22 18:49:36.078338: step 51930, loss = 0.74 (285.2 examples/sec; 0.449 sec/batch)
2018-03-22 18:49:40.925623: step 51940, loss = 0.79 (264.1 examples/sec; 0.485 sec/batch)
2018-03-22 18:49:45.743387: step 51950, loss = 0.85 (265.7 examples/sec; 0.482 sec/batch)
2018-03-22 18:49:50.957255: step 51960, loss = 0.64 (245.5 examples/sec; 0.521 sec/batch)
2018-03-22 18:49:55.557381: step 51970, loss = 0.78 (278.3 examples/sec; 0.460 sec/batch)
2018-03-22 18:50:00.399392: step 51980, loss = 0.73 (264.4 examples/sec; 0.484 sec/batch)
2018-03-22 18:50:05.149524: step 51990, loss = 0.80 (269.5 examples/sec; 0.475 sec/batch)
2018-03-22 18:50:09.732080: step 52000, loss = 0.68 (279.3 examples/sec; 0.458 sec/batch)
2018-03-22 18:50:14.154438: step 52010, loss = 0.69 (289.4 examples/sec; 0.442 sec/batch)
2018-03-22 18:50:18.333258: step 52020, loss = 0.65 (306.3 examples/sec; 0.418 sec/batch)
2018-03-22 18:50:22.965980: step 52030, loss = 0.73 (276.3 examples/sec; 0.463 sec/batch)
2018-03-22 18:50:27.412380: step 52040, loss = 0.74 (287.9 examples/sec; 0.445 sec/batch)
2018-03-22 18:50:32.766394: step 52050, loss = 0.80 (239.1 examples/sec; 0.535 sec/batch)
2018-03-22 18:50:37.365333: step 52060, loss = 0.78 (278.3 examples/sec; 0.460 sec/batch)
2018-03-22 18:50:42.337747: step 52070, loss = 0.68 (257.4 examples/sec; 0.497 sec/batch)
2018-03-22 18:50:46.842919: step 52080, loss = 0.73 (284.1 examples/sec; 0.451 sec/batch)
2018-03-22 18:50:51.639653: step 52090, loss = 0.75 (266.8 examples/sec; 0.480 sec/batch)
2018-03-22 18:50:57.117323: step 52100, loss = 0.81 (233.7 examples/sec; 0.548 sec/batch)
2018-03-22 18:51:02.298427: step 52110, loss = 0.65 (247.1 examples/sec; 0.518 sec/batch)
2018-03-22 18:51:07.063218: step 52120, loss = 0.69 (268.6 examples/sec; 0.476 sec/batch)
2018-03-22 18:51:11.873800: step 52130, loss = 0.77 (266.1 examples/sec; 0.481 sec/batch)
2018-03-22 18:51:16.387665: step 52140, loss = 0.73 (283.6 examples/sec; 0.451 sec/batch)
2018-03-22 18:51:21.875336: step 52150, loss = 0.74 (233.2 examples/sec; 0.549 sec/batch)
2018-03-22 18:51:26.369326: step 52160, loss = 0.78 (284.8 examples/sec; 0.449 sec/batch)
2018-03-22 18:51:31.528333: step 52170, loss = 0.76 (248.1 examples/sec; 0.516 sec/batch)
2018-03-22 18:51:36.402395: step 52180, loss = 0.64 (262.6 examples/sec; 0.487 sec/batch)
2018-03-22 18:51:41.438476: step 52190, loss = 0.67 (254.2 examples/sec; 0.504 sec/batch)
2018-03-22 18:51:46.170439: step 52200, loss = 0.72 (270.5 examples/sec; 0.473 sec/batch)
2018-03-22 18:51:51.205424: step 52210, loss = 0.62 (254.2 examples/sec; 0.504 sec/batch)
2018-03-22 18:51:56.046420: step 52220, loss = 0.84 (264.4 examples/sec; 0.484 sec/batch)
2018-03-22 18:52:01.015447: step 52230, loss = 0.72 (257.6 examples/sec; 0.497 sec/batch)
2018-03-22 18:52:05.580422: step 52240, loss = 0.72 (280.4 examples/sec; 0.456 sec/batch)
2018-03-22 18:52:10.793703: step 52250, loss = 0.80 (245.5 examples/sec; 0.521 sec/batch)
2018-03-22 18:52:15.220399: step 52260, loss = 0.91 (289.2 examples/sec; 0.443 sec/batch)
2018-03-22 18:52:20.456557: step 52270, loss = 0.68 (244.5 examples/sec; 0.524 sec/batch)
2018-03-22 18:52:25.114099: step 52280, loss = 0.72 (274.8 examples/sec; 0.466 sec/batch)
2018-03-22 18:52:30.287433: step 52290, loss = 0.76 (247.4 examples/sec; 0.517 sec/batch)
2018-03-22 18:52:35.232112: step 52300, loss = 0.68 (258.9 examples/sec; 0.494 sec/batch)
2018-03-22 18:52:40.233545: step 52310, loss = 0.62 (255.9 examples/sec; 0.500 sec/batch)
2018-03-22 18:52:45.222381: step 52320, loss = 0.58 (256.6 examples/sec; 0.499 sec/batch)
2018-03-22 18:52:50.381400: step 52330, loss = 0.65 (248.1 examples/sec; 0.516 sec/batch)
2018-03-22 18:52:55.322003: step 52340, loss = 0.73 (259.1 examples/sec; 0.494 sec/batch)
2018-03-22 18:53:00.540466: step 52350, loss = 0.80 (245.3 examples/sec; 0.522 sec/batch)
2018-03-22 18:53:04.949911: step 52360, loss = 0.79 (290.3 examples/sec; 0.441 sec/batch)
2018-03-22 18:53:09.610519: step 52370, loss = 0.76 (274.6 examples/sec; 0.466 sec/batch)
2018-03-22 18:53:14.413892: step 52380, loss = 0.79 (266.5 examples/sec; 0.480 sec/batch)
2018-03-22 18:53:18.842425: step 52390, loss = 0.88 (289.0 examples/sec; 0.443 sec/batch)
2018-03-22 18:53:24.021592: step 52400, loss = 0.76 (247.1 examples/sec; 0.518 sec/batch)
2018-03-22 18:53:28.596825: step 52410, loss = 0.72 (279.8 examples/sec; 0.458 sec/batch)
2018-03-22 18:53:33.688770: step 52420, loss = 0.63 (251.4 examples/sec; 0.509 sec/batch)
2018-03-22 18:53:38.563015: step 52430, loss = 0.85 (262.6 examples/sec; 0.487 sec/batch)
2018-03-22 18:53:43.671149: step 52440, loss = 0.81 (250.6 examples/sec; 0.511 sec/batch)
2018-03-22 18:53:48.373386: step 52450, loss = 0.71 (272.2 examples/sec; 0.470 sec/batch)
2018-03-22 18:53:53.809242: step 52460, loss = 0.69 (235.5 examples/sec; 0.544 sec/batch)
2018-03-22 18:53:58.173945: step 52470, loss = 0.75 (293.3 examples/sec; 0.436 sec/batch)
2018-03-22 18:54:02.995334: step 52480, loss = 0.67 (265.5 examples/sec; 0.482 sec/batch)
2018-03-22 18:54:07.340423: step 52490, loss = 0.67 (294.6 examples/sec; 0.435 sec/batch)
2018-03-22 18:54:11.858102: step 52500, loss = 0.64 (283.3 examples/sec; 0.452 sec/batch)
2018-03-22 18:54:16.353363: step 52510, loss = 0.67 (284.7 examples/sec; 0.450 sec/batch)
2018-03-22 18:54:21.717614: step 52520, loss = 0.76 (238.6 examples/sec; 0.536 sec/batch)
2018-03-22 18:54:26.716221: step 52530, loss = 0.62 (256.1 examples/sec; 0.500 sec/batch)
2018-03-22 18:54:31.926306: step 52540, loss = 0.77 (245.7 examples/sec; 0.521 sec/batch)
2018-03-22 18:54:36.332969: step 52550, loss = 0.65 (290.5 examples/sec; 0.441 sec/batch)
2018-03-22 18:54:40.750042: step 52560, loss = 0.68 (289.8 examples/sec; 0.442 sec/batch)
2018-03-22 18:54:45.264348: step 52570, loss = 0.73 (283.5 examples/sec; 0.451 sec/batch)
2018-03-22 18:54:50.277404: step 52580, loss = 0.79 (255.3 examples/sec; 0.501 sec/batch)
2018-03-22 18:54:55.198492: step 52590, loss = 0.75 (260.1 examples/sec; 0.492 sec/batch)
2018-03-22 18:55:00.677938: step 52600, loss = 0.83 (233.6 examples/sec; 0.548 sec/batch)
2018-03-22 18:55:05.315890: step 52610, loss = 0.76 (276.0 examples/sec; 0.464 sec/batch)
2018-03-22 18:55:10.240379: step 52620, loss = 0.61 (259.9 examples/sec; 0.492 sec/batch)
2018-03-22 18:55:14.875375: step 52630, loss = 0.89 (276.2 examples/sec; 0.463 sec/batch)
2018-03-22 18:55:19.391199: step 52640, loss = 0.67 (283.4 examples/sec; 0.452 sec/batch)
2018-03-22 18:55:24.482414: step 52650, loss = 0.71 (251.4 examples/sec; 0.509 sec/batch)
2018-03-22 18:55:28.873123: step 52660, loss = 0.60 (291.5 examples/sec; 0.439 sec/batch)
2018-03-22 18:55:34.166930: step 52670, loss = 0.63 (241.8 examples/sec; 0.529 sec/batch)
2018-03-22 18:55:39.141196: step 52680, loss = 0.75 (257.3 examples/sec; 0.497 sec/batch)
2018-03-22 18:55:43.999472: step 52690, loss = 0.73 (263.5 examples/sec; 0.486 sec/batch)
2018-03-22 18:55:48.888193: step 52700, loss = 0.87 (261.8 examples/sec; 0.489 sec/batch)
2018-03-22 18:55:53.723472: step 52710, loss = 0.74 (264.7 examples/sec; 0.484 sec/batch)
2018-03-22 18:55:58.435422: step 52720, loss = 0.72 (271.6 examples/sec; 0.471 sec/batch)
2018-03-22 18:56:03.463327: step 52730, loss = 0.78 (254.6 examples/sec; 0.503 sec/batch)
2018-03-22 18:56:08.183468: step 52740, loss = 0.64 (271.2 examples/sec; 0.472 sec/batch)
2018-03-22 18:56:13.034417: step 52750, loss = 0.74 (263.9 examples/sec; 0.485 sec/batch)
2018-03-22 18:56:17.821435: step 52760, loss = 0.71 (267.4 examples/sec; 0.479 sec/batch)
2018-03-22 18:56:22.871097: step 52770, loss = 0.82 (253.5 examples/sec; 0.505 sec/batch)
2018-03-22 18:56:27.634403: step 52780, loss = 0.65 (268.7 examples/sec; 0.476 sec/batch)
2018-03-22 18:56:32.787615: step 52790, loss = 0.78 (248.4 examples/sec; 0.515 sec/batch)
2018-03-22 18:56:37.579693: step 52800, loss = 0.67 (267.1 examples/sec; 0.479 sec/batch)
2018-03-22 18:56:42.621332: step 52810, loss = 0.84 (253.9 examples/sec; 0.504 sec/batch)
2018-03-22 18:56:47.498275: step 52820, loss = 0.80 (262.5 examples/sec; 0.488 sec/batch)
2018-03-22 18:56:52.794354: step 52830, loss = 0.82 (241.7 examples/sec; 0.530 sec/batch)
2018-03-22 18:56:57.424118: step 52840, loss = 0.71 (276.5 examples/sec; 0.463 sec/batch)
2018-03-22 18:57:02.401389: step 52850, loss = 0.80 (257.2 examples/sec; 0.498 sec/batch)
2018-03-22 18:57:06.950403: step 52860, loss = 0.69 (281.4 examples/sec; 0.455 sec/batch)
2018-03-22 18:57:11.549364: step 52870, loss = 0.65 (278.3 examples/sec; 0.460 sec/batch)
2018-03-22 18:57:16.286359: step 52880, loss = 0.78 (270.2 examples/sec; 0.474 sec/batch)
2018-03-22 18:57:21.241458: step 52890, loss = 0.77 (258.3 examples/sec; 0.496 sec/batch)
2018-03-22 18:57:26.030181: step 52900, loss = 0.65 (267.3 examples/sec; 0.479 sec/batch)
2018-03-22 18:57:30.936412: step 52910, loss = 0.68 (260.9 examples/sec; 0.491 sec/batch)
2018-03-22 18:57:35.705031: step 52920, loss = 0.74 (268.4 examples/sec; 0.477 sec/batch)
2018-03-22 18:57:40.983596: step 52930, loss = 0.76 (242.5 examples/sec; 0.528 sec/batch)
2018-03-22 18:57:45.602947: step 52940, loss = 0.79 (277.1 examples/sec; 0.462 sec/batch)
2018-03-22 18:57:50.429854: step 52950, loss = 0.70 (265.2 examples/sec; 0.483 sec/batch)
2018-03-22 18:57:55.018353: step 52960, loss = 0.87 (279.0 examples/sec; 0.459 sec/batch)
2018-03-22 18:57:59.557756: step 52970, loss = 0.76 (282.0 examples/sec; 0.454 sec/batch)
2018-03-22 18:58:04.253336: step 52980, loss = 0.88 (272.6 examples/sec; 0.470 sec/batch)
2018-03-22 18:58:08.601078: step 52990, loss = 0.67 (294.4 examples/sec; 0.435 sec/batch)
2018-03-22 18:58:13.811500: step 53000, loss = 0.70 (245.7 examples/sec; 0.521 sec/batch)
2018-03-22 18:58:18.864365: step 53010, loss = 0.64 (253.3 examples/sec; 0.505 sec/batch)
2018-03-22 18:58:24.188831: step 53020, loss = 0.73 (240.4 examples/sec; 0.532 sec/batch)
2018-03-22 18:58:28.887408: step 53030, loss = 0.75 (272.4 examples/sec; 0.470 sec/batch)
2018-03-22 18:58:33.956316: step 53040, loss = 0.77 (252.5 examples/sec; 0.507 sec/batch)
2018-03-22 18:58:38.534386: step 53050, loss = 0.71 (279.6 examples/sec; 0.458 sec/batch)
2018-03-22 18:58:43.709591: step 53060, loss = 0.73 (247.3 examples/sec; 0.518 sec/batch)
2018-03-22 18:58:48.223328: step 53070, loss = 0.75 (283.6 examples/sec; 0.451 sec/batch)
2018-03-22 18:58:53.029426: step 53080, loss = 0.69 (266.3 examples/sec; 0.481 sec/batch)
2018-03-22 18:58:57.932369: step 53090, loss = 0.66 (261.1 examples/sec; 0.490 sec/batch)
2018-03-22 18:59:02.824095: step 53100, loss = 0.75 (261.7 examples/sec; 0.489 sec/batch)
2018-03-22 18:59:07.754364: step 53110, loss = 0.81 (259.6 examples/sec; 0.493 sec/batch)
2018-03-22 18:59:12.815860: step 53120, loss = 0.74 (252.9 examples/sec; 0.506 sec/batch)
2018-03-22 18:59:17.533385: step 53130, loss = 0.86 (271.3 examples/sec; 0.472 sec/batch)
2018-03-22 18:59:22.918390: step 53140, loss = 0.88 (237.7 examples/sec; 0.538 sec/batch)
2018-03-22 18:59:27.793706: step 53150, loss = 0.62 (262.5 examples/sec; 0.488 sec/batch)
2018-03-22 18:59:32.979877: step 53160, loss = 0.95 (246.8 examples/sec; 0.519 sec/batch)
2018-03-22 18:59:37.344356: step 53170, loss = 0.75 (293.3 examples/sec; 0.436 sec/batch)
2018-03-22 18:59:42.574282: step 53180, loss = 0.66 (244.7 examples/sec; 0.523 sec/batch)
2018-03-22 18:59:47.225200: step 53190, loss = 0.70 (275.2 examples/sec; 0.465 sec/batch)
2018-03-22 18:59:52.362006: step 53200, loss = 0.66 (249.2 examples/sec; 0.514 sec/batch)
2018-03-22 18:59:57.095379: step 53210, loss = 0.77 (270.4 examples/sec; 0.473 sec/batch)
2018-03-22 19:00:02.133206: step 53220, loss = 0.80 (254.1 examples/sec; 0.504 sec/batch)
2018-03-22 19:00:06.821406: step 53230, loss = 0.68 (273.0 examples/sec; 0.469 sec/batch)
2018-03-22 19:00:12.443427: step 53240, loss = 0.77 (227.7 examples/sec; 0.562 sec/batch)
2018-03-22 19:00:17.085393: step 53250, loss = 0.83 (275.7 examples/sec; 0.464 sec/batch)
2018-03-22 19:00:22.430532: step 53260, loss = 0.77 (239.5 examples/sec; 0.535 sec/batch)
2018-03-22 19:00:27.051389: step 53270, loss = 0.61 (277.0 examples/sec; 0.462 sec/batch)
2018-03-22 19:00:32.109238: step 53280, loss = 0.76 (253.1 examples/sec; 0.506 sec/batch)
2018-03-22 19:00:36.950389: step 53290, loss = 0.81 (264.4 examples/sec; 0.484 sec/batch)
2018-03-22 19:00:42.093989: step 53300, loss = 0.66 (248.9 examples/sec; 0.514 sec/batch)
2018-03-22 19:00:46.903806: step 53310, loss = 0.82 (266.1 examples/sec; 0.481 sec/batch)
2018-03-22 19:00:52.093497: step 53320, loss = 0.65 (246.6 examples/sec; 0.519 sec/batch)
2018-03-22 19:00:56.735274: step 53330, loss = 0.73 (275.8 examples/sec; 0.464 sec/batch)
2018-03-22 19:01:01.148701: step 53340, loss = 0.69 (290.0 examples/sec; 0.441 sec/batch)
2018-03-22 19:01:05.906010: step 53350, loss = 0.79 (269.1 examples/sec; 0.476 sec/batch)
2018-03-22 19:01:10.704460: step 53360, loss = 0.76 (266.8 examples/sec; 0.480 sec/batch)
2018-03-22 19:01:15.574379: step 53370, loss = 0.64 (262.8 examples/sec; 0.487 sec/batch)
2018-03-22 19:01:20.064893: step 53380, loss = 0.76 (285.0 examples/sec; 0.449 sec/batch)
2018-03-22 19:01:24.843389: step 53390, loss = 0.77 (267.9 examples/sec; 0.478 sec/batch)
2018-03-22 19:01:30.007084: step 53400, loss = 0.74 (247.9 examples/sec; 0.516 sec/batch)
2018-03-22 19:01:34.936418: step 53410, loss = 0.83 (259.7 examples/sec; 0.493 sec/batch)
2018-03-22 19:01:39.452124: step 53420, loss = 0.69 (283.5 examples/sec; 0.452 sec/batch)
2018-03-22 19:01:44.223397: step 53430, loss = 0.74 (268.3 examples/sec; 0.477 sec/batch)
2018-03-22 19:01:48.649873: step 53440, loss = 0.63 (289.2 examples/sec; 0.443 sec/batch)
2018-03-22 19:01:53.459395: step 53450, loss = 0.69 (266.1 examples/sec; 0.481 sec/batch)
2018-03-22 19:01:57.815058: step 53460, loss = 0.80 (293.9 examples/sec; 0.436 sec/batch)
2018-03-22 19:02:02.563030: step 53470, loss = 0.77 (269.6 examples/sec; 0.475 sec/batch)
2018-03-22 19:02:07.270640: step 53480, loss = 0.62 (271.9 examples/sec; 0.471 sec/batch)
2018-03-22 19:02:12.110548: step 53490, loss = 0.64 (264.5 examples/sec; 0.484 sec/batch)
2018-03-22 19:02:17.364592: step 53500, loss = 0.79 (243.6 examples/sec; 0.525 sec/batch)
2018-03-22 19:02:22.790165: step 53510, loss = 0.78 (235.9 examples/sec; 0.543 sec/batch)
2018-03-22 19:02:26.964818: step 53520, loss = 0.74 (306.6 examples/sec; 0.417 sec/batch)
2018-03-22 19:02:31.776092: step 53530, loss = 0.86 (266.0 examples/sec; 0.481 sec/batch)
2018-03-22 19:02:36.607021: step 53540, loss = 0.71 (265.0 examples/sec; 0.483 sec/batch)
2018-03-22 19:02:41.665358: step 53550, loss = 0.81 (253.0 examples/sec; 0.506 sec/batch)
2018-03-22 19:02:46.222455: step 53560, loss = 0.67 (280.9 examples/sec; 0.456 sec/batch)
2018-03-22 19:02:51.613289: step 53570, loss = 0.71 (237.4 examples/sec; 0.539 sec/batch)
2018-03-22 19:02:56.618456: step 53580, loss = 0.79 (255.7 examples/sec; 0.501 sec/batch)
2018-03-22 19:03:01.800427: step 53590, loss = 0.80 (247.0 examples/sec; 0.518 sec/batch)
2018-03-22 19:03:06.655799: step 53600, loss = 0.78 (263.6 examples/sec; 0.486 sec/batch)
2018-03-22 19:03:11.654444: step 53610, loss = 0.74 (256.1 examples/sec; 0.500 sec/batch)
2018-03-22 19:03:16.472424: step 53620, loss = 0.64 (265.7 examples/sec; 0.482 sec/batch)
2018-03-22 19:03:21.364070: step 53630, loss = 0.73 (261.7 examples/sec; 0.489 sec/batch)
2018-03-22 19:03:25.709373: step 53640, loss = 0.77 (294.6 examples/sec; 0.435 sec/batch)
2018-03-22 19:03:30.403383: step 53650, loss = 0.86 (272.7 examples/sec; 0.469 sec/batch)
2018-03-22 19:03:35.746399: step 53660, loss = 0.83 (239.6 examples/sec; 0.534 sec/batch)
2018-03-22 19:03:41.023468: step 53670, loss = 0.80 (242.6 examples/sec; 0.528 sec/batch)
2018-03-22 19:03:45.660273: step 53680, loss = 0.88 (276.0 examples/sec; 0.464 sec/batch)
2018-03-22 19:03:50.628406: step 53690, loss = 0.91 (257.6 examples/sec; 0.497 sec/batch)
2018-03-22 19:03:55.280315: step 53700, loss = 0.81 (275.2 examples/sec; 0.465 sec/batch)
2018-03-22 19:04:00.151997: step 53710, loss = 0.50 (262.7 examples/sec; 0.487 sec/batch)
2018-03-22 19:04:04.761387: step 53720, loss = 0.75 (277.7 examples/sec; 0.461 sec/batch)
2018-03-22 19:04:09.830377: step 53730, loss = 0.76 (252.5 examples/sec; 0.507 sec/batch)
2018-03-22 19:04:14.622942: step 53740, loss = 0.76 (267.1 examples/sec; 0.479 sec/batch)
2018-03-22 19:04:19.531707: step 53750, loss = 0.88 (260.8 examples/sec; 0.491 sec/batch)
2018-03-22 19:04:24.556426: step 53760, loss = 0.67 (254.7 examples/sec; 0.502 sec/batch)
2018-03-22 19:04:29.190238: step 53770, loss = 0.73 (276.2 examples/sec; 0.463 sec/batch)
2018-03-22 19:04:34.295577: step 53780, loss = 0.87 (250.7 examples/sec; 0.511 sec/batch)
2018-03-22 19:04:39.047455: step 53790, loss = 0.80 (269.4 examples/sec; 0.475 sec/batch)
2018-03-22 19:04:44.167700: step 53800, loss = 0.68 (250.0 examples/sec; 0.512 sec/batch)
2018-03-22 19:04:48.963441: step 53810, loss = 0.88 (266.9 examples/sec; 0.480 sec/batch)
2018-03-22 19:04:53.878337: step 53820, loss = 0.79 (260.4 examples/sec; 0.491 sec/batch)
2018-03-22 19:04:58.168449: step 53830, loss = 0.81 (298.4 examples/sec; 0.429 sec/batch)
2018-03-22 19:05:02.953368: step 53840, loss = 0.66 (267.5 examples/sec; 0.478 sec/batch)
2018-03-22 19:05:07.619769: step 53850, loss = 0.84 (274.3 examples/sec; 0.467 sec/batch)
2018-03-22 19:05:12.689381: step 53860, loss = 0.58 (252.5 examples/sec; 0.507 sec/batch)
2018-03-22 19:05:17.261334: step 53870, loss = 0.69 (280.0 examples/sec; 0.457 sec/batch)
2018-03-22 19:05:22.502333: step 53880, loss = 0.72 (244.2 examples/sec; 0.524 sec/batch)
2018-03-22 19:05:27.224332: step 53890, loss = 0.63 (271.1 examples/sec; 0.472 sec/batch)
2018-03-22 19:05:32.238935: step 53900, loss = 0.72 (255.3 examples/sec; 0.501 sec/batch)
2018-03-22 19:05:37.029876: step 53910, loss = 0.69 (267.2 examples/sec; 0.479 sec/batch)
2018-03-22 19:05:41.877717: step 53920, loss = 0.75 (264.0 examples/sec; 0.485 sec/batch)
2018-03-22 19:05:46.734455: step 53930, loss = 0.56 (263.6 examples/sec; 0.486 sec/batch)
2018-03-22 19:05:51.519589: step 53940, loss = 0.74 (267.5 examples/sec; 0.479 sec/batch)
2018-03-22 19:05:55.929151: step 53950, loss = 0.95 (290.3 examples/sec; 0.441 sec/batch)
2018-03-22 19:06:00.850386: step 53960, loss = 0.72 (260.1 examples/sec; 0.492 sec/batch)
2018-03-22 19:06:05.781358: step 53970, loss = 0.80 (259.6 examples/sec; 0.493 sec/batch)
2018-03-22 19:06:10.744390: step 53980, loss = 0.60 (257.9 examples/sec; 0.496 sec/batch)
2018-03-22 19:06:15.628317: step 53990, loss = 0.73 (262.1 examples/sec; 0.488 sec/batch)
2018-03-22 19:06:20.905600: step 54000, loss = 0.66 (242.5 examples/sec; 0.528 sec/batch)
2018-03-22 19:06:25.359384: step 54010, loss = 0.93 (287.4 examples/sec; 0.445 sec/batch)
2018-03-22 19:06:30.476339: step 54020, loss = 0.72 (250.1 examples/sec; 0.512 sec/batch)
2018-03-22 19:06:35.412416: step 54030, loss = 0.74 (259.3 examples/sec; 0.494 sec/batch)
2018-03-22 19:06:40.280476: step 54040, loss = 0.81 (262.9 examples/sec; 0.487 sec/batch)
2018-03-22 19:06:44.840974: step 54050, loss = 0.81 (280.7 examples/sec; 0.456 sec/batch)
2018-03-22 19:06:49.455247: step 54060, loss = 0.89 (277.4 examples/sec; 0.461 sec/batch)
2018-03-22 19:06:53.888165: step 54070, loss = 0.89 (288.7 examples/sec; 0.443 sec/batch)
2018-03-22 19:06:58.959408: step 54080, loss = 0.75 (252.4 examples/sec; 0.507 sec/batch)
2018-03-22 19:07:04.008033: step 54090, loss = 0.71 (253.5 examples/sec; 0.505 sec/batch)
2018-03-22 19:07:08.755346: step 54100, loss = 0.81 (269.6 examples/sec; 0.475 sec/batch)
2018-03-22 19:07:13.513387: step 54110, loss = 0.64 (269.0 examples/sec; 0.476 sec/batch)
2018-03-22 19:07:18.242380: step 54120, loss = 0.76 (270.7 examples/sec; 0.473 sec/batch)
2018-03-22 19:07:23.020448: step 54130, loss = 0.85 (267.9 examples/sec; 0.478 sec/batch)
2018-03-22 19:07:27.869365: step 54140, loss = 0.69 (264.0 examples/sec; 0.485 sec/batch)
2018-03-22 19:07:32.721393: step 54150, loss = 0.76 (263.8 examples/sec; 0.485 sec/batch)
2018-03-22 19:07:37.445433: step 54160, loss = 0.73 (271.0 examples/sec; 0.472 sec/batch)
2018-03-22 19:07:42.561902: step 54170, loss = 0.70 (250.2 examples/sec; 0.512 sec/batch)
2018-03-22 19:07:47.264262: step 54180, loss = 0.81 (272.2 examples/sec; 0.470 sec/batch)
2018-03-22 19:07:52.528357: step 54190, loss = 0.88 (243.2 examples/sec; 0.526 sec/batch)
2018-03-22 19:07:57.420241: step 54200, loss = 0.73 (261.7 examples/sec; 0.489 sec/batch)
2018-03-22 19:08:02.530434: step 54210, loss = 0.73 (250.5 examples/sec; 0.511 sec/batch)
2018-03-22 19:08:07.512342: step 54220, loss = 0.63 (256.9 examples/sec; 0.498 sec/batch)
2018-03-22 19:08:12.451279: step 54230, loss = 0.75 (259.2 examples/sec; 0.494 sec/batch)
2018-03-22 19:08:17.159858: step 54240, loss = 0.74 (271.8 examples/sec; 0.471 sec/batch)
2018-03-22 19:08:22.179375: step 54250, loss = 0.64 (255.0 examples/sec; 0.502 sec/batch)
2018-03-22 19:08:27.234364: step 54260, loss = 0.73 (253.2 examples/sec; 0.505 sec/batch)
2018-03-22 19:08:32.669444: step 54270, loss = 0.75 (235.5 examples/sec; 0.544 sec/batch)
2018-03-22 19:08:37.203403: step 54280, loss = 0.58 (282.3 examples/sec; 0.453 sec/batch)
2018-03-22 19:08:42.075110: step 54290, loss = 0.66 (262.7 examples/sec; 0.487 sec/batch)
2018-03-22 19:08:46.623126: step 54300, loss = 0.63 (281.4 examples/sec; 0.455 sec/batch)
2018-03-22 19:08:50.791397: step 54310, loss = 0.84 (307.1 examples/sec; 0.417 sec/batch)
2018-03-22 19:08:55.580610: step 54320, loss = 0.68 (267.3 examples/sec; 0.479 sec/batch)
2018-03-22 19:09:00.571562: step 54330, loss = 0.82 (256.5 examples/sec; 0.499 sec/batch)
2018-03-22 19:09:05.436246: step 54340, loss = 0.62 (263.1 examples/sec; 0.486 sec/batch)
2018-03-22 19:09:10.665364: step 54350, loss = 0.73 (244.8 examples/sec; 0.523 sec/batch)
2018-03-22 19:09:15.247353: step 54360, loss = 0.79 (279.4 examples/sec; 0.458 sec/batch)
2018-03-22 19:09:20.430044: step 54370, loss = 0.65 (247.0 examples/sec; 0.518 sec/batch)
2018-03-22 19:09:24.984162: step 54380, loss = 0.60 (281.1 examples/sec; 0.455 sec/batch)
2018-03-22 19:09:29.833886: step 54390, loss = 0.88 (263.9 examples/sec; 0.485 sec/batch)
2018-03-22 19:09:34.687074: step 54400, loss = 0.62 (263.7 examples/sec; 0.485 sec/batch)
2018-03-22 19:09:39.382361: step 54410, loss = 0.77 (272.6 examples/sec; 0.470 sec/batch)
2018-03-22 19:09:44.121353: step 54420, loss = 0.65 (270.1 examples/sec; 0.474 sec/batch)
2018-03-22 19:09:48.519796: step 54430, loss = 0.70 (291.0 examples/sec; 0.440 sec/batch)
2018-03-22 19:09:53.148180: step 54440, loss = 0.89 (276.6 examples/sec; 0.463 sec/batch)
2018-03-22 19:09:57.976038: step 54450, loss = 0.72 (265.1 examples/sec; 0.483 sec/batch)
2018-03-22 19:10:02.941369: step 54460, loss = 0.72 (257.8 examples/sec; 0.497 sec/batch)
2018-03-22 19:10:07.472272: step 54470, loss = 0.78 (282.5 examples/sec; 0.453 sec/batch)
2018-03-22 19:10:12.797376: step 54480, loss = 0.91 (240.4 examples/sec; 0.533 sec/batch)
2018-03-22 19:10:16.801581: step 54490, loss = 0.76 (319.7 examples/sec; 0.400 sec/batch)
2018-03-22 19:10:21.497773: step 54500, loss = 0.73 (272.6 examples/sec; 0.470 sec/batch)
2018-03-22 19:10:26.024450: step 54510, loss = 0.76 (282.8 examples/sec; 0.453 sec/batch)
2018-03-22 19:10:31.174910: step 54520, loss = 0.86 (248.5 examples/sec; 0.515 sec/batch)
2018-03-22 19:10:35.966383: step 54530, loss = 0.73 (267.1 examples/sec; 0.479 sec/batch)
2018-03-22 19:10:40.911339: step 54540, loss = 0.73 (258.8 examples/sec; 0.494 sec/batch)
2018-03-22 19:10:45.662708: step 54550, loss = 0.70 (269.4 examples/sec; 0.475 sec/batch)
2018-03-22 19:10:50.665581: step 54560, loss = 0.80 (255.9 examples/sec; 0.500 sec/batch)
2018-03-22 19:10:55.446336: step 54570, loss = 0.57 (267.7 examples/sec; 0.478 sec/batch)
2018-03-22 19:11:00.667441: step 54580, loss = 0.69 (245.2 examples/sec; 0.522 sec/batch)
2018-03-22 19:11:05.378417: step 54590, loss = 0.61 (271.7 examples/sec; 0.471 sec/batch)
2018-03-22 19:11:10.308656: step 54600, loss = 0.69 (259.6 examples/sec; 0.493 sec/batch)
2018-03-22 19:11:15.057358: step 54610, loss = 0.71 (269.5 examples/sec; 0.475 sec/batch)
2018-03-22 19:11:20.247227: step 54620, loss = 0.67 (246.6 examples/sec; 0.519 sec/batch)
2018-03-22 19:11:25.174501: step 54630, loss = 0.82 (259.8 examples/sec; 0.493 sec/batch)
2018-03-22 19:11:30.563336: step 54640, loss = 0.85 (237.5 examples/sec; 0.539 sec/batch)
2018-03-22 19:11:35.158334: step 54650, loss = 0.75 (278.6 examples/sec; 0.459 sec/batch)
2018-03-22 19:11:40.016353: step 54660, loss = 0.83 (263.5 examples/sec; 0.486 sec/batch)
2018-03-22 19:11:44.598463: step 54670, loss = 0.62 (279.3 examples/sec; 0.458 sec/batch)
2018-03-22 19:11:49.964444: step 54680, loss = 0.70 (238.5 examples/sec; 0.537 sec/batch)
2018-03-22 19:11:54.528125: step 54690, loss = 0.72 (280.5 examples/sec; 0.456 sec/batch)
2018-03-22 19:11:59.807340: step 54700, loss = 0.62 (242.5 examples/sec; 0.528 sec/batch)
2018-03-22 19:12:04.729373: step 54710, loss = 0.69 (260.1 examples/sec; 0.492 sec/batch)
2018-03-22 19:12:09.259422: step 54720, loss = 0.78 (282.6 examples/sec; 0.453 sec/batch)
2018-03-22 19:12:14.276434: step 54730, loss = 0.70 (255.1 examples/sec; 0.502 sec/batch)
2018-03-22 19:12:18.791423: step 54740, loss = 0.71 (283.5 examples/sec; 0.451 sec/batch)
2018-03-22 19:12:23.650861: step 54750, loss = 0.80 (263.4 examples/sec; 0.486 sec/batch)
2018-03-22 19:12:28.419191: step 54760, loss = 0.70 (268.4 examples/sec; 0.477 sec/batch)
2018-03-22 19:12:33.090317: step 54770, loss = 0.86 (274.0 examples/sec; 0.467 sec/batch)
2018-03-22 19:12:37.346809: step 54780, loss = 0.64 (300.7 examples/sec; 0.426 sec/batch)
2018-03-22 19:12:41.887473: step 54790, loss = 0.84 (281.9 examples/sec; 0.454 sec/batch)
2018-03-22 19:12:46.159709: step 54800, loss = 0.79 (299.6 examples/sec; 0.427 sec/batch)
2018-03-22 19:12:51.244265: step 54810, loss = 0.82 (251.7 examples/sec; 0.508 sec/batch)
2018-03-22 19:12:55.799390: step 54820, loss = 0.84 (281.0 examples/sec; 0.456 sec/batch)
2018-03-22 19:13:00.725362: step 54830, loss = 0.85 (259.8 examples/sec; 0.493 sec/batch)
2018-03-22 19:13:05.638329: step 54840, loss = 0.67 (260.5 examples/sec; 0.491 sec/batch)
2018-03-22 19:13:10.668341: step 54850, loss = 0.71 (254.5 examples/sec; 0.503 sec/batch)
2018-03-22 19:13:15.101739: step 54860, loss = 0.72 (288.7 examples/sec; 0.443 sec/batch)
2018-03-22 19:13:19.705462: step 54870, loss = 0.68 (278.0 examples/sec; 0.460 sec/batch)
2018-03-22 19:13:24.034847: step 54880, loss = 0.71 (295.7 examples/sec; 0.433 sec/batch)
2018-03-22 19:13:28.284944: step 54890, loss = 0.94 (301.2 examples/sec; 0.425 sec/batch)
2018-03-22 19:13:33.473787: step 54900, loss = 0.63 (246.7 examples/sec; 0.519 sec/batch)
2018-03-22 19:13:38.248145: step 54910, loss = 0.68 (268.1 examples/sec; 0.477 sec/batch)
2018-03-22 19:13:42.432310: step 54920, loss = 0.76 (305.9 examples/sec; 0.418 sec/batch)
2018-03-22 19:13:46.660320: step 54930, loss = 0.84 (302.7 examples/sec; 0.423 sec/batch)
2018-03-22 19:13:51.972735: step 54940, loss = 0.69 (240.9 examples/sec; 0.531 sec/batch)
2018-03-22 19:13:56.469413: step 54950, loss = 0.68 (284.7 examples/sec; 0.450 sec/batch)
2018-03-22 19:14:01.433662: step 54960, loss = 0.81 (257.8 examples/sec; 0.496 sec/batch)
2018-03-22 19:14:06.070105: step 54970, loss = 0.57 (276.1 examples/sec; 0.464 sec/batch)
2018-03-22 19:14:11.167410: step 54980, loss = 0.70 (251.1 examples/sec; 0.510 sec/batch)
2018-03-22 19:14:15.764444: step 54990, loss = 0.78 (278.4 examples/sec; 0.460 sec/batch)
2018-03-22 19:14:21.312000: step 55000, loss = 0.77 (230.7 examples/sec; 0.555 sec/batch)
2018-03-22 19:14:26.154160: step 55010, loss = 0.62 (264.3 examples/sec; 0.484 sec/batch)
2018-03-22 19:14:30.808876: step 55020, loss = 0.75 (275.0 examples/sec; 0.465 sec/batch)
2018-03-22 19:14:35.957807: step 55030, loss = 0.76 (248.6 examples/sec; 0.515 sec/batch)
2018-03-22 19:14:40.764922: step 55040, loss = 0.80 (266.3 examples/sec; 0.481 sec/batch)
2018-03-22 19:14:45.579485: step 55050, loss = 0.64 (265.9 examples/sec; 0.481 sec/batch)
2018-03-22 19:14:50.857391: step 55060, loss = 0.74 (242.5 examples/sec; 0.528 sec/batch)
2018-03-22 19:14:55.612485: step 55070, loss = 0.68 (269.2 examples/sec; 0.476 sec/batch)
2018-03-22 19:15:00.940386: step 55080, loss = 0.65 (240.2 examples/sec; 0.533 sec/batch)
2018-03-22 19:15:05.560485: step 55090, loss = 0.68 (277.1 examples/sec; 0.462 sec/batch)
2018-03-22 19:15:10.721685: step 55100, loss = 0.76 (248.0 examples/sec; 0.516 sec/batch)
2018-03-22 19:15:15.536462: step 55110, loss = 0.79 (265.8 examples/sec; 0.481 sec/batch)
2018-03-22 19:15:20.519393: step 55120, loss = 0.77 (256.9 examples/sec; 0.498 sec/batch)
2018-03-22 19:15:25.338903: step 55130, loss = 0.69 (265.6 examples/sec; 0.482 sec/batch)
2018-03-22 19:15:30.022366: step 55140, loss = 0.76 (273.3 examples/sec; 0.468 sec/batch)
2018-03-22 19:15:34.554366: step 55150, loss = 0.66 (282.4 examples/sec; 0.453 sec/batch)
2018-03-22 19:15:39.080381: step 55160, loss = 0.81 (282.8 examples/sec; 0.453 sec/batch)
2018-03-22 19:15:44.426665: step 55170, loss = 0.74 (239.4 examples/sec; 0.535 sec/batch)
2018-03-22 19:15:49.437448: step 55180, loss = 0.92 (255.4 examples/sec; 0.501 sec/batch)
2018-03-22 19:15:54.647347: step 55190, loss = 0.63 (245.7 examples/sec; 0.521 sec/batch)
2018-03-22 19:15:59.364501: step 55200, loss = 0.63 (271.4 examples/sec; 0.472 sec/batch)
2018-03-22 19:16:04.415743: step 55210, loss = 0.67 (253.4 examples/sec; 0.505 sec/batch)
2018-03-22 19:16:09.481375: step 55220, loss = 0.79 (252.7 examples/sec; 0.507 sec/batch)
2018-03-22 19:16:14.121058: step 55230, loss = 0.88 (275.9 examples/sec; 0.464 sec/batch)
2018-03-22 19:16:19.147373: step 55240, loss = 0.67 (254.7 examples/sec; 0.503 sec/batch)
2018-03-22 19:16:24.256414: step 55250, loss = 0.68 (250.5 examples/sec; 0.511 sec/batch)
2018-03-22 19:16:28.779206: step 55260, loss = 0.66 (283.0 examples/sec; 0.452 sec/batch)
2018-03-22 19:16:33.553508: step 55270, loss = 0.65 (268.1 examples/sec; 0.477 sec/batch)
2018-03-22 19:16:37.843558: step 55280, loss = 0.83 (298.4 examples/sec; 0.429 sec/batch)
2018-03-22 19:16:42.720479: step 55290, loss = 0.76 (262.5 examples/sec; 0.488 sec/batch)
2018-03-22 19:16:47.752107: step 55300, loss = 0.93 (254.4 examples/sec; 0.503 sec/batch)
2018-03-22 19:16:53.385313: step 55310, loss = 0.74 (227.2 examples/sec; 0.563 sec/batch)
2018-03-22 19:16:58.214485: step 55320, loss = 0.78 (265.1 examples/sec; 0.483 sec/batch)
2018-03-22 19:17:03.063605: step 55330, loss = 0.74 (264.0 examples/sec; 0.485 sec/batch)
2018-03-22 19:17:07.718378: step 55340, loss = 0.76 (275.0 examples/sec; 0.465 sec/batch)
2018-03-22 19:17:12.942868: step 55350, loss = 0.72 (245.0 examples/sec; 0.522 sec/batch)
2018-03-22 19:17:17.581287: step 55360, loss = 0.67 (276.0 examples/sec; 0.464 sec/batch)
2018-03-22 19:17:22.354389: step 55370, loss = 0.62 (268.2 examples/sec; 0.477 sec/batch)
2018-03-22 19:17:26.936530: step 55380, loss = 0.64 (279.3 examples/sec; 0.458 sec/batch)
2018-03-22 19:17:31.737865: step 55390, loss = 0.63 (266.6 examples/sec; 0.480 sec/batch)
2018-03-22 19:17:36.080039: step 55400, loss = 0.81 (294.8 examples/sec; 0.434 sec/batch)
2018-03-22 19:17:41.224367: step 55410, loss = 0.73 (248.8 examples/sec; 0.514 sec/batch)
2018-03-22 19:17:45.807952: step 55420, loss = 0.79 (279.3 examples/sec; 0.458 sec/batch)
2018-03-22 19:17:50.813347: step 55430, loss = 0.72 (255.7 examples/sec; 0.501 sec/batch)
2018-03-22 19:17:55.350390: step 55440, loss = 0.69 (282.1 examples/sec; 0.454 sec/batch)
2018-03-22 19:18:00.299164: step 55450, loss = 0.65 (258.7 examples/sec; 0.495 sec/batch)
2018-03-22 19:18:04.801408: step 55460, loss = 0.78 (284.3 examples/sec; 0.450 sec/batch)
2018-03-22 19:18:09.525423: step 55470, loss = 0.72 (271.0 examples/sec; 0.472 sec/batch)
2018-03-22 19:18:14.581364: step 55480, loss = 0.73 (253.2 examples/sec; 0.506 sec/batch)
2018-03-22 19:18:19.474508: step 55490, loss = 0.71 (261.6 examples/sec; 0.489 sec/batch)
2018-03-22 19:18:24.863762: step 55500, loss = 0.88 (237.5 examples/sec; 0.539 sec/batch)
2018-03-22 19:18:29.872047: step 55510, loss = 0.68 (255.6 examples/sec; 0.501 sec/batch)
2018-03-22 19:18:34.830398: step 55520, loss = 0.67 (258.2 examples/sec; 0.496 sec/batch)
2018-03-22 19:18:39.953571: step 55530, loss = 0.69 (249.8 examples/sec; 0.512 sec/batch)
2018-03-22 19:18:44.664359: step 55540, loss = 0.66 (271.7 examples/sec; 0.471 sec/batch)
2018-03-22 19:18:49.795395: step 55550, loss = 0.81 (249.5 examples/sec; 0.513 sec/batch)
2018-03-22 19:18:54.602227: step 55560, loss = 0.64 (266.3 examples/sec; 0.481 sec/batch)
2018-03-22 19:18:59.508080: step 55570, loss = 0.67 (260.9 examples/sec; 0.491 sec/batch)
2018-03-22 19:19:04.154717: step 55580, loss = 0.73 (275.5 examples/sec; 0.465 sec/batch)
2018-03-22 19:19:08.649685: step 55590, loss = 0.75 (284.8 examples/sec; 0.449 sec/batch)
2018-03-22 19:19:14.046884: step 55600, loss = 0.73 (237.2 examples/sec; 0.540 sec/batch)
2018-03-22 19:19:18.889435: step 55610, loss = 0.65 (264.3 examples/sec; 0.484 sec/batch)
2018-03-22 19:19:23.397115: step 55620, loss = 0.74 (284.0 examples/sec; 0.451 sec/batch)
2018-03-22 19:19:28.304490: step 55630, loss = 0.70 (260.8 examples/sec; 0.491 sec/batch)
2018-03-22 19:19:33.338434: step 55640, loss = 0.70 (254.3 examples/sec; 0.503 sec/batch)
2018-03-22 19:19:38.247407: step 55650, loss = 0.89 (260.7 examples/sec; 0.491 sec/batch)
2018-03-22 19:19:43.525488: step 55660, loss = 0.77 (242.5 examples/sec; 0.528 sec/batch)
2018-03-22 19:19:48.342695: step 55670, loss = 0.84 (265.7 examples/sec; 0.482 sec/batch)
2018-03-22 19:19:53.481879: step 55680, loss = 0.73 (249.1 examples/sec; 0.514 sec/batch)
2018-03-22 19:19:57.884789: step 55690, loss = 0.67 (290.7 examples/sec; 0.440 sec/batch)
2018-03-22 19:20:03.165691: step 55700, loss = 0.77 (242.4 examples/sec; 0.528 sec/batch)
2018-03-22 19:20:07.932422: step 55710, loss = 0.79 (268.5 examples/sec; 0.477 sec/batch)
2018-03-22 19:20:12.951502: step 55720, loss = 0.64 (255.0 examples/sec; 0.502 sec/batch)
2018-03-22 19:20:17.320190: step 55730, loss = 0.82 (293.0 examples/sec; 0.437 sec/batch)
2018-03-22 19:20:21.859901: step 55740, loss = 0.79 (282.0 examples/sec; 0.454 sec/batch)
2018-03-22 19:20:26.090843: step 55750, loss = 0.59 (302.5 examples/sec; 0.423 sec/batch)
2018-03-22 19:20:31.107203: step 55760, loss = 0.78 (255.2 examples/sec; 0.502 sec/batch)
2018-03-22 19:20:35.580393: step 55770, loss = 0.80 (286.1 examples/sec; 0.447 sec/batch)
2018-03-22 19:20:40.643266: step 55780, loss = 0.71 (252.8 examples/sec; 0.506 sec/batch)
2018-03-22 19:20:45.171330: step 55790, loss = 0.77 (282.7 examples/sec; 0.453 sec/batch)
2018-03-22 19:20:50.383910: step 55800, loss = 0.74 (245.6 examples/sec; 0.521 sec/batch)
2018-03-22 19:20:54.795380: step 55810, loss = 0.82 (290.2 examples/sec; 0.441 sec/batch)
2018-03-22 19:20:59.278966: step 55820, loss = 0.77 (285.5 examples/sec; 0.448 sec/batch)
2018-03-22 19:21:03.989441: step 55830, loss = 0.83 (271.7 examples/sec; 0.471 sec/batch)
2018-03-22 19:21:08.739391: step 55840, loss = 0.67 (269.5 examples/sec; 0.475 sec/batch)
2018-03-22 19:21:13.640785: step 55850, loss = 0.78 (261.2 examples/sec; 0.490 sec/batch)
2018-03-22 19:21:17.869246: step 55860, loss = 0.58 (302.7 examples/sec; 0.423 sec/batch)
2018-03-22 19:21:22.768919: step 55870, loss = 0.82 (261.2 examples/sec; 0.490 sec/batch)
2018-03-22 19:21:27.098373: step 55880, loss = 0.94 (295.6 examples/sec; 0.433 sec/batch)
2018-03-22 19:21:32.237408: step 55890, loss = 0.76 (249.1 examples/sec; 0.514 sec/batch)
2018-03-22 19:21:37.160097: step 55900, loss = 0.68 (260.0 examples/sec; 0.492 sec/batch)
2018-03-22 19:21:42.177734: step 55910, loss = 0.58 (255.1 examples/sec; 0.502 sec/batch)
2018-03-22 19:21:47.013403: step 55920, loss = 0.71 (264.7 examples/sec; 0.484 sec/batch)
2018-03-22 19:21:51.349432: step 55930, loss = 0.84 (295.2 examples/sec; 0.434 sec/batch)
2018-03-22 19:21:56.017418: step 55940, loss = 0.63 (274.2 examples/sec; 0.467 sec/batch)
2018-03-22 19:22:01.380944: step 55950, loss = 0.62 (238.6 examples/sec; 0.536 sec/batch)
2018-03-22 19:22:06.147088: step 55960, loss = 0.68 (268.6 examples/sec; 0.477 sec/batch)
2018-03-22 19:22:11.325102: step 55970, loss = 0.73 (247.2 examples/sec; 0.518 sec/batch)
2018-03-22 19:22:16.243428: step 55980, loss = 0.86 (260.3 examples/sec; 0.492 sec/batch)
2018-03-22 19:22:21.396449: step 55990, loss = 0.76 (248.4 examples/sec; 0.515 sec/batch)
2018-03-22 19:22:26.163151: step 56000, loss = 0.69 (268.5 examples/sec; 0.477 sec/batch)
2018-03-22 19:22:31.636505: step 56010, loss = 0.85 (233.9 examples/sec; 0.547 sec/batch)
2018-03-22 19:22:36.123422: step 56020, loss = 0.75 (285.3 examples/sec; 0.449 sec/batch)
2018-03-22 19:22:41.263009: step 56030, loss = 0.82 (249.0 examples/sec; 0.514 sec/batch)
2018-03-22 19:22:46.016414: step 56040, loss = 0.81 (269.3 examples/sec; 0.475 sec/batch)
2018-03-22 19:22:51.104826: step 56050, loss = 0.73 (251.6 examples/sec; 0.509 sec/batch)
2018-03-22 19:22:56.136386: step 56060, loss = 0.61 (254.4 examples/sec; 0.503 sec/batch)
2018-03-22 19:23:01.617308: step 56070, loss = 0.61 (233.5 examples/sec; 0.548 sec/batch)
2018-03-22 19:23:06.539948: step 56080, loss = 0.73 (260.0 examples/sec; 0.492 sec/batch)
2018-03-22 19:23:11.888486: step 56090, loss = 0.92 (239.3 examples/sec; 0.535 sec/batch)
2018-03-22 19:23:16.495009: step 56100, loss = 0.65 (277.9 examples/sec; 0.461 sec/batch)
2018-03-22 19:23:21.661487: step 56110, loss = 0.69 (247.8 examples/sec; 0.517 sec/batch)
2018-03-22 19:23:26.454262: step 56120, loss = 0.75 (267.1 examples/sec; 0.479 sec/batch)
2018-03-22 19:23:31.254380: step 56130, loss = 0.95 (266.7 examples/sec; 0.480 sec/batch)
2018-03-22 19:23:35.631035: step 56140, loss = 0.64 (292.5 examples/sec; 0.438 sec/batch)
2018-03-22 19:23:40.498394: step 56150, loss = 0.91 (263.0 examples/sec; 0.487 sec/batch)
2018-03-22 19:23:44.838389: step 56160, loss = 0.71 (294.9 examples/sec; 0.434 sec/batch)
2018-03-22 19:23:49.937611: step 56170, loss = 0.79 (251.0 examples/sec; 0.510 sec/batch)
2018-03-22 19:23:54.560618: step 56180, loss = 0.58 (276.9 examples/sec; 0.462 sec/batch)
2018-03-22 19:23:59.552917: step 56190, loss = 0.82 (256.4 examples/sec; 0.499 sec/batch)
2018-03-22 19:24:04.745644: step 56200, loss = 0.67 (246.5 examples/sec; 0.519 sec/batch)
2018-03-22 19:24:09.159011: step 56210, loss = 0.64 (290.0 examples/sec; 0.441 sec/batch)
2018-03-22 19:24:13.317629: step 56220, loss = 0.65 (307.8 examples/sec; 0.416 sec/batch)
2018-03-22 19:24:17.438263: step 56230, loss = 0.86 (310.6 examples/sec; 0.412 sec/batch)
2018-03-22 19:24:22.312807: step 56240, loss = 0.67 (262.6 examples/sec; 0.487 sec/batch)
2018-03-22 19:24:26.852421: step 56250, loss = 0.71 (282.0 examples/sec; 0.454 sec/batch)
2018-03-22 19:24:31.810008: step 56260, loss = 0.75 (258.2 examples/sec; 0.496 sec/batch)
2018-03-22 19:24:36.165452: step 56270, loss = 0.98 (293.9 examples/sec; 0.436 sec/batch)
2018-03-22 19:24:41.135375: step 56280, loss = 0.56 (257.5 examples/sec; 0.497 sec/batch)
2018-03-22 19:24:45.986744: step 56290, loss = 0.69 (263.8 examples/sec; 0.485 sec/batch)
2018-03-22 19:24:51.073553: step 56300, loss = 0.75 (251.6 examples/sec; 0.509 sec/batch)
2018-03-22 19:24:55.656283: step 56310, loss = 0.79 (279.3 examples/sec; 0.458 sec/batch)
2018-03-22 19:25:00.609372: step 56320, loss = 0.71 (258.4 examples/sec; 0.495 sec/batch)
2018-03-22 19:25:05.120488: step 56330, loss = 0.60 (283.7 examples/sec; 0.451 sec/batch)
2018-03-22 19:25:10.297917: step 56340, loss = 0.75 (247.2 examples/sec; 0.518 sec/batch)
2018-03-22 19:25:14.837210: step 56350, loss = 0.70 (282.0 examples/sec; 0.454 sec/batch)
2018-03-22 19:25:19.349458: step 56360, loss = 0.68 (283.7 examples/sec; 0.451 sec/batch)
2018-03-22 19:25:24.379476: step 56370, loss = 0.81 (254.5 examples/sec; 0.503 sec/batch)
2018-03-22 19:25:29.246393: step 56380, loss = 0.77 (263.0 examples/sec; 0.487 sec/batch)
2018-03-22 19:25:34.075222: step 56390, loss = 0.67 (265.1 examples/sec; 0.483 sec/batch)
2018-03-22 19:25:39.240006: step 56400, loss = 0.75 (247.8 examples/sec; 0.516 sec/batch)
2018-03-22 19:25:43.890419: step 56410, loss = 0.88 (275.2 examples/sec; 0.465 sec/batch)
2018-03-22 19:25:48.427343: step 56420, loss = 0.88 (282.1 examples/sec; 0.454 sec/batch)
2018-03-22 19:25:53.644707: step 56430, loss = 0.83 (245.3 examples/sec; 0.522 sec/batch)
2018-03-22 19:25:58.445377: step 56440, loss = 0.80 (266.6 examples/sec; 0.480 sec/batch)
2018-03-22 19:26:03.599453: step 56450, loss = 0.79 (248.3 examples/sec; 0.515 sec/batch)
2018-03-22 19:26:08.219194: step 56460, loss = 0.78 (277.1 examples/sec; 0.462 sec/batch)
2018-03-22 19:26:13.421362: step 56470, loss = 0.83 (246.1 examples/sec; 0.520 sec/batch)
2018-03-22 19:26:18.167366: step 56480, loss = 0.71 (269.7 examples/sec; 0.475 sec/batch)
2018-03-22 19:26:23.093422: step 56490, loss = 0.80 (259.8 examples/sec; 0.493 sec/batch)
2018-03-22 19:26:28.031770: step 56500, loss = 0.60 (259.2 examples/sec; 0.494 sec/batch)
2018-03-22 19:26:32.980415: step 56510, loss = 0.75 (258.7 examples/sec; 0.495 sec/batch)
2018-03-22 19:26:37.715382: step 56520, loss = 0.78 (270.3 examples/sec; 0.473 sec/batch)
2018-03-22 19:26:42.646482: step 56530, loss = 0.74 (259.6 examples/sec; 0.493 sec/batch)
2018-03-22 19:26:47.811436: step 56540, loss = 0.70 (247.8 examples/sec; 0.516 sec/batch)
2018-03-22 19:26:52.952102: step 56550, loss = 0.87 (249.0 examples/sec; 0.514 sec/batch)
2018-03-22 19:26:57.640281: step 56560, loss = 0.74 (273.0 examples/sec; 0.469 sec/batch)
2018-03-22 19:27:02.490327: step 56570, loss = 0.69 (263.9 examples/sec; 0.485 sec/batch)
2018-03-22 19:27:07.270369: step 56580, loss = 0.75 (267.8 examples/sec; 0.478 sec/batch)
2018-03-22 19:27:12.359319: step 56590, loss = 0.71 (251.5 examples/sec; 0.509 sec/batch)
2018-03-22 19:27:17.273357: step 56600, loss = 0.69 (260.5 examples/sec; 0.491 sec/batch)
2018-03-22 19:27:22.330276: step 56610, loss = 0.61 (253.1 examples/sec; 0.506 sec/batch)
2018-03-22 19:27:26.625564: step 56620, loss = 0.79 (298.0 examples/sec; 0.430 sec/batch)
2018-03-22 19:27:31.527634: step 56630, loss = 0.81 (261.1 examples/sec; 0.490 sec/batch)
2018-03-22 19:27:36.161456: step 56640, loss = 0.77 (276.2 examples/sec; 0.463 sec/batch)
2018-03-22 19:27:40.846455: step 56650, loss = 0.87 (273.2 examples/sec; 0.469 sec/batch)
2018-03-22 19:27:45.441418: step 56660, loss = 0.82 (278.6 examples/sec; 0.459 sec/batch)
2018-03-22 19:27:50.470440: step 56670, loss = 0.83 (254.5 examples/sec; 0.503 sec/batch)
2018-03-22 19:27:55.100167: step 56680, loss = 0.80 (276.5 examples/sec; 0.463 sec/batch)
2018-03-22 19:27:59.993585: step 56690, loss = 0.74 (261.6 examples/sec; 0.489 sec/batch)
2018-03-22 19:28:04.915879: step 56700, loss = 0.97 (260.0 examples/sec; 0.492 sec/batch)
2018-03-22 19:28:09.868594: step 56710, loss = 0.83 (258.4 examples/sec; 0.495 sec/batch)
2018-03-22 19:28:14.988443: step 56720, loss = 0.76 (250.0 examples/sec; 0.512 sec/batch)
2018-03-22 19:28:20.159391: step 56730, loss = 0.58 (247.5 examples/sec; 0.517 sec/batch)
2018-03-22 19:28:24.947485: step 56740, loss = 0.66 (267.3 examples/sec; 0.479 sec/batch)
2018-03-22 19:28:30.098334: step 56750, loss = 0.85 (248.5 examples/sec; 0.515 sec/batch)
2018-03-22 19:28:34.816499: step 56760, loss = 0.67 (271.3 examples/sec; 0.472 sec/batch)
2018-03-22 19:28:40.153174: step 56770, loss = 0.70 (239.8 examples/sec; 0.534 sec/batch)
2018-03-22 19:28:44.798889: step 56780, loss = 0.54 (275.5 examples/sec; 0.465 sec/batch)
2018-03-22 19:28:49.777782: step 56790, loss = 0.87 (257.1 examples/sec; 0.498 sec/batch)
2018-03-22 19:28:54.669467: step 56800, loss = 0.73 (261.7 examples/sec; 0.489 sec/batch)
2018-03-22 19:28:59.333844: step 56810, loss = 0.80 (274.4 examples/sec; 0.466 sec/batch)
2018-03-22 19:29:03.717442: step 56820, loss = 0.73 (292.0 examples/sec; 0.438 sec/batch)
2018-03-22 19:29:08.008496: step 56830, loss = 0.78 (298.3 examples/sec; 0.429 sec/batch)
2018-03-22 19:29:13.158337: step 56840, loss = 0.69 (248.6 examples/sec; 0.515 sec/batch)
2018-03-22 19:29:18.169380: step 56850, loss = 0.84 (255.4 examples/sec; 0.501 sec/batch)
2018-03-22 19:29:23.245389: step 56860, loss = 0.65 (252.2 examples/sec; 0.508 sec/batch)
2018-03-22 19:29:27.699039: step 56870, loss = 0.75 (287.4 examples/sec; 0.445 sec/batch)
2018-03-22 19:29:31.986234: step 56880, loss = 0.80 (298.6 examples/sec; 0.429 sec/batch)
2018-03-22 19:29:36.567413: step 56890, loss = 0.68 (279.4 examples/sec; 0.458 sec/batch)
2018-03-22 19:29:42.184164: step 56900, loss = 0.61 (227.9 examples/sec; 0.562 sec/batch)
2018-03-22 19:29:46.943365: step 56910, loss = 0.57 (269.0 examples/sec; 0.476 sec/batch)
2018-03-22 19:29:52.185426: step 56920, loss = 0.80 (244.2 examples/sec; 0.524 sec/batch)
2018-03-22 19:29:57.109574: step 56930, loss = 0.68 (259.9 examples/sec; 0.492 sec/batch)
2018-03-22 19:30:02.011917: step 56940, loss = 0.68 (261.1 examples/sec; 0.490 sec/batch)
2018-03-22 19:30:07.024438: step 56950, loss = 0.69 (255.4 examples/sec; 0.501 sec/batch)
2018-03-22 19:30:12.282414: step 56960, loss = 0.77 (243.4 examples/sec; 0.526 sec/batch)
2018-03-22 19:30:17.079492: step 56970, loss = 0.77 (266.8 examples/sec; 0.480 sec/batch)
2018-03-22 19:30:22.245408: step 56980, loss = 0.80 (247.8 examples/sec; 0.517 sec/batch)
2018-03-22 19:30:26.941944: step 56990, loss = 0.95 (272.5 examples/sec; 0.470 sec/batch)
2018-03-22 19:30:32.328256: step 57000, loss = 0.71 (237.6 examples/sec; 0.539 sec/batch)
2018-03-22 19:30:36.784408: step 57010, loss = 0.76 (287.2 examples/sec; 0.446 sec/batch)
2018-03-22 19:30:41.336027: step 57020, loss = 0.64 (281.2 examples/sec; 0.455 sec/batch)
2018-03-22 19:30:46.304433: step 57030, loss = 0.91 (257.6 examples/sec; 0.497 sec/batch)
2018-03-22 19:30:50.926406: step 57040, loss = 0.68 (276.9 examples/sec; 0.462 sec/batch)
2018-03-22 19:30:55.562681: step 57050, loss = 0.67 (276.1 examples/sec; 0.464 sec/batch)
2018-03-22 19:31:00.340264: step 57060, loss = 0.68 (267.9 examples/sec; 0.478 sec/batch)
2018-03-22 19:31:04.985402: step 57070, loss = 0.54 (275.6 examples/sec; 0.465 sec/batch)
2018-03-22 19:31:10.564426: step 57080, loss = 0.81 (229.4 examples/sec; 0.558 sec/batch)
2018-03-22 19:31:15.128435: step 57090, loss = 0.82 (280.5 examples/sec; 0.456 sec/batch)
2018-03-22 19:31:20.339057: step 57100, loss = 0.77 (245.7 examples/sec; 0.521 sec/batch)
2018-03-22 19:31:24.987354: step 57110, loss = 0.65 (275.4 examples/sec; 0.465 sec/batch)
2018-03-22 19:31:29.770131: step 57120, loss = 0.70 (267.6 examples/sec; 0.478 sec/batch)
2018-03-22 19:31:34.411268: step 57130, loss = 0.61 (275.8 examples/sec; 0.464 sec/batch)
2018-03-22 19:31:39.428685: step 57140, loss = 0.78 (255.1 examples/sec; 0.502 sec/batch)
2018-03-22 19:31:44.239376: step 57150, loss = 0.61 (266.1 examples/sec; 0.481 sec/batch)
2018-03-22 19:31:49.251971: step 57160, loss = 0.65 (255.4 examples/sec; 0.501 sec/batch)
2018-03-22 19:31:53.845787: step 57170, loss = 0.84 (278.6 examples/sec; 0.459 sec/batch)
2018-03-22 19:31:58.587265: step 57180, loss = 0.59 (270.0 examples/sec; 0.474 sec/batch)
2018-03-22 19:32:04.079370: step 57190, loss = 0.73 (233.1 examples/sec; 0.549 sec/batch)
2018-03-22 19:32:09.771306: step 57200, loss = 0.73 (224.9 examples/sec; 0.569 sec/batch)
2018-03-22 19:32:14.854506: step 57210, loss = 0.73 (251.8 examples/sec; 0.508 sec/batch)
2018-03-22 19:32:19.566083: step 57220, loss = 0.85 (271.7 examples/sec; 0.471 sec/batch)
2018-03-22 19:32:24.111441: step 57230, loss = 0.66 (281.6 examples/sec; 0.455 sec/batch)
2018-03-22 19:32:28.677464: step 57240, loss = 0.83 (280.3 examples/sec; 0.457 sec/batch)
2018-03-22 19:32:33.235375: step 57250, loss = 0.61 (280.8 examples/sec; 0.456 sec/batch)
2018-03-22 19:32:37.875397: step 57260, loss = 0.66 (275.9 examples/sec; 0.464 sec/batch)
2018-03-22 19:32:43.056407: step 57270, loss = 0.70 (247.1 examples/sec; 0.518 sec/batch)
2018-03-22 19:32:47.067695: step 57280, loss = 0.60 (319.1 examples/sec; 0.401 sec/batch)
2018-03-22 19:32:52.338427: step 57290, loss = 0.80 (242.9 examples/sec; 0.527 sec/batch)
2018-03-22 19:32:57.198845: step 57300, loss = 0.74 (263.4 examples/sec; 0.486 sec/batch)
2018-03-22 19:33:02.377566: step 57310, loss = 0.91 (247.2 examples/sec; 0.518 sec/batch)
2018-03-22 19:33:07.068390: step 57320, loss = 0.80 (272.9 examples/sec; 0.469 sec/batch)
2018-03-22 19:33:12.023393: step 57330, loss = 0.71 (258.3 examples/sec; 0.495 sec/batch)
2018-03-22 19:33:16.151397: step 57340, loss = 0.73 (310.1 examples/sec; 0.413 sec/batch)
2018-03-22 19:33:21.200491: step 57350, loss = 0.79 (253.5 examples/sec; 0.505 sec/batch)
2018-03-22 19:33:25.690440: step 57360, loss = 0.63 (285.1 examples/sec; 0.449 sec/batch)
2018-03-22 19:33:31.202451: step 57370, loss = 0.53 (232.2 examples/sec; 0.551 sec/batch)
2018-03-22 19:33:36.077823: step 57380, loss = 0.72 (262.5 examples/sec; 0.488 sec/batch)
2018-03-22 19:33:40.971136: step 57390, loss = 0.55 (261.6 examples/sec; 0.489 sec/batch)
2018-03-22 19:33:45.859987: step 57400, loss = 0.80 (261.8 examples/sec; 0.489 sec/batch)
2018-03-22 19:33:51.043875: step 57410, loss = 0.74 (246.9 examples/sec; 0.518 sec/batch)
2018-03-22 19:33:55.397643: step 57420, loss = 0.78 (294.0 examples/sec; 0.435 sec/batch)
2018-03-22 19:34:00.731409: step 57430, loss = 0.71 (240.0 examples/sec; 0.533 sec/batch)
2018-03-22 19:34:05.168366: step 57440, loss = 0.75 (288.5 examples/sec; 0.444 sec/batch)
2018-03-22 19:34:10.280827: step 57450, loss = 0.66 (250.4 examples/sec; 0.511 sec/batch)
2018-03-22 19:34:15.056976: step 57460, loss = 0.90 (268.0 examples/sec; 0.478 sec/batch)
2018-03-22 19:34:20.187821: step 57470, loss = 0.70 (249.5 examples/sec; 0.513 sec/batch)
2018-03-22 19:34:25.026482: step 57480, loss = 0.85 (264.5 examples/sec; 0.484 sec/batch)
2018-03-22 19:34:29.644372: step 57490, loss = 0.68 (277.2 examples/sec; 0.462 sec/batch)
2018-03-22 19:34:34.251072: step 57500, loss = 0.58 (277.9 examples/sec; 0.461 sec/batch)
2018-03-22 19:34:39.044423: step 57510, loss = 0.74 (267.0 examples/sec; 0.479 sec/batch)
2018-03-22 19:34:43.767923: step 57520, loss = 0.69 (271.0 examples/sec; 0.472 sec/batch)
2018-03-22 19:34:48.454461: step 57530, loss = 0.86 (273.1 examples/sec; 0.469 sec/batch)
2018-03-22 19:34:53.492816: step 57540, loss = 0.73 (254.1 examples/sec; 0.504 sec/batch)
2018-03-22 19:34:58.515392: step 57550, loss = 0.70 (254.8 examples/sec; 0.502 sec/batch)
2018-03-22 19:35:03.757404: step 57560, loss = 0.78 (244.2 examples/sec; 0.524 sec/batch)
2018-03-22 19:35:08.305655: step 57570, loss = 0.70 (281.4 examples/sec; 0.455 sec/batch)
2018-03-22 19:35:13.457434: step 57580, loss = 0.77 (248.5 examples/sec; 0.515 sec/batch)
2018-03-22 19:35:17.894026: step 57590, loss = 0.77 (288.5 examples/sec; 0.444 sec/batch)
2018-03-22 19:35:22.981784: step 57600, loss = 0.84 (251.6 examples/sec; 0.509 sec/batch)
2018-03-22 19:35:27.847350: step 57610, loss = 0.91 (263.1 examples/sec; 0.487 sec/batch)
2018-03-22 19:35:32.705505: step 57620, loss = 0.79 (263.5 examples/sec; 0.486 sec/batch)
2018-03-22 19:35:37.266344: step 57630, loss = 0.76 (280.6 examples/sec; 0.456 sec/batch)
2018-03-22 19:35:42.420474: step 57640, loss = 0.71 (248.3 examples/sec; 0.515 sec/batch)
2018-03-22 19:35:46.929928: step 57650, loss = 0.64 (283.8 examples/sec; 0.451 sec/batch)
2018-03-22 19:35:52.162363: step 57660, loss = 0.79 (244.6 examples/sec; 0.523 sec/batch)
2018-03-22 19:35:56.656371: step 57670, loss = 0.78 (284.8 examples/sec; 0.449 sec/batch)
2018-03-22 19:36:01.841927: step 57680, loss = 0.71 (246.8 examples/sec; 0.519 sec/batch)
2018-03-22 19:36:06.919455: step 57690, loss = 0.63 (252.1 examples/sec; 0.508 sec/batch)
2018-03-22 19:36:12.038248: step 57700, loss = 0.68 (250.1 examples/sec; 0.512 sec/batch)
2018-03-22 19:36:16.945434: step 57710, loss = 0.67 (260.8 examples/sec; 0.491 sec/batch)
2018-03-22 19:36:22.279332: step 57720, loss = 0.84 (240.0 examples/sec; 0.533 sec/batch)
2018-03-22 19:36:26.628446: step 57730, loss = 0.73 (294.3 examples/sec; 0.435 sec/batch)
2018-03-22 19:36:31.799776: step 57740, loss = 0.77 (247.5 examples/sec; 0.517 sec/batch)
2018-03-22 19:36:36.057649: step 57750, loss = 0.66 (300.6 examples/sec; 0.426 sec/batch)
2018-03-22 19:36:40.583440: step 57760, loss = 0.84 (282.8 examples/sec; 0.453 sec/batch)
2018-03-22 19:36:45.475357: step 57770, loss = 0.78 (261.7 examples/sec; 0.489 sec/batch)
2018-03-22 19:36:50.918520: step 57780, loss = 0.83 (235.2 examples/sec; 0.544 sec/batch)
2018-03-22 19:36:55.708434: step 57790, loss = 0.73 (267.2 examples/sec; 0.479 sec/batch)
2018-03-22 19:37:01.230972: step 57800, loss = 0.77 (231.8 examples/sec; 0.552 sec/batch)
2018-03-22 19:37:05.445936: step 57810, loss = 0.76 (303.7 examples/sec; 0.421 sec/batch)
2018-03-22 19:37:10.462440: step 57820, loss = 0.69 (255.2 examples/sec; 0.502 sec/batch)
2018-03-22 19:37:15.304427: step 57830, loss = 0.66 (264.4 examples/sec; 0.484 sec/batch)
2018-03-22 19:37:20.310443: step 57840, loss = 0.69 (255.7 examples/sec; 0.501 sec/batch)
2018-03-22 19:37:24.952614: step 57850, loss = 0.70 (275.7 examples/sec; 0.464 sec/batch)
2018-03-22 19:37:30.071963: step 57860, loss = 0.79 (250.0 examples/sec; 0.512 sec/batch)
2018-03-22 19:37:34.685257: step 57870, loss = 0.68 (277.5 examples/sec; 0.461 sec/batch)
2018-03-22 19:37:39.608176: step 57880, loss = 0.78 (260.0 examples/sec; 0.492 sec/batch)
2018-03-22 19:37:44.497510: step 57890, loss = 0.63 (261.8 examples/sec; 0.489 sec/batch)
2018-03-22 19:37:49.855444: step 57900, loss = 0.82 (238.9 examples/sec; 0.536 sec/batch)
2018-03-22 19:37:54.521327: step 57910, loss = 0.74 (274.3 examples/sec; 0.467 sec/batch)
2018-03-22 19:37:59.195877: step 57920, loss = 0.77 (273.8 examples/sec; 0.467 sec/batch)
2018-03-22 19:38:04.058399: step 57930, loss = 0.54 (263.2 examples/sec; 0.486 sec/batch)
2018-03-22 19:38:08.714411: step 57940, loss = 0.68 (274.9 examples/sec; 0.466 sec/batch)
2018-03-22 19:38:13.802471: step 57950, loss = 0.70 (251.6 examples/sec; 0.509 sec/batch)
2018-03-22 19:38:18.698342: step 57960, loss = 0.75 (261.4 examples/sec; 0.490 sec/batch)
2018-03-22 19:38:23.651264: step 57970, loss = 0.71 (258.4 examples/sec; 0.495 sec/batch)
2018-03-22 19:38:28.395460: step 57980, loss = 0.75 (269.8 examples/sec; 0.474 sec/batch)
2018-03-22 19:38:33.075883: step 57990, loss = 0.74 (273.5 examples/sec; 0.468 sec/batch)
2018-03-22 19:38:38.105159: step 58000, loss = 0.64 (254.5 examples/sec; 0.503 sec/batch)
2018-03-22 19:38:43.272378: step 58010, loss = 0.71 (247.7 examples/sec; 0.517 sec/batch)
2018-03-22 19:38:47.897290: step 58020, loss = 0.78 (276.8 examples/sec; 0.462 sec/batch)
2018-03-22 19:38:52.894137: step 58030, loss = 0.63 (256.2 examples/sec; 0.500 sec/batch)
2018-03-22 19:38:57.440849: step 58040, loss = 0.80 (281.6 examples/sec; 0.455 sec/batch)
2018-03-22 19:39:02.610663: step 58050, loss = 0.75 (247.5 examples/sec; 0.517 sec/batch)
2018-03-22 19:39:07.113223: step 58060, loss = 0.71 (284.3 examples/sec; 0.450 sec/batch)
2018-03-22 19:39:11.838371: step 58070, loss = 0.80 (270.9 examples/sec; 0.473 sec/batch)
2018-03-22 19:39:16.163622: step 58080, loss = 0.74 (295.9 examples/sec; 0.433 sec/batch)
2018-03-22 19:39:21.572488: step 58090, loss = 0.76 (236.6 examples/sec; 0.541 sec/batch)
2018-03-22 19:39:26.759717: step 58100, loss = 0.60 (246.8 examples/sec; 0.519 sec/batch)
2018-03-22 19:39:31.694371: step 58110, loss = 0.77 (259.4 examples/sec; 0.493 sec/batch)
2018-03-22 19:39:36.748424: step 58120, loss = 0.76 (253.3 examples/sec; 0.505 sec/batch)
2018-03-22 19:39:41.623201: step 58130, loss = 0.79 (262.6 examples/sec; 0.487 sec/batch)
2018-03-22 19:39:46.347657: step 58140, loss = 0.81 (270.9 examples/sec; 0.472 sec/batch)
2018-03-22 19:39:51.241191: step 58150, loss = 0.79 (261.6 examples/sec; 0.489 sec/batch)
2018-03-22 19:39:55.975422: step 58160, loss = 0.83 (270.4 examples/sec; 0.473 sec/batch)
2018-03-22 19:40:00.832608: step 58170, loss = 0.73 (263.5 examples/sec; 0.486 sec/batch)
2018-03-22 19:40:05.655369: step 58180, loss = 0.71 (265.4 examples/sec; 0.482 sec/batch)
2018-03-22 19:40:10.623827: step 58190, loss = 0.68 (257.6 examples/sec; 0.497 sec/batch)
2018-03-22 19:40:15.391079: step 58200, loss = 0.78 (268.5 examples/sec; 0.477 sec/batch)
2018-03-22 19:40:19.932366: step 58210, loss = 0.65 (281.9 examples/sec; 0.454 sec/batch)
2018-03-22 19:40:24.852358: step 58220, loss = 0.65 (260.2 examples/sec; 0.492 sec/batch)
2018-03-22 19:40:29.396768: step 58230, loss = 0.70 (281.7 examples/sec; 0.454 sec/batch)
2018-03-22 19:40:33.661609: step 58240, loss = 0.71 (300.1 examples/sec; 0.426 sec/batch)
2018-03-22 19:40:38.351386: step 58250, loss = 0.72 (272.9 examples/sec; 0.469 sec/batch)
2018-03-22 19:40:43.444453: step 58260, loss = 0.75 (251.3 examples/sec; 0.509 sec/batch)
2018-03-22 19:40:47.965387: step 58270, loss = 0.74 (283.1 examples/sec; 0.452 sec/batch)
2018-03-22 19:40:53.008110: step 58280, loss = 0.68 (253.8 examples/sec; 0.504 sec/batch)
2018-03-22 19:40:57.535067: step 58290, loss = 0.89 (282.8 examples/sec; 0.453 sec/batch)
2018-03-22 19:41:02.568809: step 58300, loss = 0.68 (254.3 examples/sec; 0.503 sec/batch)
2018-03-22 19:41:06.984163: step 58310, loss = 0.78 (289.9 examples/sec; 0.442 sec/batch)
2018-03-22 19:41:11.901300: step 58320, loss = 0.64 (260.3 examples/sec; 0.492 sec/batch)
2018-03-22 19:41:16.722896: step 58330, loss = 0.67 (265.5 examples/sec; 0.482 sec/batch)
2018-03-22 19:41:21.811324: step 58340, loss = 0.63 (251.6 examples/sec; 0.509 sec/batch)
2018-03-22 19:41:26.193439: step 58350, loss = 0.73 (292.1 examples/sec; 0.438 sec/batch)
2018-03-22 19:41:30.820286: step 58360, loss = 0.59 (276.6 examples/sec; 0.463 sec/batch)
2018-03-22 19:41:35.640319: step 58370, loss = 0.60 (265.6 examples/sec; 0.482 sec/batch)
2018-03-22 19:41:40.322606: step 58380, loss = 0.73 (273.4 examples/sec; 0.468 sec/batch)
2018-03-22 19:41:45.309127: step 58390, loss = 0.97 (256.7 examples/sec; 0.499 sec/batch)
2018-03-22 19:41:51.048109: step 58400, loss = 0.65 (223.0 examples/sec; 0.574 sec/batch)
2018-03-22 19:41:55.707665: step 58410, loss = 0.74 (274.7 examples/sec; 0.466 sec/batch)
2018-03-22 19:42:00.511367: step 58420, loss = 0.55 (266.5 examples/sec; 0.480 sec/batch)
2018-03-22 19:42:05.155412: step 58430, loss = 0.76 (275.6 examples/sec; 0.464 sec/batch)
2018-03-22 19:42:09.987552: step 58440, loss = 0.74 (264.9 examples/sec; 0.483 sec/batch)
2018-03-22 19:42:14.704434: step 58450, loss = 0.61 (271.4 examples/sec; 0.472 sec/batch)
2018-03-22 19:42:19.873437: step 58460, loss = 0.78 (247.6 examples/sec; 0.517 sec/batch)
2018-03-22 19:42:24.739448: step 58470, loss = 0.66 (263.0 examples/sec; 0.487 sec/batch)
2018-03-22 19:42:29.712434: step 58480, loss = 0.75 (257.4 examples/sec; 0.497 sec/batch)
2018-03-22 19:42:34.445587: step 58490, loss = 0.65 (270.4 examples/sec; 0.473 sec/batch)
2018-03-22 19:42:39.579274: step 58500, loss = 0.67 (249.3 examples/sec; 0.513 sec/batch)
2018-03-22 19:42:44.489452: step 58510, loss = 0.86 (260.7 examples/sec; 0.491 sec/batch)
2018-03-22 19:42:49.291415: step 58520, loss = 0.75 (266.6 examples/sec; 0.480 sec/batch)
2018-03-22 19:42:54.248364: step 58530, loss = 0.92 (258.2 examples/sec; 0.496 sec/batch)
2018-03-22 19:42:58.830405: step 58540, loss = 0.69 (279.4 examples/sec; 0.458 sec/batch)
2018-03-22 19:43:03.661489: step 58550, loss = 0.78 (265.0 examples/sec; 0.483 sec/batch)
2018-03-22 19:43:08.041422: step 58560, loss = 0.79 (292.2 examples/sec; 0.438 sec/batch)
2018-03-22 19:43:13.013411: step 58570, loss = 0.74 (257.4 examples/sec; 0.497 sec/batch)
2018-03-22 19:43:17.516860: step 58580, loss = 0.62 (284.2 examples/sec; 0.450 sec/batch)
2018-03-22 19:43:22.176433: step 58590, loss = 0.72 (274.7 examples/sec; 0.466 sec/batch)
2018-03-22 19:43:26.905243: step 58600, loss = 0.65 (270.7 examples/sec; 0.473 sec/batch)
2018-03-22 19:43:32.194467: step 58610, loss = 0.82 (242.0 examples/sec; 0.529 sec/batch)
2018-03-22 19:43:36.645390: step 58620, loss = 0.68 (287.6 examples/sec; 0.445 sec/batch)
2018-03-22 19:43:41.735340: step 58630, loss = 0.84 (251.5 examples/sec; 0.509 sec/batch)
2018-03-22 19:43:46.488894: step 58640, loss = 0.71 (269.3 examples/sec; 0.475 sec/batch)
2018-03-22 19:43:51.425689: step 58650, loss = 0.78 (259.3 examples/sec; 0.494 sec/batch)
2018-03-22 19:43:56.421357: step 58660, loss = 0.61 (256.2 examples/sec; 0.500 sec/batch)
2018-03-22 19:44:01.453371: step 58670, loss = 0.78 (254.4 examples/sec; 0.503 sec/batch)
2018-03-22 19:44:06.067227: step 58680, loss = 0.70 (277.4 examples/sec; 0.461 sec/batch)
2018-03-22 19:44:11.004162: step 58690, loss = 0.71 (259.3 examples/sec; 0.494 sec/batch)
2018-03-22 19:44:15.842861: step 58700, loss = 0.83 (264.5 examples/sec; 0.484 sec/batch)
2018-03-22 19:44:20.676365: step 58710, loss = 0.75 (264.8 examples/sec; 0.483 sec/batch)
2018-03-22 19:44:24.721333: step 58720, loss = 0.80 (316.4 examples/sec; 0.404 sec/batch)
2018-03-22 19:44:29.098410: step 58730, loss = 0.72 (292.4 examples/sec; 0.438 sec/batch)
2018-03-22 19:44:33.770977: step 58740, loss = 0.60 (273.9 examples/sec; 0.467 sec/batch)
2018-03-22 19:44:38.510158: step 58750, loss = 0.88 (270.1 examples/sec; 0.474 sec/batch)
2018-03-22 19:44:43.312909: step 58760, loss = 0.78 (266.5 examples/sec; 0.480 sec/batch)
2018-03-22 19:44:48.331747: step 58770, loss = 0.57 (255.0 examples/sec; 0.502 sec/batch)
2018-03-22 19:44:53.361423: step 58780, loss = 0.69 (254.5 examples/sec; 0.503 sec/batch)
2018-03-22 19:44:57.698701: step 58790, loss = 0.55 (295.1 examples/sec; 0.434 sec/batch)
2018-03-22 19:45:02.506020: step 58800, loss = 0.67 (266.3 examples/sec; 0.481 sec/batch)
2018-03-22 19:45:07.258375: step 58810, loss = 0.66 (269.3 examples/sec; 0.475 sec/batch)
2018-03-22 19:45:12.168344: step 58820, loss = 0.73 (260.7 examples/sec; 0.491 sec/batch)
2018-03-22 19:45:17.284359: step 58830, loss = 0.81 (250.2 examples/sec; 0.512 sec/batch)
2018-03-22 19:45:22.537503: step 58840, loss = 1.03 (243.7 examples/sec; 0.525 sec/batch)
2018-03-22 19:45:26.793305: step 58850, loss = 0.65 (300.8 examples/sec; 0.426 sec/batch)
2018-03-22 19:45:31.874160: step 58860, loss = 0.77 (251.9 examples/sec; 0.508 sec/batch)
2018-03-22 19:45:36.483626: step 58870, loss = 0.68 (277.7 examples/sec; 0.461 sec/batch)
2018-03-22 19:45:41.594459: step 58880, loss = 0.61 (250.4 examples/sec; 0.511 sec/batch)
2018-03-22 19:45:45.822425: step 58890, loss = 0.81 (302.7 examples/sec; 0.423 sec/batch)
2018-03-22 19:45:51.107483: step 58900, loss = 0.68 (242.2 examples/sec; 0.529 sec/batch)
2018-03-22 19:45:55.816451: step 58910, loss = 0.64 (271.8 examples/sec; 0.471 sec/batch)
2018-03-22 19:46:00.868355: step 58920, loss = 0.87 (253.4 examples/sec; 0.505 sec/batch)
2018-03-22 19:46:05.657401: step 58930, loss = 0.56 (267.3 examples/sec; 0.479 sec/batch)
2018-03-22 19:46:10.753412: step 58940, loss = 0.73 (251.2 examples/sec; 0.510 sec/batch)
2018-03-22 19:46:15.506459: step 58950, loss = 0.72 (269.3 examples/sec; 0.475 sec/batch)
2018-03-22 19:46:20.904368: step 58960, loss = 0.84 (237.1 examples/sec; 0.540 sec/batch)
2018-03-22 19:46:25.641461: step 58970, loss = 0.91 (270.2 examples/sec; 0.474 sec/batch)
2018-03-22 19:46:30.715422: step 58980, loss = 0.71 (252.3 examples/sec; 0.507 sec/batch)
2018-03-22 19:46:35.536409: step 58990, loss = 0.55 (265.5 examples/sec; 0.482 sec/batch)
2018-03-22 19:46:41.120323: step 59000, loss = 0.64 (229.2 examples/sec; 0.558 sec/batch)
2018-03-22 19:46:45.575293: step 59010, loss = 0.63 (287.3 examples/sec; 0.445 sec/batch)
2018-03-22 19:46:50.298413: step 59020, loss = 0.72 (271.0 examples/sec; 0.472 sec/batch)
2018-03-22 19:46:54.841449: step 59030, loss = 0.61 (281.8 examples/sec; 0.454 sec/batch)
2018-03-22 19:46:59.402835: step 59040, loss = 0.75 (280.6 examples/sec; 0.456 sec/batch)
2018-03-22 19:47:04.130267: step 59050, loss = 0.60 (270.8 examples/sec; 0.473 sec/batch)
2018-03-22 19:47:08.547391: step 59060, loss = 0.78 (289.8 examples/sec; 0.442 sec/batch)
2018-03-22 19:47:13.536330: step 59070, loss = 0.80 (256.6 examples/sec; 0.499 sec/batch)
2018-03-22 19:47:18.142126: step 59080, loss = 0.81 (277.9 examples/sec; 0.461 sec/batch)
2018-03-22 19:47:23.535398: step 59090, loss = 0.73 (237.3 examples/sec; 0.539 sec/batch)
2018-03-22 19:47:28.197507: step 59100, loss = 0.70 (274.6 examples/sec; 0.466 sec/batch)
2018-03-22 19:47:33.088316: step 59110, loss = 0.63 (261.7 examples/sec; 0.489 sec/batch)
2018-03-22 19:47:38.114462: step 59120, loss = 0.85 (254.7 examples/sec; 0.503 sec/batch)
2018-03-22 19:47:43.832417: step 59130, loss = 0.63 (223.9 examples/sec; 0.572 sec/batch)
2018-03-22 19:47:48.485286: step 59140, loss = 0.80 (275.1 examples/sec; 0.465 sec/batch)
2018-03-22 19:47:53.481428: step 59150, loss = 0.73 (256.2 examples/sec; 0.500 sec/batch)
2018-03-22 19:47:58.156181: step 59160, loss = 0.78 (273.8 examples/sec; 0.467 sec/batch)
2018-03-22 19:48:03.136532: step 59170, loss = 0.68 (257.0 examples/sec; 0.498 sec/batch)
2018-03-22 19:48:07.775807: step 59180, loss = 0.95 (275.9 examples/sec; 0.464 sec/batch)
2018-03-22 19:48:12.233996: step 59190, loss = 0.52 (287.1 examples/sec; 0.446 sec/batch)
2018-03-22 19:48:16.778073: step 59200, loss = 0.64 (281.7 examples/sec; 0.454 sec/batch)
2018-03-22 19:48:21.420408: step 59210, loss = 0.88 (275.7 examples/sec; 0.464 sec/batch)
2018-03-22 19:48:25.754482: step 59220, loss = 0.76 (295.3 examples/sec; 0.433 sec/batch)
2018-03-22 19:48:30.349388: step 59230, loss = 0.74 (278.6 examples/sec; 0.459 sec/batch)
2018-03-22 19:48:34.893406: step 59240, loss = 0.62 (281.7 examples/sec; 0.454 sec/batch)
2018-03-22 19:48:40.031432: step 59250, loss = 0.77 (249.1 examples/sec; 0.514 sec/batch)
2018-03-22 19:48:44.980441: step 59260, loss = 0.73 (258.6 examples/sec; 0.495 sec/batch)
2018-03-22 19:48:49.443927: step 59270, loss = 0.74 (286.8 examples/sec; 0.446 sec/batch)
2018-03-22 19:48:54.081638: step 59280, loss = 0.81 (276.0 examples/sec; 0.464 sec/batch)
2018-03-22 19:48:58.764207: step 59290, loss = 0.72 (273.4 examples/sec; 0.468 sec/batch)
2018-03-22 19:49:04.657021: step 59300, loss = 0.73 (217.2 examples/sec; 0.589 sec/batch)
2018-03-22 19:49:10.222526: step 59310, loss = 0.68 (230.0 examples/sec; 0.557 sec/batch)
2018-03-22 19:49:14.805646: step 59320, loss = 0.64 (279.3 examples/sec; 0.458 sec/batch)
2018-03-22 19:49:19.258858: step 59330, loss = 0.95 (287.4 examples/sec; 0.445 sec/batch)
2018-03-22 19:49:23.811397: step 59340, loss = 0.69 (281.2 examples/sec; 0.455 sec/batch)
2018-03-22 19:49:28.390312: step 59350, loss = 0.68 (279.5 examples/sec; 0.458 sec/batch)
2018-03-22 19:49:33.330391: step 59360, loss = 0.73 (259.1 examples/sec; 0.494 sec/batch)
2018-03-22 19:49:38.028354: step 59370, loss = 0.69 (272.5 examples/sec; 0.470 sec/batch)
2018-03-22 19:49:42.785907: step 59380, loss = 0.76 (269.0 examples/sec; 0.476 sec/batch)
2018-03-22 19:49:47.368643: step 59390, loss = 0.66 (279.3 examples/sec; 0.458 sec/batch)
2018-03-22 19:49:52.437509: step 59400, loss = 0.69 (252.5 examples/sec; 0.507 sec/batch)
2018-03-22 19:49:56.782097: step 59410, loss = 0.63 (294.6 examples/sec; 0.434 sec/batch)
2018-03-22 19:50:01.753215: step 59420, loss = 0.70 (257.5 examples/sec; 0.497 sec/batch)
2018-03-22 19:50:06.188429: step 59430, loss = 0.50 (288.6 examples/sec; 0.444 sec/batch)
2018-03-22 19:50:11.073401: step 59440, loss = 0.80 (262.0 examples/sec; 0.488 sec/batch)
2018-03-22 19:50:15.444003: step 59450, loss = 0.83 (292.9 examples/sec; 0.437 sec/batch)
2018-03-22 19:50:20.071096: step 59460, loss = 0.76 (276.6 examples/sec; 0.463 sec/batch)
2018-03-22 19:50:24.883426: step 59470, loss = 0.76 (266.0 examples/sec; 0.481 sec/batch)
2018-03-22 19:50:30.320431: step 59480, loss = 0.82 (235.4 examples/sec; 0.544 sec/batch)
2018-03-22 19:50:35.094170: step 59490, loss = 0.65 (268.1 examples/sec; 0.477 sec/batch)
2018-03-22 19:50:40.121796: step 59500, loss = 0.74 (254.6 examples/sec; 0.503 sec/batch)
2018-03-22 19:50:44.750966: step 59510, loss = 0.67 (276.5 examples/sec; 0.463 sec/batch)
2018-03-22 19:50:49.779868: step 59520, loss = 0.83 (254.5 examples/sec; 0.503 sec/batch)
2018-03-22 19:50:54.198640: step 59530, loss = 0.66 (289.7 examples/sec; 0.442 sec/batch)
2018-03-22 19:50:59.204290: step 59540, loss = 0.79 (255.7 examples/sec; 0.501 sec/batch)
2018-03-22 19:51:04.232399: step 59550, loss = 0.69 (254.6 examples/sec; 0.503 sec/batch)
2018-03-22 19:51:09.216241: step 59560, loss = 0.96 (256.8 examples/sec; 0.498 sec/batch)
2018-03-22 19:51:14.407466: step 59570, loss = 0.71 (246.6 examples/sec; 0.519 sec/batch)
2018-03-22 19:51:19.733447: step 59580, loss = 0.82 (240.3 examples/sec; 0.533 sec/batch)
2018-03-22 19:51:24.752387: step 59590, loss = 0.82 (255.0 examples/sec; 0.502 sec/batch)
2018-03-22 19:51:29.811494: step 59600, loss = 0.73 (253.0 examples/sec; 0.506 sec/batch)
2018-03-22 19:51:34.834403: step 59610, loss = 0.68 (254.8 examples/sec; 0.502 sec/batch)
2018-03-22 19:51:39.623411: step 59620, loss = 0.79 (267.3 examples/sec; 0.479 sec/batch)
2018-03-22 19:51:44.073195: step 59630, loss = 0.68 (287.7 examples/sec; 0.445 sec/batch)
2018-03-22 19:51:48.659684: step 59640, loss = 0.60 (279.1 examples/sec; 0.459 sec/batch)
2018-03-22 19:51:53.690305: step 59650, loss = 0.73 (254.4 examples/sec; 0.503 sec/batch)
2018-03-22 19:51:58.355659: step 59660, loss = 0.71 (274.4 examples/sec; 0.467 sec/batch)
2018-03-22 19:52:03.466239: step 59670, loss = 0.63 (250.5 examples/sec; 0.511 sec/batch)
2018-03-22 19:52:07.903368: step 59680, loss = 0.82 (288.5 examples/sec; 0.444 sec/batch)
2018-03-22 19:52:12.499960: step 59690, loss = 0.71 (278.5 examples/sec; 0.460 sec/batch)
2018-03-22 19:52:17.279295: step 59700, loss = 0.68 (267.8 examples/sec; 0.478 sec/batch)
2018-03-22 19:52:22.293393: step 59710, loss = 0.85 (255.3 examples/sec; 0.501 sec/batch)
2018-03-22 19:52:27.183515: step 59720, loss = 0.73 (261.8 examples/sec; 0.489 sec/batch)
2018-03-22 19:52:32.666348: step 59730, loss = 0.67 (233.5 examples/sec; 0.548 sec/batch)
2018-03-22 19:52:37.187364: step 59740, loss = 0.77 (283.1 examples/sec; 0.452 sec/batch)
2018-03-22 19:52:42.460430: step 59750, loss = 0.69 (242.7 examples/sec; 0.527 sec/batch)
2018-03-22 19:52:46.700380: step 59760, loss = 0.87 (301.9 examples/sec; 0.424 sec/batch)
2018-03-22 19:52:52.088380: step 59770, loss = 0.73 (237.6 examples/sec; 0.539 sec/batch)
2018-03-22 19:52:56.996803: step 59780, loss = 0.77 (260.8 examples/sec; 0.491 sec/batch)
2018-03-22 19:53:01.868675: step 59790, loss = 0.77 (262.7 examples/sec; 0.487 sec/batch)
2018-03-22 19:53:06.927386: step 59800, loss = 0.78 (253.0 examples/sec; 0.506 sec/batch)
2018-03-22 19:53:11.722237: step 59810, loss = 0.84 (267.0 examples/sec; 0.479 sec/batch)
2018-03-22 19:53:16.409323: step 59820, loss = 0.71 (273.1 examples/sec; 0.469 sec/batch)
2018-03-22 19:53:21.118749: step 59830, loss = 0.58 (271.8 examples/sec; 0.471 sec/batch)
2018-03-22 19:53:25.801449: step 59840, loss = 0.79 (273.3 examples/sec; 0.468 sec/batch)
2018-03-22 19:53:30.401474: step 59850, loss = 0.64 (278.3 examples/sec; 0.460 sec/batch)
2018-03-22 19:53:35.206786: step 59860, loss = 0.77 (266.4 examples/sec; 0.481 sec/batch)
2018-03-22 19:53:40.092882: step 59870, loss = 0.71 (262.0 examples/sec; 0.489 sec/batch)
2018-03-22 19:53:44.771426: step 59880, loss = 0.63 (273.6 examples/sec; 0.468 sec/batch)
2018-03-22 19:53:49.930487: step 59890, loss = 0.74 (248.1 examples/sec; 0.516 sec/batch)
2018-03-22 19:53:55.049745: step 59900, loss = 0.64 (250.0 examples/sec; 0.512 sec/batch)
2018-03-22 19:54:00.002017: step 59910, loss = 0.79 (258.5 examples/sec; 0.495 sec/batch)
2018-03-22 19:54:04.933829: step 59920, loss = 0.79 (259.5 examples/sec; 0.493 sec/batch)
2018-03-22 19:54:09.921029: step 59930, loss = 0.75 (256.7 examples/sec; 0.499 sec/batch)
2018-03-22 19:54:14.687397: step 59940, loss = 0.65 (268.5 examples/sec; 0.477 sec/batch)
2018-03-22 19:54:20.344506: step 59950, loss = 0.68 (226.3 examples/sec; 0.566 sec/batch)
2018-03-22 19:54:25.070837: step 59960, loss = 0.64 (270.8 examples/sec; 0.473 sec/batch)
2018-03-22 19:54:29.984905: step 59970, loss = 0.70 (260.5 examples/sec; 0.491 sec/batch)
2018-03-22 19:54:34.439488: step 59980, loss = 0.67 (287.3 examples/sec; 0.445 sec/batch)
2018-03-22 19:54:38.833841: step 59990, loss = 0.74 (291.3 examples/sec; 0.439 sec/batch)
2018-03-22 19:54:43.474967: step 60000, loss = 0.58 (275.8 examples/sec; 0.464 sec/batch)
2018-03-22 19:54:48.189453: step 60010, loss = 0.71 (271.5 examples/sec; 0.471 sec/batch)
2018-03-22 19:54:53.464448: step 60020, loss = 0.64 (242.7 examples/sec; 0.527 sec/batch)
2018-03-22 19:54:58.014347: step 60030, loss = 0.76 (281.3 examples/sec; 0.455 sec/batch)
2018-03-22 19:55:03.036536: step 60040, loss = 0.83 (254.9 examples/sec; 0.502 sec/batch)
2018-03-22 19:55:07.919388: step 60050, loss = 0.63 (262.1 examples/sec; 0.488 sec/batch)
2018-03-22 19:55:12.918461: step 60060, loss = 0.78 (256.0 examples/sec; 0.500 sec/batch)
2018-03-22 19:55:17.595376: step 60070, loss = 0.76 (273.7 examples/sec; 0.468 sec/batch)
2018-03-22 19:55:22.705262: step 60080, loss = 0.60 (250.5 examples/sec; 0.511 sec/batch)
2018-03-22 19:55:27.390762: step 60090, loss = 0.76 (273.2 examples/sec; 0.469 sec/batch)
2018-03-22 19:55:32.635018: step 60100, loss = 0.68 (244.1 examples/sec; 0.524 sec/batch)
2018-03-22 19:55:37.166441: step 60110, loss = 0.88 (282.5 examples/sec; 0.453 sec/batch)
2018-03-22 19:55:42.369336: step 60120, loss = 0.72 (246.0 examples/sec; 0.520 sec/batch)
2018-03-22 19:55:47.253053: step 60130, loss = 0.73 (262.1 examples/sec; 0.488 sec/batch)
2018-03-22 19:55:51.926426: step 60140, loss = 0.72 (273.9 examples/sec; 0.467 sec/batch)
2018-03-22 19:55:56.566259: step 60150, loss = 0.81 (275.9 examples/sec; 0.464 sec/batch)
2018-03-22 19:56:01.591333: step 60160, loss = 0.67 (254.7 examples/sec; 0.503 sec/batch)
2018-03-22 19:56:05.730620: step 60170, loss = 0.91 (309.2 examples/sec; 0.414 sec/batch)
2018-03-22 19:56:10.750315: step 60180, loss = 0.75 (255.0 examples/sec; 0.502 sec/batch)
2018-03-22 19:56:15.599359: step 60190, loss = 0.72 (264.0 examples/sec; 0.485 sec/batch)
2018-03-22 19:56:20.710859: step 60200, loss = 0.68 (250.4 examples/sec; 0.511 sec/batch)
2018-03-22 19:56:25.438390: step 60210, loss = 0.72 (270.8 examples/sec; 0.473 sec/batch)
2018-03-22 19:56:30.615005: step 60220, loss = 1.03 (247.3 examples/sec; 0.518 sec/batch)
2018-03-22 19:56:35.088375: step 60230, loss = 0.74 (286.1 examples/sec; 0.447 sec/batch)
2018-03-22 19:56:39.735413: step 60240, loss = 1.08 (275.4 examples/sec; 0.465 sec/batch)
2018-03-22 19:56:44.733296: step 60250, loss = 0.72 (256.1 examples/sec; 0.500 sec/batch)
2018-03-22 19:56:49.700434: step 60260, loss = 0.58 (257.7 examples/sec; 0.497 sec/batch)
2018-03-22 19:56:54.722039: step 60270, loss = 0.69 (254.9 examples/sec; 0.502 sec/batch)
2018-03-22 19:56:59.690336: step 60280, loss = 0.67 (257.6 examples/sec; 0.497 sec/batch)
2018-03-22 19:57:03.995329: step 60290, loss = 0.70 (297.3 examples/sec; 0.430 sec/batch)
2018-03-22 19:57:09.221708: step 60300, loss = 0.59 (244.9 examples/sec; 0.523 sec/batch)
2018-03-22 19:57:14.346357: step 60310, loss = 0.75 (249.8 examples/sec; 0.512 sec/batch)
2018-03-22 19:57:19.024881: step 60320, loss = 0.82 (273.6 examples/sec; 0.468 sec/batch)
2018-03-22 19:57:24.523382: step 60330, loss = 0.78 (232.8 examples/sec; 0.550 sec/batch)
2018-03-22 19:57:29.234463: step 60340, loss = 0.71 (271.7 examples/sec; 0.471 sec/batch)
2018-03-22 19:57:34.107181: step 60350, loss = 0.68 (262.7 examples/sec; 0.487 sec/batch)
2018-03-22 19:57:38.883431: step 60360, loss = 0.77 (268.0 examples/sec; 0.478 sec/batch)
2018-03-22 19:57:44.043414: step 60370, loss = 0.63 (248.1 examples/sec; 0.516 sec/batch)
2018-03-22 19:57:48.967453: step 60380, loss = 0.60 (259.9 examples/sec; 0.492 sec/batch)
2018-03-22 19:57:53.760683: step 60390, loss = 0.87 (267.0 examples/sec; 0.479 sec/batch)
2018-03-22 19:57:58.553461: step 60400, loss = 0.59 (267.1 examples/sec; 0.479 sec/batch)
2018-03-22 19:58:03.794824: step 60410, loss = 0.70 (244.2 examples/sec; 0.524 sec/batch)
2018-03-22 19:58:08.891829: step 60420, loss = 0.83 (251.1 examples/sec; 0.510 sec/batch)
2018-03-22 19:58:13.752416: step 60430, loss = 0.71 (263.3 examples/sec; 0.486 sec/batch)
2018-03-22 19:58:18.511564: step 60440, loss = 0.71 (269.0 examples/sec; 0.476 sec/batch)
2018-03-22 19:58:23.490300: step 60450, loss = 0.67 (257.1 examples/sec; 0.498 sec/batch)
2018-03-22 19:58:27.777319: step 60460, loss = 0.73 (298.6 examples/sec; 0.429 sec/batch)
2018-03-22 19:58:32.504609: step 60470, loss = 0.70 (270.8 examples/sec; 0.473 sec/batch)
2018-03-22 19:58:37.107339: step 60480, loss = 0.59 (278.1 examples/sec; 0.460 sec/batch)
2018-03-22 19:58:41.940527: step 60490, loss = 0.84 (264.8 examples/sec; 0.483 sec/batch)
2018-03-22 19:58:46.795151: step 60500, loss = 0.89 (263.7 examples/sec; 0.485 sec/batch)
2018-03-22 19:58:51.529442: step 60510, loss = 0.78 (270.4 examples/sec; 0.473 sec/batch)
2018-03-22 19:58:56.234225: step 60520, loss = 0.70 (272.1 examples/sec; 0.470 sec/batch)
2018-03-22 19:59:01.080447: step 60530, loss = 0.72 (264.1 examples/sec; 0.485 sec/batch)
2018-03-22 19:59:05.685421: step 60540, loss = 0.72 (278.0 examples/sec; 0.460 sec/batch)
2018-03-22 19:59:10.381499: step 60550, loss = 0.66 (272.6 examples/sec; 0.470 sec/batch)
2018-03-22 19:59:15.017352: step 60560, loss = 0.61 (276.1 examples/sec; 0.464 sec/batch)
2018-03-22 19:59:19.958875: step 60570, loss = 0.76 (259.0 examples/sec; 0.494 sec/batch)
2018-03-22 19:59:25.330448: step 60580, loss = 0.78 (238.3 examples/sec; 0.537 sec/batch)
2018-03-22 19:59:30.460759: step 60590, loss = 0.71 (249.5 examples/sec; 0.513 sec/batch)
2018-03-22 19:59:35.246122: step 60600, loss = 0.75 (267.5 examples/sec; 0.479 sec/batch)
2018-03-22 19:59:40.010505: step 60610, loss = 0.77 (268.7 examples/sec; 0.476 sec/batch)
2018-03-22 19:59:44.468722: step 60620, loss = 0.67 (287.1 examples/sec; 0.446 sec/batch)
2018-03-22 19:59:49.182316: step 60630, loss = 0.75 (271.6 examples/sec; 0.471 sec/batch)
2018-03-22 19:59:54.398381: step 60640, loss = 0.78 (245.4 examples/sec; 0.522 sec/batch)
2018-03-22 19:59:58.812939: step 60650, loss = 0.68 (289.9 examples/sec; 0.441 sec/batch)
2018-03-22 20:00:03.522417: step 60660, loss = 0.75 (271.8 examples/sec; 0.471 sec/batch)
2018-03-22 20:00:08.010302: step 60670, loss = 0.64 (285.2 examples/sec; 0.449 sec/batch)
2018-03-22 20:00:12.892418: step 60680, loss = 0.66 (262.2 examples/sec; 0.488 sec/batch)
2018-03-22 20:00:17.765410: step 60690, loss = 0.67 (262.7 examples/sec; 0.487 sec/batch)
2018-03-22 20:00:23.175259: step 60700, loss = 0.76 (236.6 examples/sec; 0.541 sec/batch)
2018-03-22 20:00:27.264499: step 60710, loss = 0.65 (313.0 examples/sec; 0.409 sec/batch)
2018-03-22 20:00:32.270439: step 60720, loss = 0.60 (255.7 examples/sec; 0.501 sec/batch)
2018-03-22 20:00:36.873468: step 60730, loss = 0.64 (278.1 examples/sec; 0.460 sec/batch)
2018-03-22 20:00:42.265439: step 60740, loss = 0.81 (237.4 examples/sec; 0.539 sec/batch)
2018-03-22 20:00:46.922398: step 60750, loss = 0.77 (274.9 examples/sec; 0.466 sec/batch)
2018-03-22 20:00:51.939422: step 60760, loss = 0.76 (255.1 examples/sec; 0.502 sec/batch)
2018-03-22 20:00:56.740372: step 60770, loss = 0.60 (266.6 examples/sec; 0.480 sec/batch)
2018-03-22 20:01:01.630387: step 60780, loss = 0.71 (261.8 examples/sec; 0.489 sec/batch)
2018-03-22 20:01:06.720507: step 60790, loss = 0.77 (251.5 examples/sec; 0.509 sec/batch)
2018-03-22 20:01:12.289849: step 60800, loss = 0.70 (229.8 examples/sec; 0.557 sec/batch)
2018-03-22 20:01:16.883363: step 60810, loss = 0.78 (278.7 examples/sec; 0.459 sec/batch)
2018-03-22 20:01:21.906182: step 60820, loss = 0.66 (254.8 examples/sec; 0.502 sec/batch)
2018-03-22 20:01:26.455394: step 60830, loss = 0.81 (281.4 examples/sec; 0.455 sec/batch)
2018-03-22 20:01:31.633357: step 60840, loss = 0.68 (247.2 examples/sec; 0.518 sec/batch)
2018-03-22 20:01:36.584710: step 60850, loss = 0.69 (258.5 examples/sec; 0.495 sec/batch)
2018-03-22 20:01:41.537325: step 60860, loss = 0.74 (258.4 examples/sec; 0.495 sec/batch)
2018-03-22 20:01:46.148116: step 60870, loss = 0.76 (277.6 examples/sec; 0.461 sec/batch)
2018-03-22 20:01:51.031981: step 60880, loss = 0.83 (262.1 examples/sec; 0.488 sec/batch)
2018-03-22 20:01:55.635387: step 60890, loss = 0.87 (278.1 examples/sec; 0.460 sec/batch)
2018-03-22 20:02:00.709190: step 60900, loss = 0.66 (252.3 examples/sec; 0.507 sec/batch)
2018-03-22 20:02:05.426396: step 60910, loss = 0.90 (271.3 examples/sec; 0.472 sec/batch)
2018-03-22 20:02:10.978621: step 60920, loss = 0.62 (230.5 examples/sec; 0.555 sec/batch)
2018-03-22 20:02:15.447123: step 60930, loss = 0.64 (286.4 examples/sec; 0.447 sec/batch)
2018-03-22 20:02:20.365353: step 60940, loss = 0.68 (260.3 examples/sec; 0.492 sec/batch)
2018-03-22 20:02:24.435131: step 60950, loss = 0.61 (314.5 examples/sec; 0.407 sec/batch)
2018-03-22 20:02:28.635306: step 60960, loss = 0.54 (304.7 examples/sec; 0.420 sec/batch)
2018-03-22 20:02:33.206107: step 60970, loss = 0.69 (280.0 examples/sec; 0.457 sec/batch)
2018-03-22 20:02:37.887314: step 60980, loss = 0.90 (273.4 examples/sec; 0.468 sec/batch)
2018-03-22 20:02:43.093206: step 60990, loss = 0.79 (245.9 examples/sec; 0.521 sec/batch)
2018-03-22 20:02:47.877766: step 61000, loss = 0.74 (267.5 examples/sec; 0.478 sec/batch)
2018-03-22 20:02:53.160450: step 61010, loss = 0.78 (242.3 examples/sec; 0.528 sec/batch)
2018-03-22 20:02:57.850061: step 61020, loss = 0.74 (272.9 examples/sec; 0.469 sec/batch)
2018-03-22 20:03:02.961426: step 61030, loss = 0.85 (250.4 examples/sec; 0.511 sec/batch)
2018-03-22 20:03:07.802317: step 61040, loss = 0.79 (264.4 examples/sec; 0.484 sec/batch)
2018-03-22 20:03:12.626491: step 61050, loss = 0.72 (265.3 examples/sec; 0.482 sec/batch)
2018-03-22 20:03:17.329999: step 61060, loss = 0.60 (272.1 examples/sec; 0.470 sec/batch)
2018-03-22 20:03:22.338384: step 61070, loss = 0.68 (255.6 examples/sec; 0.501 sec/batch)
2018-03-22 20:03:27.022657: step 61080, loss = 0.70 (273.3 examples/sec; 0.468 sec/batch)
2018-03-22 20:03:31.793372: step 61090, loss = 0.80 (268.3 examples/sec; 0.477 sec/batch)
2018-03-22 20:03:36.433562: step 61100, loss = 0.83 (275.9 examples/sec; 0.464 sec/batch)
2018-03-22 20:03:41.634302: step 61110, loss = 0.71 (246.1 examples/sec; 0.520 sec/batch)
2018-03-22 20:03:46.217356: step 61120, loss = 0.75 (279.3 examples/sec; 0.458 sec/batch)
2018-03-22 20:03:51.036139: step 61130, loss = 0.81 (265.6 examples/sec; 0.482 sec/batch)
2018-03-22 20:03:55.334140: step 61140, loss = 0.57 (297.8 examples/sec; 0.430 sec/batch)
2018-03-22 20:04:00.687471: step 61150, loss = 0.63 (239.1 examples/sec; 0.535 sec/batch)
2018-03-22 20:04:05.149403: step 61160, loss = 0.69 (286.9 examples/sec; 0.446 sec/batch)
2018-03-22 20:04:10.303906: step 61170, loss = 0.81 (248.3 examples/sec; 0.515 sec/batch)
2018-03-22 20:04:15.145438: step 61180, loss = 0.62 (264.4 examples/sec; 0.484 sec/batch)
2018-03-22 20:04:19.988115: step 61190, loss = 0.62 (264.3 examples/sec; 0.484 sec/batch)
2018-03-22 20:04:24.511291: step 61200, loss = 0.69 (283.0 examples/sec; 0.452 sec/batch)
2018-03-22 20:04:29.107249: step 61210, loss = 0.81 (278.5 examples/sec; 0.460 sec/batch)
2018-03-22 20:04:34.193911: step 61220, loss = 0.67 (251.6 examples/sec; 0.509 sec/batch)
2018-03-22 20:04:38.988429: step 61230, loss = 0.66 (267.0 examples/sec; 0.479 sec/batch)
2018-03-22 20:04:44.160911: step 61240, loss = 0.74 (247.5 examples/sec; 0.517 sec/batch)
2018-03-22 20:04:48.547466: step 61250, loss = 0.65 (291.8 examples/sec; 0.439 sec/batch)
2018-03-22 20:04:53.450391: step 61260, loss = 0.90 (261.1 examples/sec; 0.490 sec/batch)
2018-03-22 20:04:57.818195: step 61270, loss = 0.70 (293.1 examples/sec; 0.437 sec/batch)
2018-03-22 20:05:02.880149: step 61280, loss = 0.76 (252.9 examples/sec; 0.506 sec/batch)
2018-03-22 20:05:07.335528: step 61290, loss = 0.76 (287.3 examples/sec; 0.446 sec/batch)
2018-03-22 20:05:12.492960: step 61300, loss = 0.71 (248.2 examples/sec; 0.516 sec/batch)
2018-03-22 20:05:17.258239: step 61310, loss = 0.57 (268.6 examples/sec; 0.477 sec/batch)
2018-03-22 20:05:22.207369: step 61320, loss = 0.72 (258.6 examples/sec; 0.495 sec/batch)
2018-03-22 20:05:26.934377: step 61330, loss = 0.66 (270.8 examples/sec; 0.473 sec/batch)
2018-03-22 20:05:32.307380: step 61340, loss = 0.68 (238.2 examples/sec; 0.537 sec/batch)
2018-03-22 20:05:37.211532: step 61350, loss = 0.60 (261.0 examples/sec; 0.490 sec/batch)
2018-03-22 20:05:42.380366: step 61360, loss = 0.70 (247.6 examples/sec; 0.517 sec/batch)
2018-03-22 20:05:47.346401: step 61370, loss = 0.65 (257.8 examples/sec; 0.497 sec/batch)
2018-03-22 20:05:52.209387: step 61380, loss = 0.66 (263.2 examples/sec; 0.486 sec/batch)
2018-03-22 20:05:56.796688: step 61390, loss = 0.82 (279.0 examples/sec; 0.459 sec/batch)
2018-03-22 20:06:02.548464: step 61400, loss = 0.70 (222.5 examples/sec; 0.575 sec/batch)
2018-03-22 20:06:07.279599: step 61410, loss = 0.72 (270.5 examples/sec; 0.473 sec/batch)
2018-03-22 20:06:12.310334: step 61420, loss = 0.74 (254.4 examples/sec; 0.503 sec/batch)
2018-03-22 20:06:16.590217: step 61430, loss = 0.87 (299.1 examples/sec; 0.428 sec/batch)
2018-03-22 20:06:21.252643: step 61440, loss = 0.71 (274.5 examples/sec; 0.466 sec/batch)
2018-03-22 20:06:25.353603: step 61450, loss = 0.72 (312.1 examples/sec; 0.410 sec/batch)
2018-03-22 20:06:29.806358: step 61460, loss = 0.75 (287.5 examples/sec; 0.445 sec/batch)
2018-03-22 20:06:34.911281: step 61470, loss = 0.74 (250.7 examples/sec; 0.510 sec/batch)
2018-03-22 20:06:39.687445: step 61480, loss = 0.88 (268.0 examples/sec; 0.478 sec/batch)
2018-03-22 20:06:44.732643: step 61490, loss = 0.73 (253.7 examples/sec; 0.505 sec/batch)
2018-03-22 20:06:49.727213: step 61500, loss = 0.63 (256.3 examples/sec; 0.499 sec/batch)
2018-03-22 20:06:54.602379: step 61510, loss = 0.66 (262.6 examples/sec; 0.488 sec/batch)
2018-03-22 20:06:58.906423: step 61520, loss = 0.72 (297.4 examples/sec; 0.430 sec/batch)
2018-03-22 20:07:04.057345: step 61530, loss = 0.75 (248.5 examples/sec; 0.515 sec/batch)
2018-03-22 20:07:08.994410: step 61540, loss = 0.79 (259.3 examples/sec; 0.494 sec/batch)
2018-03-22 20:07:14.313348: step 61550, loss = 0.92 (240.6 examples/sec; 0.532 sec/batch)
2018-03-22 20:07:19.079507: step 61560, loss = 0.76 (268.6 examples/sec; 0.477 sec/batch)
2018-03-22 20:07:23.728130: step 61570, loss = 0.73 (275.4 examples/sec; 0.465 sec/batch)
2018-03-22 20:07:28.440054: step 61580, loss = 0.57 (271.7 examples/sec; 0.471 sec/batch)
2018-03-22 20:07:33.404302: step 61590, loss = 0.85 (257.8 examples/sec; 0.496 sec/batch)
2018-03-22 20:07:38.204962: step 61600, loss = 0.77 (266.6 examples/sec; 0.480 sec/batch)
2018-03-22 20:07:43.400544: step 61610, loss = 0.57 (246.4 examples/sec; 0.520 sec/batch)
2018-03-22 20:07:48.019799: step 61620, loss = 0.74 (277.1 examples/sec; 0.462 sec/batch)
2018-03-22 20:07:52.912349: step 61630, loss = 0.64 (261.6 examples/sec; 0.489 sec/batch)
2018-03-22 20:07:57.544421: step 61640, loss = 0.65 (276.3 examples/sec; 0.463 sec/batch)
2018-03-22 20:08:02.367396: step 61650, loss = 0.78 (265.4 examples/sec; 0.482 sec/batch)
2018-03-22 20:08:07.161388: step 61660, loss = 0.76 (267.0 examples/sec; 0.479 sec/batch)
2018-03-22 20:08:12.066716: step 61670, loss = 0.72 (260.9 examples/sec; 0.491 sec/batch)
2018-03-22 20:08:16.631651: step 61680, loss = 0.74 (280.4 examples/sec; 0.456 sec/batch)
2018-03-22 20:08:21.513173: step 61690, loss = 0.78 (262.2 examples/sec; 0.488 sec/batch)
2018-03-22 20:08:26.251769: step 61700, loss = 0.76 (270.1 examples/sec; 0.474 sec/batch)
2018-03-22 20:08:31.401396: step 61710, loss = 0.75 (248.6 examples/sec; 0.515 sec/batch)
2018-03-22 20:08:36.180876: step 61720, loss = 0.76 (267.8 examples/sec; 0.478 sec/batch)
2018-03-22 20:08:41.287313: step 61730, loss = 0.76 (250.7 examples/sec; 0.511 sec/batch)
2018-03-22 20:08:45.799838: step 61740, loss = 0.76 (283.7 examples/sec; 0.451 sec/batch)
2018-03-22 20:08:50.669203: step 61750, loss = 0.85 (262.9 examples/sec; 0.487 sec/batch)
2018-03-22 20:08:55.215423: step 61760, loss = 0.77 (281.6 examples/sec; 0.455 sec/batch)
2018-03-22 20:09:00.367904: step 61770, loss = 0.68 (248.4 examples/sec; 0.515 sec/batch)
2018-03-22 20:09:05.268332: step 61780, loss = 0.76 (261.2 examples/sec; 0.490 sec/batch)
2018-03-22 20:09:10.627275: step 61790, loss = 0.71 (238.9 examples/sec; 0.536 sec/batch)
2018-03-22 20:09:15.658018: step 61800, loss = 0.78 (254.4 examples/sec; 0.503 sec/batch)
2018-03-22 20:09:20.862336: step 61810, loss = 0.73 (245.9 examples/sec; 0.520 sec/batch)
2018-03-22 20:09:25.416252: step 61820, loss = 0.88 (281.1 examples/sec; 0.455 sec/batch)
2018-03-22 20:09:30.364340: step 61830, loss = 0.65 (258.7 examples/sec; 0.495 sec/batch)
2018-03-22 20:09:35.196377: step 61840, loss = 0.80 (264.9 examples/sec; 0.483 sec/batch)
2018-03-22 20:09:40.247689: step 61850, loss = 0.88 (253.4 examples/sec; 0.505 sec/batch)
2018-03-22 20:09:44.945427: step 61860, loss = 0.71 (272.5 examples/sec; 0.470 sec/batch)
2018-03-22 20:09:50.327104: step 61870, loss = 0.65 (237.8 examples/sec; 0.538 sec/batch)
2018-03-22 20:09:54.719369: step 61880, loss = 0.74 (291.4 examples/sec; 0.439 sec/batch)
2018-03-22 20:09:59.632446: step 61890, loss = 0.83 (260.5 examples/sec; 0.491 sec/batch)
2018-03-22 20:10:04.959515: step 61900, loss = 0.88 (240.3 examples/sec; 0.533 sec/batch)
2018-03-22 20:10:09.915761: step 61910, loss = 0.70 (258.3 examples/sec; 0.496 sec/batch)
2018-03-22 20:10:14.688994: step 61920, loss = 0.75 (268.2 examples/sec; 0.477 sec/batch)
2018-03-22 20:10:19.126343: step 61930, loss = 0.65 (288.5 examples/sec; 0.444 sec/batch)
2018-03-22 20:10:24.173157: step 61940, loss = 0.97 (253.6 examples/sec; 0.505 sec/batch)
2018-03-22 20:10:29.403426: step 61950, loss = 0.69 (244.7 examples/sec; 0.523 sec/batch)
2018-03-22 20:10:34.224567: step 61960, loss = 0.81 (265.5 examples/sec; 0.482 sec/batch)
2018-03-22 20:10:38.951390: step 61970, loss = 0.91 (270.8 examples/sec; 0.473 sec/batch)
2018-03-22 20:10:43.919327: step 61980, loss = 0.90 (257.7 examples/sec; 0.497 sec/batch)
2018-03-22 20:10:48.780802: step 61990, loss = 0.68 (263.3 examples/sec; 0.486 sec/batch)
2018-03-22 20:10:54.071836: step 62000, loss = 0.80 (241.9 examples/sec; 0.529 sec/batch)
2018-03-22 20:10:58.579412: step 62010, loss = 0.62 (284.0 examples/sec; 0.451 sec/batch)
2018-03-22 20:11:03.506448: step 62020, loss = 0.76 (259.8 examples/sec; 0.493 sec/batch)
2018-03-22 20:11:08.445441: step 62030, loss = 0.69 (259.2 examples/sec; 0.494 sec/batch)
2018-03-22 20:11:13.558150: step 62040, loss = 0.77 (250.4 examples/sec; 0.511 sec/batch)
2018-03-22 20:11:18.095421: step 62050, loss = 0.76 (282.1 examples/sec; 0.454 sec/batch)
2018-03-22 20:11:23.261568: step 62060, loss = 0.64 (247.8 examples/sec; 0.517 sec/batch)
2018-03-22 20:11:27.831468: step 62070, loss = 0.55 (280.1 examples/sec; 0.457 sec/batch)
2018-03-22 20:11:33.014389: step 62080, loss = 0.78 (247.0 examples/sec; 0.518 sec/batch)
2018-03-22 20:11:37.434207: step 62090, loss = 0.76 (289.6 examples/sec; 0.442 sec/batch)
2018-03-22 20:11:42.752908: step 62100, loss = 0.71 (240.7 examples/sec; 0.532 sec/batch)
2018-03-22 20:11:47.325902: step 62110, loss = 0.63 (279.9 examples/sec; 0.457 sec/batch)
2018-03-22 20:11:51.998418: step 62120, loss = 0.71 (273.9 examples/sec; 0.467 sec/batch)
2018-03-22 20:11:56.725431: step 62130, loss = 0.76 (270.8 examples/sec; 0.473 sec/batch)
2018-03-22 20:12:01.781441: step 62140, loss = 0.71 (253.2 examples/sec; 0.506 sec/batch)
2018-03-22 20:12:06.005033: step 62150, loss = 0.65 (303.1 examples/sec; 0.422 sec/batch)
2018-03-22 20:12:10.718245: step 62160, loss = 0.86 (271.6 examples/sec; 0.471 sec/batch)
2018-03-22 20:12:15.476291: step 62170, loss = 0.72 (269.0 examples/sec; 0.476 sec/batch)
2018-03-22 20:12:20.821403: step 62180, loss = 0.61 (239.5 examples/sec; 0.535 sec/batch)
2018-03-22 20:12:25.310384: step 62190, loss = 0.61 (285.1 examples/sec; 0.449 sec/batch)
2018-03-22 20:12:30.619961: step 62200, loss = 0.84 (241.1 examples/sec; 0.531 sec/batch)
2018-03-22 20:12:35.207818: step 62210, loss = 0.64 (279.0 examples/sec; 0.459 sec/batch)
2018-03-22 20:12:40.841376: step 62220, loss = 0.74 (227.2 examples/sec; 0.563 sec/batch)
2018-03-22 20:12:45.819408: step 62230, loss = 0.63 (257.1 examples/sec; 0.498 sec/batch)
2018-03-22 20:12:51.009867: step 62240, loss = 0.80 (246.6 examples/sec; 0.519 sec/batch)
2018-03-22 20:12:55.522380: step 62250, loss = 0.63 (283.7 examples/sec; 0.451 sec/batch)
2018-03-22 20:13:00.675502: step 62260, loss = 0.58 (248.4 examples/sec; 0.515 sec/batch)
2018-03-22 20:13:05.428459: step 62270, loss = 0.86 (269.3 examples/sec; 0.475 sec/batch)
2018-03-22 20:13:10.480361: step 62280, loss = 0.79 (253.4 examples/sec; 0.505 sec/batch)
2018-03-22 20:13:15.041357: step 62290, loss = 0.78 (280.6 examples/sec; 0.456 sec/batch)
2018-03-22 20:13:20.054344: step 62300, loss = 0.52 (255.3 examples/sec; 0.501 sec/batch)
2018-03-22 20:13:24.751412: step 62310, loss = 0.66 (272.5 examples/sec; 0.470 sec/batch)
2018-03-22 20:13:29.629314: step 62320, loss = 0.68 (262.4 examples/sec; 0.488 sec/batch)
2018-03-22 20:13:34.053494: step 62330, loss = 0.78 (289.3 examples/sec; 0.442 sec/batch)
2018-03-22 20:13:38.593347: step 62340, loss = 0.72 (281.9 examples/sec; 0.454 sec/batch)
2018-03-22 20:13:43.852043: step 62350, loss = 0.56 (243.4 examples/sec; 0.526 sec/batch)
2018-03-22 20:13:48.446417: step 62360, loss = 0.63 (278.6 examples/sec; 0.459 sec/batch)
2018-03-22 20:13:53.432932: step 62370, loss = 0.87 (256.7 examples/sec; 0.499 sec/batch)
2018-03-22 20:13:58.402808: step 62380, loss = 0.77 (257.6 examples/sec; 0.497 sec/batch)
2018-03-22 20:14:03.416080: step 62390, loss = 0.79 (255.3 examples/sec; 0.501 sec/batch)
2018-03-22 20:14:08.022858: step 62400, loss = 0.83 (277.9 examples/sec; 0.461 sec/batch)
2018-03-22 20:14:13.219432: step 62410, loss = 0.84 (246.3 examples/sec; 0.520 sec/batch)
2018-03-22 20:14:18.144368: step 62420, loss = 0.69 (259.9 examples/sec; 0.492 sec/batch)
2018-03-22 20:14:23.289356: step 62430, loss = 0.74 (248.8 examples/sec; 0.514 sec/batch)
2018-03-22 20:14:27.809364: step 62440, loss = 0.74 (283.2 examples/sec; 0.452 sec/batch)
2018-03-22 20:14:32.880404: step 62450, loss = 0.70 (252.4 examples/sec; 0.507 sec/batch)
2018-03-22 20:14:37.546434: step 62460, loss = 0.84 (274.3 examples/sec; 0.467 sec/batch)
2018-03-22 20:14:42.645402: step 62470, loss = 0.73 (251.0 examples/sec; 0.510 sec/batch)
2018-03-22 20:14:47.479503: step 62480, loss = 0.74 (264.8 examples/sec; 0.483 sec/batch)
2018-03-22 20:14:52.236348: step 62490, loss = 0.67 (269.1 examples/sec; 0.476 sec/batch)
2018-03-22 20:14:57.501021: step 62500, loss = 0.69 (243.1 examples/sec; 0.526 sec/batch)
2018-03-22 20:15:02.631753: step 62510, loss = 0.63 (249.5 examples/sec; 0.513 sec/batch)
2018-03-22 20:15:07.274491: step 62520, loss = 0.65 (275.7 examples/sec; 0.464 sec/batch)
2018-03-22 20:15:12.234548: step 62530, loss = 0.74 (258.1 examples/sec; 0.496 sec/batch)
2018-03-22 20:15:16.877444: step 62540, loss = 0.73 (275.7 examples/sec; 0.464 sec/batch)
2018-03-22 20:15:21.650942: step 62550, loss = 0.61 (268.1 examples/sec; 0.477 sec/batch)
2018-03-22 20:15:25.920426: step 62560, loss = 0.80 (299.8 examples/sec; 0.427 sec/batch)
2018-03-22 20:15:30.693889: step 62570, loss = 0.80 (268.1 examples/sec; 0.477 sec/batch)
2018-03-22 20:15:35.664291: step 62580, loss = 0.69 (257.5 examples/sec; 0.497 sec/batch)
2018-03-22 20:15:40.389203: step 62590, loss = 0.72 (270.9 examples/sec; 0.472 sec/batch)
2018-03-22 20:15:45.559635: step 62600, loss = 0.61 (247.6 examples/sec; 0.517 sec/batch)
2018-03-22 20:15:49.960054: step 62610, loss = 0.67 (290.9 examples/sec; 0.440 sec/batch)
2018-03-22 20:15:54.636832: step 62620, loss = 0.60 (273.7 examples/sec; 0.468 sec/batch)
2018-03-22 20:15:59.068256: step 62630, loss = 0.72 (288.8 examples/sec; 0.443 sec/batch)
2018-03-22 20:16:04.198400: step 62640, loss = 0.67 (249.5 examples/sec; 0.513 sec/batch)
2018-03-22 20:16:09.633893: step 62650, loss = 0.75 (235.5 examples/sec; 0.544 sec/batch)
2018-03-22 20:16:14.139474: step 62660, loss = 0.72 (284.1 examples/sec; 0.451 sec/batch)
2018-03-22 20:16:19.044884: step 62670, loss = 0.65 (260.9 examples/sec; 0.491 sec/batch)
2018-03-22 20:16:23.817515: step 62680, loss = 0.74 (268.2 examples/sec; 0.477 sec/batch)
2018-03-22 20:16:28.673663: step 62690, loss = 0.75 (263.6 examples/sec; 0.486 sec/batch)
2018-03-22 20:16:33.813968: step 62700, loss = 0.74 (249.0 examples/sec; 0.514 sec/batch)
2018-03-22 20:16:39.263405: step 62710, loss = 0.84 (234.9 examples/sec; 0.545 sec/batch)
2018-03-22 20:16:44.078577: step 62720, loss = 0.73 (265.8 examples/sec; 0.482 sec/batch)
2018-03-22 20:16:48.366384: step 62730, loss = 0.73 (298.5 examples/sec; 0.429 sec/batch)
2018-03-22 20:16:53.161023: step 62740, loss = 0.77 (267.0 examples/sec; 0.479 sec/batch)
2018-03-22 20:16:57.829369: step 62750, loss = 0.63 (274.2 examples/sec; 0.467 sec/batch)
2018-03-22 20:17:02.964372: step 62760, loss = 0.86 (249.3 examples/sec; 0.514 sec/batch)
2018-03-22 20:17:07.647769: step 62770, loss = 0.68 (273.3 examples/sec; 0.468 sec/batch)
2018-03-22 20:17:12.661148: step 62780, loss = 0.78 (255.3 examples/sec; 0.501 sec/batch)
2018-03-22 20:17:17.223875: step 62790, loss = 0.73 (280.5 examples/sec; 0.456 sec/batch)
2018-03-22 20:17:22.315009: step 62800, loss = 0.76 (251.4 examples/sec; 0.509 sec/batch)
2018-03-22 20:17:26.724422: step 62810, loss = 0.78 (290.3 examples/sec; 0.441 sec/batch)
2018-03-22 20:17:31.650903: step 62820, loss = 0.63 (259.8 examples/sec; 0.493 sec/batch)
2018-03-22 20:17:36.057400: step 62830, loss = 0.86 (290.5 examples/sec; 0.441 sec/batch)
2018-03-22 20:17:41.173335: step 62840, loss = 0.82 (250.2 examples/sec; 0.512 sec/batch)
2018-03-22 20:17:45.895455: step 62850, loss = 0.61 (271.1 examples/sec; 0.472 sec/batch)
2018-03-22 20:17:51.361418: step 62860, loss = 0.67 (234.2 examples/sec; 0.547 sec/batch)
2018-03-22 20:17:56.006833: step 62870, loss = 0.73 (275.5 examples/sec; 0.465 sec/batch)
2018-03-22 20:18:01.407473: step 62880, loss = 0.83 (237.0 examples/sec; 0.540 sec/batch)
2018-03-22 20:18:06.372600: step 62890, loss = 0.84 (257.8 examples/sec; 0.497 sec/batch)
2018-03-22 20:18:11.435642: step 62900, loss = 0.77 (252.8 examples/sec; 0.506 sec/batch)
2018-03-22 20:18:16.340424: step 62910, loss = 0.85 (261.0 examples/sec; 0.490 sec/batch)
2018-03-22 20:18:21.353346: step 62920, loss = 0.83 (255.3 examples/sec; 0.501 sec/batch)
2018-03-22 20:18:25.661403: step 62930, loss = 0.80 (297.1 examples/sec; 0.431 sec/batch)
2018-03-22 20:18:30.410090: step 62940, loss = 0.68 (269.5 examples/sec; 0.475 sec/batch)
2018-03-22 20:18:34.945446: step 62950, loss = 0.70 (282.2 examples/sec; 0.454 sec/batch)
2018-03-22 20:18:39.838399: step 62960, loss = 0.94 (261.6 examples/sec; 0.489 sec/batch)
2018-03-22 20:18:44.774850: step 62970, loss = 0.63 (259.3 examples/sec; 0.494 sec/batch)
2018-03-22 20:18:50.014363: step 62980, loss = 0.79 (244.3 examples/sec; 0.524 sec/batch)
2018-03-22 20:18:55.100484: step 62990, loss = 0.80 (251.7 examples/sec; 0.509 sec/batch)
2018-03-22 20:19:00.121530: step 63000, loss = 0.63 (254.9 examples/sec; 0.502 sec/batch)
2018-03-22 20:19:04.606317: step 63010, loss = 0.77 (285.4 examples/sec; 0.448 sec/batch)
2018-03-22 20:19:09.018086: step 63020, loss = 0.70 (290.1 examples/sec; 0.441 sec/batch)
2018-03-22 20:19:13.798421: step 63030, loss = 0.74 (267.8 examples/sec; 0.478 sec/batch)
2018-03-22 20:19:18.406396: step 63040, loss = 0.58 (277.8 examples/sec; 0.461 sec/batch)
2018-03-22 20:19:23.537356: step 63050, loss = 0.69 (249.5 examples/sec; 0.513 sec/batch)
2018-03-22 20:19:28.165692: step 63060, loss = 0.73 (276.6 examples/sec; 0.463 sec/batch)
2018-03-22 20:19:32.841391: step 63070, loss = 0.71 (273.8 examples/sec; 0.468 sec/batch)
2018-03-22 20:19:37.219122: step 63080, loss = 0.65 (292.4 examples/sec; 0.438 sec/batch)
2018-03-22 20:19:41.826466: step 63090, loss = 0.88 (277.8 examples/sec; 0.461 sec/batch)
2018-03-22 20:19:46.770943: step 63100, loss = 0.68 (258.9 examples/sec; 0.494 sec/batch)
2018-03-22 20:19:51.895377: step 63110, loss = 0.71 (249.8 examples/sec; 0.512 sec/batch)
2018-03-22 20:19:56.273751: step 63120, loss = 0.74 (292.3 examples/sec; 0.438 sec/batch)
2018-03-22 20:20:01.641622: step 63130, loss = 0.89 (238.5 examples/sec; 0.537 sec/batch)
2018-03-22 20:20:06.380371: step 63140, loss = 0.72 (270.1 examples/sec; 0.474 sec/batch)
2018-03-22 20:20:11.432036: step 63150, loss = 0.75 (253.4 examples/sec; 0.505 sec/batch)
2018-03-22 20:20:16.130800: step 63160, loss = 0.74 (272.4 examples/sec; 0.470 sec/batch)
2018-03-22 20:20:20.917072: step 63170, loss = 0.66 (267.4 examples/sec; 0.479 sec/batch)
2018-03-22 20:20:25.883784: step 63180, loss = 0.95 (257.7 examples/sec; 0.497 sec/batch)
2018-03-22 20:20:31.138450: step 63190, loss = 0.68 (243.6 examples/sec; 0.525 sec/batch)
2018-03-22 20:20:36.034272: step 63200, loss = 0.77 (261.4 examples/sec; 0.490 sec/batch)
2018-03-22 20:20:40.931412: step 63210, loss = 0.66 (261.4 examples/sec; 0.490 sec/batch)
2018-03-22 20:20:45.656414: step 63220, loss = 0.57 (270.9 examples/sec; 0.473 sec/batch)
2018-03-22 20:20:50.762357: step 63230, loss = 0.74 (250.7 examples/sec; 0.511 sec/batch)
2018-03-22 20:20:55.595957: step 63240, loss = 0.65 (264.8 examples/sec; 0.483 sec/batch)
2018-03-22 20:21:00.532397: step 63250, loss = 0.67 (259.3 examples/sec; 0.494 sec/batch)
2018-03-22 20:21:05.661445: step 63260, loss = 0.79 (249.6 examples/sec; 0.513 sec/batch)
2018-03-22 20:21:10.378208: step 63270, loss = 0.63 (271.4 examples/sec; 0.472 sec/batch)
2018-03-22 20:21:15.044428: step 63280, loss = 0.78 (274.3 examples/sec; 0.467 sec/batch)
2018-03-22 20:21:19.747748: step 63290, loss = 0.64 (272.1 examples/sec; 0.470 sec/batch)
2018-03-22 20:21:24.552321: step 63300, loss = 0.62 (266.4 examples/sec; 0.480 sec/batch)
2018-03-22 20:21:29.731234: step 63310, loss = 0.63 (247.2 examples/sec; 0.518 sec/batch)
2018-03-22 20:21:34.596313: step 63320, loss = 0.64 (263.1 examples/sec; 0.487 sec/batch)
2018-03-22 20:21:39.426278: step 63330, loss = 0.56 (265.0 examples/sec; 0.483 sec/batch)
2018-03-22 20:21:44.138380: step 63340, loss = 0.71 (271.6 examples/sec; 0.471 sec/batch)
2018-03-22 20:21:49.075856: step 63350, loss = 0.79 (259.2 examples/sec; 0.494 sec/batch)
2018-03-22 20:21:54.119324: step 63360, loss = 0.69 (253.8 examples/sec; 0.504 sec/batch)
2018-03-22 20:21:58.938363: step 63370, loss = 0.71 (265.6 examples/sec; 0.482 sec/batch)
2018-03-22 20:22:03.936396: step 63380, loss = 0.72 (256.1 examples/sec; 0.500 sec/batch)
2018-03-22 20:22:08.773167: step 63390, loss = 0.67 (264.6 examples/sec; 0.484 sec/batch)
2018-03-22 20:22:13.893126: step 63400, loss = 0.95 (250.0 examples/sec; 0.512 sec/batch)
2018-03-22 20:22:18.929438: step 63410, loss = 0.78 (254.2 examples/sec; 0.504 sec/batch)
2018-03-22 20:22:23.961350: step 63420, loss = 0.62 (254.4 examples/sec; 0.503 sec/batch)
2018-03-22 20:22:28.576433: step 63430, loss = 0.71 (277.4 examples/sec; 0.462 sec/batch)
2018-03-22 20:22:33.872614: step 63440, loss = 0.66 (241.7 examples/sec; 0.530 sec/batch)
2018-03-22 20:22:38.415834: step 63450, loss = 0.71 (281.7 examples/sec; 0.454 sec/batch)
2018-03-22 20:22:43.293598: step 63460, loss = 0.66 (262.4 examples/sec; 0.488 sec/batch)
2018-03-22 20:22:48.023086: step 63470, loss = 0.59 (270.6 examples/sec; 0.473 sec/batch)
2018-03-22 20:22:52.929873: step 63480, loss = 0.64 (260.9 examples/sec; 0.491 sec/batch)
2018-03-22 20:22:56.909074: step 63490, loss = 0.81 (321.7 examples/sec; 0.398 sec/batch)
2018-03-22 20:23:01.838072: step 63500, loss = 0.78 (259.7 examples/sec; 0.493 sec/batch)
2018-03-22 20:23:06.345456: step 63510, loss = 0.89 (284.0 examples/sec; 0.451 sec/batch)
2018-03-22 20:23:11.612346: step 63520, loss = 0.77 (243.0 examples/sec; 0.527 sec/batch)
2018-03-22 20:23:16.177449: step 63530, loss = 0.59 (280.4 examples/sec; 0.457 sec/batch)
2018-03-22 20:23:21.208454: step 63540, loss = 0.67 (254.4 examples/sec; 0.503 sec/batch)
2018-03-22 20:23:25.746081: step 63550, loss = 0.78 (282.1 examples/sec; 0.454 sec/batch)
2018-03-22 20:23:30.242979: step 63560, loss = 0.62 (284.6 examples/sec; 0.450 sec/batch)
2018-03-22 20:23:35.101475: step 63570, loss = 0.85 (263.5 examples/sec; 0.486 sec/batch)
2018-03-22 20:23:40.232057: step 63580, loss = 0.69 (249.5 examples/sec; 0.513 sec/batch)
2018-03-22 20:23:44.921548: step 63590, loss = 0.66 (273.0 examples/sec; 0.469 sec/batch)
2018-03-22 20:23:50.103947: step 63600, loss = 0.79 (247.0 examples/sec; 0.518 sec/batch)
2018-03-22 20:23:54.941511: step 63610, loss = 0.73 (264.6 examples/sec; 0.484 sec/batch)
2018-03-22 20:23:59.543126: step 63620, loss = 0.77 (278.2 examples/sec; 0.460 sec/batch)
2018-03-22 20:24:04.237843: step 63630, loss = 0.60 (272.6 examples/sec; 0.469 sec/batch)
2018-03-22 20:24:08.909077: step 63640, loss = 0.76 (274.0 examples/sec; 0.467 sec/batch)
2018-03-22 20:24:13.858224: step 63650, loss = 0.73 (258.6 examples/sec; 0.495 sec/batch)
2018-03-22 20:24:18.584430: step 63660, loss = 0.66 (270.8 examples/sec; 0.473 sec/batch)
2018-03-22 20:24:23.339455: step 63670, loss = 0.63 (269.2 examples/sec; 0.476 sec/batch)
2018-03-22 20:24:28.030355: step 63680, loss = 0.77 (272.9 examples/sec; 0.469 sec/batch)
2018-03-22 20:24:32.575412: step 63690, loss = 0.81 (281.6 examples/sec; 0.455 sec/batch)
2018-03-22 20:24:37.614622: step 63700, loss = 0.69 (254.0 examples/sec; 0.504 sec/batch)
2018-03-22 20:24:42.273442: step 63710, loss = 0.70 (274.7 examples/sec; 0.466 sec/batch)
2018-03-22 20:24:46.698579: step 63720, loss = 0.74 (289.3 examples/sec; 0.443 sec/batch)
2018-03-22 20:24:51.912497: step 63730, loss = 0.90 (245.5 examples/sec; 0.521 sec/batch)
2018-03-22 20:24:56.849753: step 63740, loss = 0.82 (259.3 examples/sec; 0.494 sec/batch)
2018-03-22 20:25:01.922447: step 63750, loss = 0.63 (252.3 examples/sec; 0.507 sec/batch)
2018-03-22 20:25:06.383157: step 63760, loss = 0.74 (286.9 examples/sec; 0.446 sec/batch)
2018-03-22 20:25:10.956590: step 63770, loss = 0.80 (279.9 examples/sec; 0.457 sec/batch)
2018-03-22 20:25:15.361246: step 63780, loss = 0.79 (290.6 examples/sec; 0.440 sec/batch)
2018-03-22 20:25:20.330394: step 63790, loss = 0.86 (257.6 examples/sec; 0.497 sec/batch)
2018-03-22 20:25:25.368948: step 63800, loss = 0.84 (254.0 examples/sec; 0.504 sec/batch)
2018-03-22 20:25:30.112400: step 63810, loss = 0.71 (269.8 examples/sec; 0.474 sec/batch)
2018-03-22 20:25:34.334030: step 63820, loss = 0.79 (303.2 examples/sec; 0.422 sec/batch)
2018-03-22 20:25:39.337839: step 63830, loss = 0.84 (255.8 examples/sec; 0.500 sec/batch)
2018-03-22 20:25:44.029352: step 63840, loss = 0.74 (272.8 examples/sec; 0.469 sec/batch)
2018-03-22 20:25:48.666680: step 63850, loss = 0.68 (276.0 examples/sec; 0.464 sec/batch)
2018-03-22 20:25:53.604537: step 63860, loss = 0.69 (259.2 examples/sec; 0.494 sec/batch)
2018-03-22 20:25:58.326328: step 63870, loss = 0.68 (271.1 examples/sec; 0.472 sec/batch)
2018-03-22 20:26:03.083503: step 63880, loss = 0.65 (269.1 examples/sec; 0.476 sec/batch)
2018-03-22 20:26:07.933356: step 63890, loss = 0.83 (263.9 examples/sec; 0.485 sec/batch)
2018-03-22 20:26:13.293405: step 63900, loss = 0.76 (238.8 examples/sec; 0.536 sec/batch)
2018-03-22 20:26:17.652485: step 63910, loss = 0.90 (293.6 examples/sec; 0.436 sec/batch)
2018-03-22 20:26:22.956325: step 63920, loss = 0.79 (241.3 examples/sec; 0.530 sec/batch)
2018-03-22 20:26:27.604432: step 63930, loss = 0.71 (275.4 examples/sec; 0.465 sec/batch)
2018-03-22 20:26:32.639844: step 63940, loss = 0.82 (254.2 examples/sec; 0.504 sec/batch)
2018-03-22 20:26:37.061397: step 63950, loss = 0.62 (289.5 examples/sec; 0.442 sec/batch)
2018-03-22 20:26:41.929520: step 63960, loss = 0.76 (262.9 examples/sec; 0.487 sec/batch)
2018-03-22 20:26:46.356376: step 63970, loss = 0.67 (289.1 examples/sec; 0.443 sec/batch)
2018-03-22 20:26:50.860702: step 63980, loss = 0.80 (284.2 examples/sec; 0.450 sec/batch)
2018-03-22 20:26:55.065402: step 63990, loss = 0.75 (304.4 examples/sec; 0.420 sec/batch)
2018-03-22 20:26:59.591380: step 64000, loss = 0.70 (282.8 examples/sec; 0.453 sec/batch)
2018-03-22 20:27:04.235916: step 64010, loss = 0.79 (275.6 examples/sec; 0.464 sec/batch)
2018-03-22 20:27:09.739400: step 64020, loss = 0.79 (232.6 examples/sec; 0.550 sec/batch)
2018-03-22 20:27:14.581404: step 64030, loss = 0.69 (264.4 examples/sec; 0.484 sec/batch)
2018-03-22 20:27:19.541328: step 64040, loss = 0.78 (258.1 examples/sec; 0.496 sec/batch)
2018-03-22 20:27:24.361452: step 64050, loss = 0.70 (265.6 examples/sec; 0.482 sec/batch)
2018-03-22 20:27:29.044719: step 64060, loss = 0.73 (273.3 examples/sec; 0.468 sec/batch)
2018-03-22 20:27:34.169165: step 64070, loss = 0.65 (249.8 examples/sec; 0.512 sec/batch)
2018-03-22 20:27:38.628366: step 64080, loss = 0.63 (287.0 examples/sec; 0.446 sec/batch)
2018-03-22 20:27:44.009439: step 64090, loss = 0.65 (237.9 examples/sec; 0.538 sec/batch)
2018-03-22 20:27:49.587859: step 64100, loss = 0.55 (229.5 examples/sec; 0.558 sec/batch)
2018-03-22 20:27:54.338557: step 64110, loss = 0.69 (269.4 examples/sec; 0.475 sec/batch)
2018-03-22 20:27:59.144406: step 64120, loss = 0.64 (266.3 examples/sec; 0.481 sec/batch)
2018-03-22 20:28:04.404417: step 64130, loss = 0.72 (243.3 examples/sec; 0.526 sec/batch)
2018-03-22 20:28:09.214447: step 64140, loss = 0.80 (266.1 examples/sec; 0.481 sec/batch)
2018-03-22 20:28:14.327754: step 64150, loss = 0.72 (250.3 examples/sec; 0.511 sec/batch)
2018-03-22 20:28:19.057406: step 64160, loss = 0.74 (270.6 examples/sec; 0.473 sec/batch)
2018-03-22 20:28:23.740380: step 64170, loss = 0.88 (273.3 examples/sec; 0.468 sec/batch)
2018-03-22 20:28:28.950632: step 64180, loss = 0.69 (245.7 examples/sec; 0.521 sec/batch)
2018-03-22 20:28:33.768355: step 64190, loss = 0.70 (265.7 examples/sec; 0.482 sec/batch)
2018-03-22 20:28:38.913706: step 64200, loss = 0.78 (248.8 examples/sec; 0.515 sec/batch)
2018-03-22 20:28:44.325911: step 64210, loss = 0.66 (236.5 examples/sec; 0.541 sec/batch)
2018-03-22 20:28:48.589206: step 64220, loss = 0.58 (300.2 examples/sec; 0.426 sec/batch)
2018-03-22 20:28:53.409415: step 64230, loss = 0.78 (265.5 examples/sec; 0.482 sec/batch)
2018-03-22 20:28:57.662339: step 64240, loss = 0.69 (301.0 examples/sec; 0.425 sec/batch)
2018-03-22 20:29:02.549751: step 64250, loss = 0.71 (261.9 examples/sec; 0.489 sec/batch)
2018-03-22 20:29:06.834469: step 64260, loss = 0.73 (298.7 examples/sec; 0.428 sec/batch)
2018-03-22 20:29:11.867461: step 64270, loss = 0.72 (254.3 examples/sec; 0.503 sec/batch)
2018-03-22 20:29:16.550096: step 64280, loss = 0.81 (273.4 examples/sec; 0.468 sec/batch)
2018-03-22 20:29:21.560427: step 64290, loss = 0.72 (255.5 examples/sec; 0.501 sec/batch)
2018-03-22 20:29:26.198913: step 64300, loss = 0.69 (276.0 examples/sec; 0.464 sec/batch)
2018-03-22 20:29:31.133617: step 64310, loss = 0.67 (259.4 examples/sec; 0.493 sec/batch)
2018-03-22 20:29:35.738867: step 64320, loss = 0.71 (277.9 examples/sec; 0.461 sec/batch)
2018-03-22 20:29:40.875442: step 64330, loss = 0.74 (249.2 examples/sec; 0.514 sec/batch)
2018-03-22 20:29:45.479439: step 64340, loss = 0.67 (278.0 examples/sec; 0.460 sec/batch)
2018-03-22 20:29:50.693572: step 64350, loss = 0.72 (245.5 examples/sec; 0.521 sec/batch)
2018-03-22 20:29:55.614417: step 64360, loss = 0.74 (260.1 examples/sec; 0.492 sec/batch)
2018-03-22 20:30:00.854389: step 64370, loss = 0.70 (244.3 examples/sec; 0.524 sec/batch)
2018-03-22 20:30:05.325362: step 64380, loss = 0.72 (286.3 examples/sec; 0.447 sec/batch)
2018-03-22 20:30:10.643222: step 64390, loss = 0.75 (240.7 examples/sec; 0.532 sec/batch)
2018-03-22 20:30:15.857751: step 64400, loss = 0.82 (245.5 examples/sec; 0.521 sec/batch)
2018-03-22 20:30:21.402410: step 64410, loss = 0.72 (230.9 examples/sec; 0.554 sec/batch)
2018-03-22 20:30:25.890421: step 64420, loss = 0.61 (285.2 examples/sec; 0.449 sec/batch)
2018-03-22 20:30:31.411625: step 64430, loss = 0.75 (231.8 examples/sec; 0.552 sec/batch)
2018-03-22 20:30:36.354288: step 64440, loss = 0.72 (259.0 examples/sec; 0.494 sec/batch)
2018-03-22 20:30:41.504448: step 64450, loss = 0.66 (248.5 examples/sec; 0.515 sec/batch)
2018-03-22 20:30:46.060061: step 64460, loss = 0.82 (281.0 examples/sec; 0.456 sec/batch)
2018-03-22 20:30:50.492629: step 64470, loss = 0.79 (288.8 examples/sec; 0.443 sec/batch)
2018-03-22 20:30:55.074355: step 64480, loss = 0.74 (279.4 examples/sec; 0.458 sec/batch)
2018-03-22 20:30:59.950424: step 64490, loss = 0.70 (262.5 examples/sec; 0.488 sec/batch)
2018-03-22 20:31:05.393146: step 64500, loss = 0.80 (235.2 examples/sec; 0.544 sec/batch)
2018-03-22 20:31:10.353094: step 64510, loss = 0.59 (258.1 examples/sec; 0.496 sec/batch)
2018-03-22 20:31:15.243569: step 64520, loss = 0.66 (261.7 examples/sec; 0.489 sec/batch)
2018-03-22 20:31:19.750836: step 64530, loss = 0.61 (284.0 examples/sec; 0.451 sec/batch)
2018-03-22 20:31:24.618833: step 64540, loss = 0.70 (262.9 examples/sec; 0.487 sec/batch)
2018-03-22 20:31:29.141443: step 64550, loss = 0.67 (283.0 examples/sec; 0.452 sec/batch)
2018-03-22 20:31:34.561387: step 64560, loss = 0.78 (236.2 examples/sec; 0.542 sec/batch)
2018-03-22 20:31:39.338509: step 64570, loss = 0.72 (267.9 examples/sec; 0.478 sec/batch)
2018-03-22 20:31:44.443720: step 64580, loss = 0.76 (250.7 examples/sec; 0.511 sec/batch)
2018-03-22 20:31:48.737819: step 64590, loss = 0.73 (298.1 examples/sec; 0.429 sec/batch)
2018-03-22 20:31:54.071626: step 64600, loss = 0.60 (240.0 examples/sec; 0.533 sec/batch)
2018-03-22 20:31:58.656123: step 64610, loss = 0.88 (279.2 examples/sec; 0.458 sec/batch)
2018-03-22 20:32:03.615426: step 64620, loss = 0.72 (258.1 examples/sec; 0.496 sec/batch)
2018-03-22 20:32:08.018383: step 64630, loss = 0.76 (290.7 examples/sec; 0.440 sec/batch)
2018-03-22 20:32:12.946446: step 64640, loss = 0.52 (259.7 examples/sec; 0.493 sec/batch)
2018-03-22 20:32:17.516245: step 64650, loss = 0.81 (280.1 examples/sec; 0.457 sec/batch)
2018-03-22 20:32:22.902410: step 64660, loss = 0.61 (237.6 examples/sec; 0.539 sec/batch)
2018-03-22 20:32:27.655420: step 64670, loss = 0.85 (269.3 examples/sec; 0.475 sec/batch)
2018-03-22 20:32:32.393473: step 64680, loss = 0.64 (270.2 examples/sec; 0.474 sec/batch)
2018-03-22 20:32:37.427480: step 64690, loss = 0.66 (254.3 examples/sec; 0.503 sec/batch)
2018-03-22 20:32:42.283847: step 64700, loss = 0.79 (263.6 examples/sec; 0.486 sec/batch)
2018-03-22 20:32:47.126507: step 64710, loss = 0.68 (264.3 examples/sec; 0.484 sec/batch)
2018-03-22 20:32:52.147411: step 64720, loss = 0.73 (254.9 examples/sec; 0.502 sec/batch)
2018-03-22 20:32:56.297820: step 64730, loss = 0.68 (308.4 examples/sec; 0.415 sec/batch)
2018-03-22 20:33:01.261330: step 64740, loss = 0.74 (257.9 examples/sec; 0.496 sec/batch)
2018-03-22 20:33:06.044352: step 64750, loss = 0.79 (267.6 examples/sec; 0.478 sec/batch)
2018-03-22 20:33:10.958282: step 64760, loss = 0.70 (260.5 examples/sec; 0.491 sec/batch)
2018-03-22 20:33:15.855261: step 64770, loss = 0.68 (261.4 examples/sec; 0.490 sec/batch)
2018-03-22 20:33:21.001399: step 64780, loss = 0.78 (248.7 examples/sec; 0.515 sec/batch)
2018-03-22 20:33:25.612422: step 64790, loss = 0.74 (277.6 examples/sec; 0.461 sec/batch)
2018-03-22 20:33:30.925427: step 64800, loss = 0.83 (240.9 examples/sec; 0.531 sec/batch)
2018-03-22 20:33:35.255716: step 64810, loss = 0.64 (295.6 examples/sec; 0.433 sec/batch)
2018-03-22 20:33:40.349904: step 64820, loss = 0.83 (251.3 examples/sec; 0.509 sec/batch)
2018-03-22 20:33:45.254435: step 64830, loss = 0.72 (261.0 examples/sec; 0.490 sec/batch)
2018-03-22 20:33:50.086413: step 64840, loss = 0.63 (264.9 examples/sec; 0.483 sec/batch)
2018-03-22 20:33:54.840718: step 64850, loss = 0.66 (269.2 examples/sec; 0.475 sec/batch)
2018-03-22 20:33:59.940358: step 64860, loss = 0.68 (251.0 examples/sec; 0.510 sec/batch)
2018-03-22 20:34:04.924453: step 64870, loss = 0.74 (256.8 examples/sec; 0.498 sec/batch)
2018-03-22 20:34:09.742559: step 64880, loss = 0.76 (265.7 examples/sec; 0.482 sec/batch)
2018-03-22 20:34:14.839391: step 64890, loss = 0.70 (251.1 examples/sec; 0.510 sec/batch)
2018-03-22 20:34:19.859108: step 64900, loss = 0.87 (255.0 examples/sec; 0.502 sec/batch)
2018-03-22 20:34:24.623479: step 64910, loss = 0.59 (268.7 examples/sec; 0.476 sec/batch)
2018-03-22 20:34:28.935109: step 64920, loss = 0.76 (296.9 examples/sec; 0.431 sec/batch)
2018-03-22 20:34:33.600365: step 64930, loss = 0.67 (274.4 examples/sec; 0.467 sec/batch)
2018-03-22 20:34:37.864023: step 64940, loss = 0.77 (300.2 examples/sec; 0.426 sec/batch)
2018-03-22 20:34:42.429794: step 64950, loss = 0.67 (280.3 examples/sec; 0.457 sec/batch)
2018-03-22 20:34:46.507170: step 64960, loss = 0.76 (313.9 examples/sec; 0.408 sec/batch)
2018-03-22 20:34:52.073300: step 64970, loss = 0.62 (230.0 examples/sec; 0.557 sec/batch)
2018-03-22 20:34:56.766416: step 64980, loss = 0.68 (272.7 examples/sec; 0.469 sec/batch)
2018-03-22 20:35:02.049454: step 64990, loss = 0.78 (242.3 examples/sec; 0.528 sec/batch)
2018-03-22 20:35:06.835689: step 65000, loss = 0.66 (267.4 examples/sec; 0.479 sec/batch)
2018-03-22 20:35:11.323295: step 65010, loss = 0.67 (285.2 examples/sec; 0.449 sec/batch)
2018-03-22 20:35:15.615514: step 65020, loss = 0.67 (298.2 examples/sec; 0.429 sec/batch)
2018-03-22 20:35:20.882466: step 65030, loss = 0.67 (243.0 examples/sec; 0.527 sec/batch)
2018-03-22 20:35:25.737168: step 65040, loss = 0.65 (263.7 examples/sec; 0.485 sec/batch)
2018-03-22 20:35:30.842509: step 65050, loss = 0.79 (250.7 examples/sec; 0.511 sec/batch)
2018-03-22 20:35:35.526636: step 65060, loss = 0.86 (273.3 examples/sec; 0.468 sec/batch)
2018-03-22 20:35:40.370688: step 65070, loss = 0.70 (264.2 examples/sec; 0.484 sec/batch)
2018-03-22 20:35:44.556040: step 65080, loss = 0.75 (305.8 examples/sec; 0.419 sec/batch)
2018-03-22 20:35:49.223560: step 65090, loss = 0.70 (274.2 examples/sec; 0.467 sec/batch)
2018-03-22 20:35:54.405687: step 65100, loss = 0.79 (247.0 examples/sec; 0.518 sec/batch)
2018-03-22 20:35:59.315998: step 65110, loss = 0.79 (260.7 examples/sec; 0.491 sec/batch)
2018-03-22 20:36:04.164432: step 65120, loss = 0.69 (264.0 examples/sec; 0.485 sec/batch)
2018-03-22 20:36:08.866800: step 65130, loss = 0.66 (272.2 examples/sec; 0.470 sec/batch)
2018-03-22 20:36:13.542309: step 65140, loss = 0.78 (273.8 examples/sec; 0.468 sec/batch)
2018-03-22 20:36:18.913510: step 65150, loss = 0.81 (238.3 examples/sec; 0.537 sec/batch)
2018-03-22 20:36:23.738377: step 65160, loss = 0.88 (265.3 examples/sec; 0.482 sec/batch)
2018-03-22 20:36:28.629446: step 65170, loss = 0.58 (261.7 examples/sec; 0.489 sec/batch)
2018-03-22 20:36:33.699622: step 65180, loss = 0.74 (252.5 examples/sec; 0.507 sec/batch)
2018-03-22 20:36:38.168269: step 65190, loss = 0.73 (286.4 examples/sec; 0.447 sec/batch)
2018-03-22 20:36:43.318330: step 65200, loss = 0.71 (248.5 examples/sec; 0.515 sec/batch)
2018-03-22 20:36:47.771629: step 65210, loss = 0.74 (287.4 examples/sec; 0.445 sec/batch)
2018-03-22 20:36:52.339324: step 65220, loss = 0.67 (280.2 examples/sec; 0.457 sec/batch)
2018-03-22 20:36:57.134404: step 65230, loss = 0.86 (266.9 examples/sec; 0.480 sec/batch)
2018-03-22 20:37:02.432358: step 65240, loss = 0.60 (241.6 examples/sec; 0.530 sec/batch)
2018-03-22 20:37:07.106332: step 65250, loss = 0.64 (273.9 examples/sec; 0.467 sec/batch)
2018-03-22 20:37:12.340331: step 65260, loss = 0.79 (244.6 examples/sec; 0.523 sec/batch)
2018-03-22 20:37:16.800381: step 65270, loss = 0.78 (287.0 examples/sec; 0.446 sec/batch)
2018-03-22 20:37:21.920543: step 65280, loss = 0.69 (250.0 examples/sec; 0.512 sec/batch)
2018-03-22 20:37:26.649490: step 65290, loss = 0.58 (270.7 examples/sec; 0.473 sec/batch)
2018-03-22 20:37:32.249716: step 65300, loss = 0.77 (228.6 examples/sec; 0.560 sec/batch)
2018-03-22 20:37:36.896214: step 65310, loss = 0.69 (275.5 examples/sec; 0.465 sec/batch)
2018-03-22 20:37:41.873664: step 65320, loss = 0.61 (257.1 examples/sec; 0.498 sec/batch)
2018-03-22 20:37:46.581376: step 65330, loss = 0.94 (271.9 examples/sec; 0.471 sec/batch)
2018-03-22 20:37:51.613412: step 65340, loss = 0.77 (254.4 examples/sec; 0.503 sec/batch)
2018-03-22 20:37:56.398802: step 65350, loss = 0.74 (267.5 examples/sec; 0.479 sec/batch)
2018-03-22 20:38:01.621662: step 65360, loss = 0.80 (245.1 examples/sec; 0.522 sec/batch)
2018-03-22 20:38:06.171366: step 65370, loss = 0.61 (281.3 examples/sec; 0.455 sec/batch)
2018-03-22 20:38:10.695378: step 65380, loss = 0.79 (282.9 examples/sec; 0.452 sec/batch)
2018-03-22 20:38:15.397587: step 65390, loss = 0.72 (272.2 examples/sec; 0.470 sec/batch)
2018-03-22 20:38:20.432232: step 65400, loss = 0.71 (254.2 examples/sec; 0.503 sec/batch)
2018-03-22 20:38:25.241401: step 65410, loss = 0.68 (266.2 examples/sec; 0.481 sec/batch)
2018-03-22 20:38:29.667399: step 65420, loss = 0.60 (289.2 examples/sec; 0.443 sec/batch)
2018-03-22 20:38:33.952500: step 65430, loss = 0.72 (298.7 examples/sec; 0.429 sec/batch)
2018-03-22 20:38:38.468370: step 65440, loss = 0.84 (283.4 examples/sec; 0.452 sec/batch)
2018-03-22 20:38:43.675375: step 65450, loss = 0.70 (245.8 examples/sec; 0.521 sec/batch)
2018-03-22 20:38:48.511365: step 65460, loss = 0.83 (264.7 examples/sec; 0.484 sec/batch)
2018-03-22 20:38:53.795486: step 65470, loss = 0.68 (242.2 examples/sec; 0.528 sec/batch)
2018-03-22 20:38:58.898109: step 65480, loss = 0.67 (250.9 examples/sec; 0.510 sec/batch)
2018-03-22 20:39:03.444384: step 65490, loss = 0.63 (281.6 examples/sec; 0.455 sec/batch)
2018-03-22 20:39:08.113605: step 65500, loss = 0.70 (274.1 examples/sec; 0.467 sec/batch)
2018-03-22 20:39:12.977357: step 65510, loss = 0.71 (263.2 examples/sec; 0.486 sec/batch)
2018-03-22 20:39:17.985386: step 65520, loss = 0.75 (255.6 examples/sec; 0.501 sec/batch)
2018-03-22 20:39:22.795651: step 65530, loss = 0.65 (266.1 examples/sec; 0.481 sec/batch)
2018-03-22 20:39:27.517675: step 65540, loss = 0.74 (271.1 examples/sec; 0.472 sec/batch)
2018-03-22 20:39:31.971203: step 65550, loss = 0.67 (287.4 examples/sec; 0.445 sec/batch)
2018-03-22 20:39:36.415603: step 65560, loss = 0.78 (288.0 examples/sec; 0.444 sec/batch)
2018-03-22 20:39:41.331446: step 65570, loss = 0.78 (260.4 examples/sec; 0.492 sec/batch)
2018-03-22 20:39:45.898394: step 65580, loss = 0.70 (280.3 examples/sec; 0.457 sec/batch)
2018-03-22 20:39:51.205355: step 65590, loss = 0.68 (241.2 examples/sec; 0.531 sec/batch)
2018-03-22 20:39:55.900525: step 65600, loss = 0.84 (272.6 examples/sec; 0.470 sec/batch)
2018-03-22 20:40:00.723162: step 65610, loss = 0.93 (265.4 examples/sec; 0.482 sec/batch)
2018-03-22 20:40:05.291356: step 65620, loss = 0.62 (280.2 examples/sec; 0.457 sec/batch)
2018-03-22 20:40:10.618385: step 65630, loss = 0.86 (240.3 examples/sec; 0.533 sec/batch)
2018-03-22 20:40:16.003404: step 65640, loss = 0.63 (237.7 examples/sec; 0.539 sec/batch)
2018-03-22 20:40:20.777607: step 65650, loss = 0.85 (268.1 examples/sec; 0.477 sec/batch)
2018-03-22 20:40:25.475098: step 65660, loss = 0.81 (272.5 examples/sec; 0.470 sec/batch)
2018-03-22 20:40:30.328236: step 65670, loss = 0.65 (263.7 examples/sec; 0.485 sec/batch)
2018-03-22 20:40:35.363598: step 65680, loss = 0.78 (254.2 examples/sec; 0.504 sec/batch)
2018-03-22 20:40:40.332415: step 65690, loss = 0.54 (257.6 examples/sec; 0.497 sec/batch)
2018-03-22 20:40:44.789100: step 65700, loss = 0.80 (287.2 examples/sec; 0.446 sec/batch)
2018-03-22 20:40:49.878384: step 65710, loss = 0.70 (251.5 examples/sec; 0.509 sec/batch)
2018-03-22 20:40:54.706442: step 65720, loss = 0.66 (265.1 examples/sec; 0.483 sec/batch)
2018-03-22 20:40:59.619833: step 65730, loss = 0.70 (260.5 examples/sec; 0.491 sec/batch)
2018-03-22 20:41:04.564351: step 65740, loss = 0.74 (258.9 examples/sec; 0.494 sec/batch)
2018-03-22 20:41:09.707008: step 65750, loss = 0.71 (248.9 examples/sec; 0.514 sec/batch)
2018-03-22 20:41:14.665390: step 65760, loss = 0.74 (258.1 examples/sec; 0.496 sec/batch)
2018-03-22 20:41:19.917421: step 65770, loss = 0.59 (243.7 examples/sec; 0.525 sec/batch)
2018-03-22 20:41:24.781545: step 65780, loss = 0.77 (263.2 examples/sec; 0.486 sec/batch)
2018-03-22 20:41:29.915354: step 65790, loss = 0.79 (249.3 examples/sec; 0.513 sec/batch)
2018-03-22 20:41:35.002806: step 65800, loss = 0.71 (251.6 examples/sec; 0.509 sec/batch)
2018-03-22 20:41:39.993335: step 65810, loss = 0.70 (256.5 examples/sec; 0.499 sec/batch)
2018-03-22 20:41:44.663406: step 65820, loss = 0.77 (274.1 examples/sec; 0.467 sec/batch)
2018-03-22 20:41:50.475416: step 65830, loss = 0.91 (220.2 examples/sec; 0.581 sec/batch)
2018-03-22 20:41:56.091380: step 65840, loss = 0.84 (227.9 examples/sec; 0.562 sec/batch)
2018-03-22 20:42:01.218387: step 65850, loss = 0.65 (249.7 examples/sec; 0.513 sec/batch)
2018-03-22 20:42:05.824402: step 65860, loss = 0.69 (277.9 examples/sec; 0.461 sec/batch)
2018-03-22 20:42:11.184359: step 65870, loss = 0.77 (238.8 examples/sec; 0.536 sec/batch)
2018-03-22 20:42:16.047270: step 65880, loss = 0.95 (263.2 examples/sec; 0.486 sec/batch)
2018-03-22 20:42:20.901390: step 65890, loss = 0.60 (263.7 examples/sec; 0.485 sec/batch)
2018-03-22 20:42:25.599856: step 65900, loss = 0.76 (272.4 examples/sec; 0.470 sec/batch)
2018-03-22 20:42:30.979688: step 65910, loss = 0.64 (237.9 examples/sec; 0.538 sec/batch)
2018-03-22 20:42:35.979455: step 65920, loss = 0.76 (256.0 examples/sec; 0.500 sec/batch)
2018-03-22 20:42:41.112596: step 65930, loss = 0.73 (249.4 examples/sec; 0.513 sec/batch)
2018-03-22 20:42:46.165434: step 65940, loss = 0.64 (253.3 examples/sec; 0.505 sec/batch)
2018-03-22 20:42:51.616513: step 65950, loss = 0.64 (234.8 examples/sec; 0.545 sec/batch)
2018-03-22 20:42:56.311318: step 65960, loss = 0.84 (272.6 examples/sec; 0.469 sec/batch)
2018-03-22 20:43:01.408946: step 65970, loss = 0.72 (251.1 examples/sec; 0.510 sec/batch)
2018-03-22 20:43:06.199415: step 65980, loss = 0.74 (267.2 examples/sec; 0.479 sec/batch)
2018-03-22 20:43:11.532404: step 65990, loss = 0.70 (240.0 examples/sec; 0.533 sec/batch)
2018-03-22 20:43:16.562535: step 66000, loss = 0.68 (254.5 examples/sec; 0.503 sec/batch)
2018-03-22 20:43:21.712215: step 66010, loss = 0.70 (248.6 examples/sec; 0.515 sec/batch)
2018-03-22 20:43:26.844460: step 66020, loss = 0.66 (249.4 examples/sec; 0.513 sec/batch)
2018-03-22 20:43:31.629419: step 66030, loss = 0.73 (267.5 examples/sec; 0.478 sec/batch)
2018-03-22 20:43:36.719501: step 66040, loss = 0.67 (251.5 examples/sec; 0.509 sec/batch)
2018-03-22 20:43:41.950402: step 66050, loss = 0.71 (244.7 examples/sec; 0.523 sec/batch)
2018-03-22 20:43:46.471398: step 66060, loss = 0.68 (283.1 examples/sec; 0.452 sec/batch)
2018-03-22 20:43:51.824035: step 66070, loss = 0.77 (239.1 examples/sec; 0.535 sec/batch)
2018-03-22 20:43:56.599401: step 66080, loss = 0.79 (268.0 examples/sec; 0.478 sec/batch)
2018-03-22 20:44:02.038661: step 66090, loss = 0.67 (235.3 examples/sec; 0.544 sec/batch)
2018-03-22 20:44:06.970537: step 66100, loss = 0.59 (259.5 examples/sec; 0.493 sec/batch)
2018-03-22 20:44:12.482390: step 66110, loss = 0.76 (232.2 examples/sec; 0.551 sec/batch)
2018-03-22 20:44:17.357690: step 66120, loss = 0.68 (262.5 examples/sec; 0.488 sec/batch)
2018-03-22 20:44:22.271415: step 66130, loss = 0.80 (260.5 examples/sec; 0.491 sec/batch)
2018-03-22 20:44:26.425370: step 66140, loss = 0.65 (308.1 examples/sec; 0.415 sec/batch)
2018-03-22 20:44:31.454466: step 66150, loss = 0.74 (254.5 examples/sec; 0.503 sec/batch)
2018-03-22 20:44:36.281400: step 66160, loss = 0.69 (265.2 examples/sec; 0.483 sec/batch)
2018-03-22 20:44:41.221423: step 66170, loss = 0.59 (259.1 examples/sec; 0.494 sec/batch)
2018-03-22 20:44:46.004421: step 66180, loss = 0.66 (267.6 examples/sec; 0.478 sec/batch)
2018-03-22 20:44:51.532234: step 66190, loss = 0.72 (231.6 examples/sec; 0.553 sec/batch)
2018-03-22 20:44:57.105352: step 66200, loss = 0.74 (229.7 examples/sec; 0.557 sec/batch)
2018-03-22 20:45:02.455360: step 66210, loss = 0.71 (239.3 examples/sec; 0.535 sec/batch)
2018-03-22 20:45:07.282401: step 66220, loss = 0.82 (265.2 examples/sec; 0.483 sec/batch)
2018-03-22 20:45:12.507330: step 66230, loss = 0.70 (245.0 examples/sec; 0.522 sec/batch)
2018-03-22 20:45:17.733417: step 66240, loss = 0.71 (244.9 examples/sec; 0.523 sec/batch)
2018-03-22 20:45:23.029357: step 66250, loss = 0.66 (241.7 examples/sec; 0.530 sec/batch)
2018-03-22 20:45:27.336435: step 66260, loss = 0.80 (297.2 examples/sec; 0.431 sec/batch)
2018-03-22 20:45:32.331319: step 66270, loss = 0.63 (256.3 examples/sec; 0.499 sec/batch)
2018-03-22 20:45:37.789380: step 66280, loss = 0.90 (234.5 examples/sec; 0.546 sec/batch)
2018-03-22 20:45:42.920401: step 66290, loss = 0.70 (249.5 examples/sec; 0.513 sec/batch)
2018-03-22 20:45:48.423758: step 66300, loss = 0.59 (232.6 examples/sec; 0.550 sec/batch)
2018-03-22 20:45:53.647356: step 66310, loss = 0.75 (245.0 examples/sec; 0.522 sec/batch)
2018-03-22 20:45:58.485909: step 66320, loss = 0.68 (264.5 examples/sec; 0.484 sec/batch)
2018-03-22 20:46:03.501906: step 66330, loss = 0.61 (255.2 examples/sec; 0.502 sec/batch)
2018-03-22 20:46:08.266350: step 66340, loss = 0.84 (268.7 examples/sec; 0.476 sec/batch)
2018-03-22 20:46:13.446483: step 66350, loss = 0.78 (247.1 examples/sec; 0.518 sec/batch)
2018-03-22 20:46:17.688358: step 66360, loss = 0.66 (301.8 examples/sec; 0.424 sec/batch)
2018-03-22 20:46:22.801382: step 66370, loss = 0.84 (250.3 examples/sec; 0.511 sec/batch)
2018-03-22 20:46:28.146463: step 66380, loss = 0.76 (239.5 examples/sec; 0.535 sec/batch)
2018-03-22 20:46:33.337343: step 66390, loss = 0.65 (246.6 examples/sec; 0.519 sec/batch)
2018-03-22 20:46:38.237978: step 66400, loss = 0.80 (261.2 examples/sec; 0.490 sec/batch)
2018-03-22 20:46:43.336335: step 66410, loss = 0.64 (251.1 examples/sec; 0.510 sec/batch)
2018-03-22 20:46:47.937715: step 66420, loss = 0.67 (278.2 examples/sec; 0.460 sec/batch)
2018-03-22 20:46:53.330250: step 66430, loss = 0.58 (237.4 examples/sec; 0.539 sec/batch)
2018-03-22 20:46:58.470180: step 66440, loss = 0.70 (249.0 examples/sec; 0.514 sec/batch)
2018-03-22 20:47:03.299316: step 66450, loss = 0.66 (265.1 examples/sec; 0.483 sec/batch)
2018-03-22 20:47:08.420411: step 66460, loss = 0.64 (249.9 examples/sec; 0.512 sec/batch)
2018-03-22 20:47:13.307356: step 66470, loss = 0.73 (261.9 examples/sec; 0.489 sec/batch)
2018-03-22 20:47:18.351914: step 66480, loss = 0.80 (253.7 examples/sec; 0.504 sec/batch)
2018-03-22 20:47:23.716354: step 66490, loss = 0.71 (238.6 examples/sec; 0.536 sec/batch)
2018-03-22 20:47:28.552048: step 66500, loss = 0.75 (264.7 examples/sec; 0.484 sec/batch)
2018-03-22 20:47:33.677466: step 66510, loss = 0.68 (249.7 examples/sec; 0.513 sec/batch)
2018-03-22 20:47:38.447368: step 66520, loss = 0.62 (268.3 examples/sec; 0.477 sec/batch)
2018-03-22 20:47:43.469635: step 66530, loss = 0.72 (254.9 examples/sec; 0.502 sec/batch)
2018-03-22 20:47:48.803866: step 66540, loss = 0.71 (240.0 examples/sec; 0.533 sec/batch)
2018-03-22 20:47:53.694713: step 66550, loss = 0.72 (261.7 examples/sec; 0.489 sec/batch)
2018-03-22 20:47:58.576329: step 66560, loss = 0.74 (262.2 examples/sec; 0.488 sec/batch)
2018-03-22 20:48:03.680443: step 66570, loss = 0.60 (250.8 examples/sec; 0.510 sec/batch)
2018-03-22 20:48:08.978828: step 66580, loss = 0.67 (241.6 examples/sec; 0.530 sec/batch)
2018-03-22 20:48:12.949904: step 66590, loss = 0.76 (322.3 examples/sec; 0.397 sec/batch)
2018-03-22 20:48:17.609464: step 66600, loss = 0.92 (274.7 examples/sec; 0.466 sec/batch)
2018-03-22 20:48:22.099566: step 66610, loss = 0.58 (285.1 examples/sec; 0.449 sec/batch)
2018-03-22 20:48:26.646426: step 66620, loss = 0.73 (281.5 examples/sec; 0.455 sec/batch)
2018-03-22 20:48:31.494379: step 66630, loss = 0.63 (264.0 examples/sec; 0.485 sec/batch)
2018-03-22 20:48:36.772505: step 66640, loss = 0.77 (242.5 examples/sec; 0.528 sec/batch)
2018-03-22 20:48:42.190382: step 66650, loss = 0.81 (236.3 examples/sec; 0.542 sec/batch)
2018-03-22 20:48:47.371086: step 66660, loss = 0.61 (247.1 examples/sec; 0.518 sec/batch)
2018-03-22 20:48:52.151372: step 66670, loss = 0.77 (267.8 examples/sec; 0.478 sec/batch)
2018-03-22 20:48:57.241114: step 66680, loss = 0.66 (251.5 examples/sec; 0.509 sec/batch)
2018-03-22 20:49:02.155549: step 66690, loss = 0.72 (260.5 examples/sec; 0.491 sec/batch)
2018-03-22 20:49:07.381974: step 66700, loss = 0.70 (244.9 examples/sec; 0.523 sec/batch)
2018-03-22 20:49:12.445409: step 66710, loss = 0.58 (252.8 examples/sec; 0.506 sec/batch)
2018-03-22 20:49:17.365465: step 66720, loss = 0.80 (260.2 examples/sec; 0.492 sec/batch)
2018-03-22 20:49:22.457409: step 66730, loss = 0.72 (251.4 examples/sec; 0.509 sec/batch)
2018-03-22 20:49:27.700422: step 66740, loss = 0.66 (244.1 examples/sec; 0.524 sec/batch)
2018-03-22 20:49:33.070784: step 66750, loss = 0.93 (238.3 examples/sec; 0.537 sec/batch)
2018-03-22 20:49:38.306407: step 66760, loss = 0.67 (244.5 examples/sec; 0.524 sec/batch)
2018-03-22 20:49:43.504344: step 66770, loss = 0.64 (246.3 examples/sec; 0.520 sec/batch)
2018-03-22 20:49:48.862548: step 66780, loss = 0.66 (238.9 examples/sec; 0.536 sec/batch)
2018-03-22 20:49:54.019510: step 66790, loss = 0.73 (248.2 examples/sec; 0.516 sec/batch)
2018-03-22 20:49:59.412623: step 66800, loss = 0.65 (237.3 examples/sec; 0.539 sec/batch)
2018-03-22 20:50:04.382368: step 66810, loss = 0.71 (257.6 examples/sec; 0.497 sec/batch)
2018-03-22 20:50:08.796384: step 66820, loss = 0.62 (290.0 examples/sec; 0.441 sec/batch)
2018-03-22 20:50:13.784404: step 66830, loss = 0.85 (256.6 examples/sec; 0.499 sec/batch)
2018-03-22 20:50:19.106126: step 66840, loss = 0.74 (240.5 examples/sec; 0.532 sec/batch)
2018-03-22 20:50:23.747105: step 66850, loss = 0.69 (275.8 examples/sec; 0.464 sec/batch)
2018-03-22 20:50:28.971406: step 66860, loss = 0.76 (245.0 examples/sec; 0.522 sec/batch)
2018-03-22 20:50:34.507506: step 66870, loss = 0.67 (231.2 examples/sec; 0.554 sec/batch)
2018-03-22 20:50:39.110938: step 66880, loss = 0.81 (278.1 examples/sec; 0.460 sec/batch)
2018-03-22 20:50:43.896453: step 66890, loss = 0.81 (267.5 examples/sec; 0.479 sec/batch)
2018-03-22 20:50:49.554223: step 66900, loss = 0.75 (226.2 examples/sec; 0.566 sec/batch)
2018-03-22 20:50:54.352337: step 66910, loss = 0.85 (266.8 examples/sec; 0.480 sec/batch)
2018-03-22 20:50:59.318393: step 66920, loss = 0.62 (257.7 examples/sec; 0.497 sec/batch)
2018-03-22 20:51:04.275558: step 66930, loss = 0.56 (258.2 examples/sec; 0.496 sec/batch)
2018-03-22 20:51:08.770578: step 66940, loss = 0.67 (284.8 examples/sec; 0.450 sec/batch)
2018-03-22 20:51:13.795373: step 66950, loss = 0.85 (254.7 examples/sec; 0.502 sec/batch)
2018-03-22 20:51:18.957230: step 66960, loss = 0.82 (248.0 examples/sec; 0.516 sec/batch)
2018-03-22 20:51:23.777614: step 66970, loss = 0.84 (265.5 examples/sec; 0.482 sec/batch)
2018-03-22 20:51:28.889628: step 66980, loss = 0.69 (250.4 examples/sec; 0.511 sec/batch)
2018-03-22 20:51:33.958320: step 66990, loss = 0.78 (252.5 examples/sec; 0.507 sec/batch)
2018-03-22 20:51:39.325118: step 67000, loss = 0.68 (238.5 examples/sec; 0.537 sec/batch)
2018-03-22 20:51:44.101404: step 67010, loss = 0.73 (268.0 examples/sec; 0.478 sec/batch)
2018-03-22 20:51:49.137424: step 67020, loss = 0.66 (254.2 examples/sec; 0.504 sec/batch)
2018-03-22 20:51:54.441395: step 67030, loss = 0.69 (241.3 examples/sec; 0.530 sec/batch)
2018-03-22 20:51:59.011385: step 67040, loss = 0.86 (280.1 examples/sec; 0.457 sec/batch)
2018-03-22 20:52:04.111475: step 67050, loss = 0.76 (251.0 examples/sec; 0.510 sec/batch)
2018-03-22 20:52:08.733926: step 67060, loss = 0.76 (276.9 examples/sec; 0.462 sec/batch)
2018-03-22 20:52:13.714601: step 67070, loss = 0.86 (257.0 examples/sec; 0.498 sec/batch)
2018-03-22 20:52:18.109907: step 67080, loss = 0.74 (291.2 examples/sec; 0.440 sec/batch)
2018-03-22 20:52:22.980375: step 67090, loss = 0.76 (262.8 examples/sec; 0.487 sec/batch)
2018-03-22 20:52:28.241809: step 67100, loss = 0.78 (243.3 examples/sec; 0.526 sec/batch)
2018-03-22 20:52:33.712385: step 67110, loss = 0.67 (234.0 examples/sec; 0.547 sec/batch)
2018-03-22 20:52:38.344440: step 67120, loss = 0.88 (276.3 examples/sec; 0.463 sec/batch)
2018-03-22 20:52:43.047550: step 67130, loss = 0.69 (272.2 examples/sec; 0.470 sec/batch)
2018-03-22 20:52:48.061503: step 67140, loss = 0.67 (255.3 examples/sec; 0.501 sec/batch)
2018-03-22 20:52:53.210396: step 67150, loss = 0.78 (248.6 examples/sec; 0.515 sec/batch)
2018-03-22 20:52:58.562416: step 67160, loss = 0.59 (239.2 examples/sec; 0.535 sec/batch)
2018-03-22 20:53:03.506502: step 67170, loss = 0.63 (258.9 examples/sec; 0.494 sec/batch)
2018-03-22 20:53:08.164152: step 67180, loss = 0.82 (274.8 examples/sec; 0.466 sec/batch)
2018-03-22 20:53:13.230476: step 67190, loss = 0.65 (252.6 examples/sec; 0.507 sec/batch)
2018-03-22 20:53:18.390173: step 67200, loss = 0.73 (248.1 examples/sec; 0.516 sec/batch)
2018-03-22 20:53:23.630901: step 67210, loss = 0.75 (244.2 examples/sec; 0.524 sec/batch)
2018-03-22 20:53:28.843459: step 67220, loss = 0.84 (245.6 examples/sec; 0.521 sec/batch)
2018-03-22 20:53:33.838383: step 67230, loss = 0.62 (256.3 examples/sec; 0.499 sec/batch)
2018-03-22 20:53:38.788053: step 67240, loss = 0.88 (258.6 examples/sec; 0.495 sec/batch)
2018-03-22 20:53:44.164175: step 67250, loss = 0.68 (238.1 examples/sec; 0.538 sec/batch)
2018-03-22 20:53:49.087122: step 67260, loss = 0.63 (260.0 examples/sec; 0.492 sec/batch)
2018-03-22 20:53:53.841418: step 67270, loss = 0.80 (269.2 examples/sec; 0.475 sec/batch)
2018-03-22 20:53:58.965207: step 67280, loss = 0.63 (249.8 examples/sec; 0.512 sec/batch)
2018-03-22 20:54:03.850196: step 67290, loss = 0.70 (262.0 examples/sec; 0.488 sec/batch)
2018-03-22 20:54:08.060273: step 67300, loss = 0.71 (304.0 examples/sec; 0.421 sec/batch)
2018-03-22 20:54:12.909531: step 67310, loss = 0.88 (264.0 examples/sec; 0.485 sec/batch)
2018-03-22 20:54:18.248389: step 67320, loss = 0.60 (239.8 examples/sec; 0.534 sec/batch)
2018-03-22 20:54:23.036717: step 67330, loss = 0.78 (267.3 examples/sec; 0.479 sec/batch)
2018-03-22 20:54:28.366425: step 67340, loss = 0.80 (240.2 examples/sec; 0.533 sec/batch)
2018-03-22 20:54:33.073286: step 67350, loss = 0.63 (271.9 examples/sec; 0.471 sec/batch)
2018-03-22 20:54:37.552484: step 67360, loss = 0.74 (285.8 examples/sec; 0.448 sec/batch)
2018-03-22 20:54:42.939460: step 67370, loss = 0.77 (237.6 examples/sec; 0.539 sec/batch)
2018-03-22 20:54:48.001410: step 67380, loss = 0.80 (252.9 examples/sec; 0.506 sec/batch)
2018-03-22 20:54:53.250451: step 67390, loss = 0.72 (243.9 examples/sec; 0.525 sec/batch)
2018-03-22 20:54:58.422503: step 67400, loss = 0.64 (247.5 examples/sec; 0.517 sec/batch)
2018-03-22 20:55:03.979972: step 67410, loss = 0.93 (230.3 examples/sec; 0.556 sec/batch)
2018-03-22 20:55:08.577415: step 67420, loss = 0.78 (278.4 examples/sec; 0.460 sec/batch)
2018-03-22 20:55:13.688414: step 67430, loss = 0.70 (250.4 examples/sec; 0.511 sec/batch)
2018-03-22 20:55:18.760393: step 67440, loss = 0.70 (252.4 examples/sec; 0.507 sec/batch)
2018-03-22 20:55:24.012230: step 67450, loss = 0.75 (243.7 examples/sec; 0.525 sec/batch)
2018-03-22 20:55:28.718438: step 67460, loss = 0.63 (272.0 examples/sec; 0.471 sec/batch)
2018-03-22 20:55:33.764438: step 67470, loss = 0.69 (253.7 examples/sec; 0.505 sec/batch)
2018-03-22 20:55:38.720911: step 67480, loss = 0.61 (258.2 examples/sec; 0.496 sec/batch)
2018-03-22 20:55:43.947396: step 67490, loss = 0.70 (244.9 examples/sec; 0.523 sec/batch)
2018-03-22 20:55:49.016086: step 67500, loss = 0.72 (252.5 examples/sec; 0.507 sec/batch)
2018-03-22 20:55:54.066356: step 67510, loss = 0.75 (253.5 examples/sec; 0.505 sec/batch)
2018-03-22 20:55:58.518049: step 67520, loss = 0.73 (287.5 examples/sec; 0.445 sec/batch)
2018-03-22 20:56:03.246783: step 67530, loss = 0.77 (270.7 examples/sec; 0.473 sec/batch)
2018-03-22 20:56:07.682721: step 67540, loss = 0.63 (288.5 examples/sec; 0.444 sec/batch)
2018-03-22 20:56:12.637368: step 67550, loss = 0.70 (258.3 examples/sec; 0.495 sec/batch)
2018-03-22 20:56:17.415392: step 67560, loss = 0.75 (267.9 examples/sec; 0.478 sec/batch)
2018-03-22 20:56:22.218392: step 67570, loss = 0.59 (266.5 examples/sec; 0.480 sec/batch)
2018-03-22 20:56:27.107439: step 67580, loss = 0.60 (261.8 examples/sec; 0.489 sec/batch)
2018-03-22 20:56:32.194533: step 67590, loss = 0.61 (251.6 examples/sec; 0.509 sec/batch)
2018-03-22 20:56:37.088716: step 67600, loss = 0.77 (261.5 examples/sec; 0.489 sec/batch)
2018-03-22 20:56:42.842373: step 67610, loss = 0.81 (222.5 examples/sec; 0.575 sec/batch)
2018-03-22 20:56:47.663361: step 67620, loss = 0.68 (265.5 examples/sec; 0.482 sec/batch)
2018-03-22 20:56:53.138676: step 67630, loss = 0.79 (233.8 examples/sec; 0.548 sec/batch)
2018-03-22 20:56:58.078042: step 67640, loss = 0.76 (259.1 examples/sec; 0.494 sec/batch)
2018-03-22 20:57:03.303942: step 67650, loss = 0.85 (244.9 examples/sec; 0.523 sec/batch)
2018-03-22 20:57:08.202449: step 67660, loss = 0.83 (261.3 examples/sec; 0.490 sec/batch)
2018-03-22 20:57:13.509416: step 67670, loss = 0.70 (241.2 examples/sec; 0.531 sec/batch)
2018-03-22 20:57:18.463421: step 67680, loss = 0.58 (258.4 examples/sec; 0.495 sec/batch)
2018-03-22 20:57:23.595374: step 67690, loss = 0.64 (249.4 examples/sec; 0.513 sec/batch)
2018-03-22 20:57:28.896804: step 67700, loss = 0.79 (241.4 examples/sec; 0.530 sec/batch)
2018-03-22 20:57:33.937446: step 67710, loss = 0.75 (253.9 examples/sec; 0.504 sec/batch)
2018-03-22 20:57:39.292459: step 67720, loss = 0.74 (239.0 examples/sec; 0.535 sec/batch)
2018-03-22 20:57:44.394430: step 67730, loss = 0.70 (250.9 examples/sec; 0.510 sec/batch)
2018-03-22 20:57:48.809451: step 67740, loss = 0.77 (289.9 examples/sec; 0.442 sec/batch)
2018-03-22 20:57:53.627172: step 67750, loss = 0.71 (265.7 examples/sec; 0.482 sec/batch)
2018-03-22 20:57:58.247355: step 67760, loss = 0.78 (277.0 examples/sec; 0.462 sec/batch)
2018-03-22 20:58:03.610409: step 67770, loss = 0.68 (238.7 examples/sec; 0.536 sec/batch)
2018-03-22 20:58:08.857440: step 67780, loss = 0.80 (243.9 examples/sec; 0.525 sec/batch)
2018-03-22 20:58:13.910272: step 67790, loss = 0.67 (253.3 examples/sec; 0.505 sec/batch)
2018-03-22 20:58:19.079492: step 67800, loss = 0.72 (247.6 examples/sec; 0.517 sec/batch)
2018-03-22 20:58:23.246387: step 67810, loss = 0.84 (307.2 examples/sec; 0.417 sec/batch)
2018-03-22 20:58:28.155374: step 67820, loss = 0.82 (260.7 examples/sec; 0.491 sec/batch)
2018-03-22 20:58:33.696464: step 67830, loss = 0.60 (231.0 examples/sec; 0.554 sec/batch)
2018-03-22 20:58:38.668390: step 67840, loss = 0.65 (257.4 examples/sec; 0.497 sec/batch)
2018-03-22 20:58:44.607424: step 67850, loss = 0.77 (215.5 examples/sec; 0.594 sec/batch)
2018-03-22 20:58:48.910381: step 67860, loss = 0.73 (297.5 examples/sec; 0.430 sec/batch)
2018-03-22 20:58:53.528403: step 67870, loss = 0.72 (277.2 examples/sec; 0.462 sec/batch)
2018-03-22 20:58:58.745395: step 67880, loss = 0.78 (245.4 examples/sec; 0.522 sec/batch)
2018-03-22 20:59:03.815426: step 67890, loss = 0.69 (252.5 examples/sec; 0.507 sec/batch)
2018-03-22 20:59:09.111471: step 67900, loss = 0.53 (241.7 examples/sec; 0.530 sec/batch)
2018-03-22 20:59:14.114908: step 67910, loss = 0.66 (255.8 examples/sec; 0.500 sec/batch)
2018-03-22 20:59:19.455247: step 67920, loss = 0.65 (239.7 examples/sec; 0.534 sec/batch)
2018-03-22 20:59:24.794448: step 67930, loss = 0.78 (239.7 examples/sec; 0.534 sec/batch)
2018-03-22 20:59:29.651412: step 67940, loss = 0.63 (263.5 examples/sec; 0.486 sec/batch)
2018-03-22 20:59:34.516390: step 67950, loss = 0.73 (263.1 examples/sec; 0.486 sec/batch)
2018-03-22 20:59:39.874125: step 67960, loss = 0.70 (238.9 examples/sec; 0.536 sec/batch)
2018-03-22 20:59:44.799708: step 67970, loss = 0.68 (259.9 examples/sec; 0.493 sec/batch)
2018-03-22 20:59:50.006284: step 67980, loss = 0.70 (245.8 examples/sec; 0.521 sec/batch)
2018-03-22 20:59:54.560860: step 67990, loss = 0.59 (281.0 examples/sec; 0.455 sec/batch)
2018-03-22 20:59:59.905778: step 68000, loss = 0.85 (239.5 examples/sec; 0.534 sec/batch)
2018-03-22 21:00:05.231164: step 68010, loss = 0.74 (240.4 examples/sec; 0.533 sec/batch)
2018-03-22 21:00:10.280562: step 68020, loss = 0.88 (253.5 examples/sec; 0.505 sec/batch)
2018-03-22 21:00:15.582386: step 68030, loss = 0.71 (241.4 examples/sec; 0.530 sec/batch)
2018-03-22 21:00:20.931439: step 68040, loss = 0.74 (239.3 examples/sec; 0.535 sec/batch)
2018-03-22 21:00:25.702374: step 68050, loss = 0.72 (268.3 examples/sec; 0.477 sec/batch)
2018-03-22 21:00:30.872401: step 68060, loss = 0.82 (247.6 examples/sec; 0.517 sec/batch)
2018-03-22 21:00:35.627389: step 68070, loss = 0.79 (269.2 examples/sec; 0.475 sec/batch)
2018-03-22 21:00:40.816438: step 68080, loss = 0.83 (246.7 examples/sec; 0.519 sec/batch)
2018-03-22 21:00:45.500443: step 68090, loss = 0.75 (273.3 examples/sec; 0.468 sec/batch)
2018-03-22 21:00:50.732303: step 68100, loss = 0.64 (244.7 examples/sec; 0.523 sec/batch)
2018-03-22 21:00:55.618648: step 68110, loss = 0.74 (262.0 examples/sec; 0.489 sec/batch)
2018-03-22 21:01:00.771431: step 68120, loss = 0.72 (248.4 examples/sec; 0.515 sec/batch)
2018-03-22 21:01:05.813399: step 68130, loss = 0.75 (253.9 examples/sec; 0.504 sec/batch)
2018-03-22 21:01:11.212394: step 68140, loss = 0.73 (237.1 examples/sec; 0.540 sec/batch)
2018-03-22 21:01:16.005421: step 68150, loss = 0.66 (267.1 examples/sec; 0.479 sec/batch)
2018-03-22 21:01:21.607504: step 68160, loss = 0.74 (228.5 examples/sec; 0.560 sec/batch)
2018-03-22 21:01:26.076103: step 68170, loss = 0.69 (286.4 examples/sec; 0.447 sec/batch)
2018-03-22 21:01:30.811865: step 68180, loss = 0.63 (270.3 examples/sec; 0.474 sec/batch)
2018-03-22 21:01:35.215045: step 68190, loss = 0.72 (290.7 examples/sec; 0.440 sec/batch)
2018-03-22 21:01:40.891638: step 68200, loss = 0.89 (225.5 examples/sec; 0.568 sec/batch)
2018-03-22 21:01:45.912349: step 68210, loss = 0.89 (254.9 examples/sec; 0.502 sec/batch)
2018-03-22 21:01:51.131283: step 68220, loss = 0.75 (245.3 examples/sec; 0.522 sec/batch)
2018-03-22 21:01:56.100427: step 68230, loss = 0.84 (257.6 examples/sec; 0.497 sec/batch)
2018-03-22 21:02:01.785392: step 68240, loss = 0.69 (225.2 examples/sec; 0.568 sec/batch)
2018-03-22 21:02:06.570455: step 68250, loss = 0.67 (267.5 examples/sec; 0.479 sec/batch)
2018-03-22 21:02:11.678420: step 68260, loss = 0.68 (250.6 examples/sec; 0.511 sec/batch)
2018-03-22 21:02:16.512480: step 68270, loss = 0.81 (264.8 examples/sec; 0.483 sec/batch)
2018-03-22 21:02:21.957026: step 68280, loss = 0.68 (235.1 examples/sec; 0.544 sec/batch)
2018-03-22 21:02:27.423357: step 68290, loss = 0.86 (234.2 examples/sec; 0.547 sec/batch)
2018-03-22 21:02:32.576187: step 68300, loss = 0.68 (248.4 examples/sec; 0.515 sec/batch)
2018-03-22 21:02:37.304012: step 68310, loss = 0.59 (270.7 examples/sec; 0.473 sec/batch)
2018-03-22 21:02:42.508307: step 68320, loss = 0.60 (246.0 examples/sec; 0.520 sec/batch)
2018-03-22 21:02:47.835390: step 68330, loss = 0.72 (240.3 examples/sec; 0.533 sec/batch)
2018-03-22 21:02:53.345662: step 68340, loss = 0.76 (232.3 examples/sec; 0.551 sec/batch)
2018-03-22 21:02:57.702134: step 68350, loss = 0.71 (293.8 examples/sec; 0.436 sec/batch)
2018-03-22 21:03:02.564549: step 68360, loss = 0.65 (263.2 examples/sec; 0.486 sec/batch)
2018-03-22 21:03:07.561348: step 68370, loss = 0.78 (256.2 examples/sec; 0.500 sec/batch)
2018-03-22 21:03:13.158554: step 68380, loss = 0.67 (228.7 examples/sec; 0.560 sec/batch)
2018-03-22 21:03:17.837973: step 68390, loss = 0.76 (273.5 examples/sec; 0.468 sec/batch)
2018-03-22 21:03:23.093114: step 68400, loss = 0.73 (243.6 examples/sec; 0.526 sec/batch)
2018-03-22 21:03:28.186358: step 68410, loss = 0.75 (251.3 examples/sec; 0.509 sec/batch)
2018-03-22 21:03:33.791470: step 68420, loss = 1.02 (228.4 examples/sec; 0.561 sec/batch)
2018-03-22 21:03:38.708444: step 68430, loss = 0.84 (260.3 examples/sec; 0.492 sec/batch)
2018-03-22 21:03:43.980460: step 68440, loss = 0.71 (242.8 examples/sec; 0.527 sec/batch)
2018-03-22 21:03:49.378432: step 68450, loss = 0.64 (237.1 examples/sec; 0.540 sec/batch)
2018-03-22 21:03:54.733424: step 68460, loss = 0.87 (239.0 examples/sec; 0.535 sec/batch)
2018-03-22 21:03:59.640403: step 68470, loss = 0.87 (260.9 examples/sec; 0.491 sec/batch)
2018-03-22 21:04:04.345437: step 68480, loss = 0.85 (272.0 examples/sec; 0.471 sec/batch)
2018-03-22 21:04:09.812393: step 68490, loss = 0.73 (234.1 examples/sec; 0.547 sec/batch)
2018-03-22 21:04:15.261670: step 68500, loss = 0.85 (234.9 examples/sec; 0.545 sec/batch)
2018-03-22 21:04:20.342756: step 68510, loss = 0.59 (251.9 examples/sec; 0.508 sec/batch)
2018-03-22 21:04:25.589286: step 68520, loss = 0.70 (244.0 examples/sec; 0.525 sec/batch)
2018-03-22 21:04:30.771442: step 68530, loss = 0.73 (247.0 examples/sec; 0.518 sec/batch)
2018-03-22 21:04:35.575879: step 68540, loss = 0.81 (266.4 examples/sec; 0.480 sec/batch)
2018-03-22 21:04:40.695444: step 68550, loss = 0.67 (250.0 examples/sec; 0.512 sec/batch)
2018-03-22 21:04:45.487395: step 68560, loss = 0.91 (267.1 examples/sec; 0.479 sec/batch)
2018-03-22 21:04:50.371185: step 68570, loss = 0.55 (262.1 examples/sec; 0.488 sec/batch)
2018-03-22 21:04:55.289458: step 68580, loss = 0.57 (260.3 examples/sec; 0.492 sec/batch)
2018-03-22 21:05:00.590426: step 68590, loss = 0.84 (241.5 examples/sec; 0.530 sec/batch)
2018-03-22 21:05:05.312526: step 68600, loss = 0.77 (271.1 examples/sec; 0.472 sec/batch)
2018-03-22 21:05:10.309464: step 68610, loss = 0.67 (256.2 examples/sec; 0.500 sec/batch)
2018-03-22 21:05:15.493464: step 68620, loss = 0.68 (246.9 examples/sec; 0.518 sec/batch)
2018-03-22 21:05:20.196358: step 68630, loss = 0.76 (272.2 examples/sec; 0.470 sec/batch)
2018-03-22 21:05:24.503529: step 68640, loss = 0.50 (297.2 examples/sec; 0.431 sec/batch)
2018-03-22 21:05:29.111341: step 68650, loss = 0.61 (277.8 examples/sec; 0.461 sec/batch)
2018-03-22 21:05:34.336828: step 68660, loss = 0.68 (245.0 examples/sec; 0.523 sec/batch)
2018-03-22 21:05:39.592491: step 68670, loss = 0.72 (243.5 examples/sec; 0.526 sec/batch)
2018-03-22 21:05:44.507683: step 68680, loss = 0.84 (260.4 examples/sec; 0.492 sec/batch)
2018-03-22 21:05:49.661455: step 68690, loss = 0.62 (248.4 examples/sec; 0.515 sec/batch)
2018-03-22 21:05:55.172049: step 68700, loss = 0.73 (232.3 examples/sec; 0.551 sec/batch)
2018-03-22 21:05:59.847847: step 68710, loss = 0.49 (273.7 examples/sec; 0.468 sec/batch)
2018-03-22 21:06:05.021410: step 68720, loss = 0.77 (247.4 examples/sec; 0.517 sec/batch)
2018-03-22 21:06:10.134385: step 68730, loss = 0.89 (250.3 examples/sec; 0.511 sec/batch)
2018-03-22 21:06:15.349322: step 68740, loss = 0.73 (245.4 examples/sec; 0.521 sec/batch)
2018-03-22 21:06:20.506235: step 68750, loss = 0.79 (248.2 examples/sec; 0.516 sec/batch)
2018-03-22 21:06:25.629391: step 68760, loss = 0.66 (249.8 examples/sec; 0.512 sec/batch)
2018-03-22 21:06:30.838374: step 68770, loss = 0.80 (245.7 examples/sec; 0.521 sec/batch)
2018-03-22 21:06:35.648373: step 68780, loss = 0.73 (266.1 examples/sec; 0.481 sec/batch)
2018-03-22 21:06:41.554395: step 68790, loss = 0.80 (216.7 examples/sec; 0.591 sec/batch)
2018-03-22 21:06:46.679404: step 68800, loss = 0.86 (249.8 examples/sec; 0.513 sec/batch)
2018-03-22 21:06:51.483201: step 68810, loss = 0.72 (266.5 examples/sec; 0.480 sec/batch)
2018-03-22 21:06:56.594736: step 68820, loss = 0.67 (250.4 examples/sec; 0.511 sec/batch)
2018-03-22 21:07:01.342395: step 68830, loss = 0.69 (269.6 examples/sec; 0.475 sec/batch)
2018-03-22 21:07:06.793372: step 68840, loss = 0.73 (234.8 examples/sec; 0.545 sec/batch)
2018-03-22 21:07:11.706518: step 68850, loss = 0.71 (260.5 examples/sec; 0.491 sec/batch)
2018-03-22 21:07:16.855406: step 68860, loss = 0.78 (248.6 examples/sec; 0.515 sec/batch)
2018-03-22 21:07:21.905454: step 68870, loss = 0.81 (253.5 examples/sec; 0.505 sec/batch)
2018-03-22 21:07:26.756400: step 68880, loss = 0.77 (263.9 examples/sec; 0.485 sec/batch)
2018-03-22 21:07:31.320462: step 68890, loss = 0.67 (280.5 examples/sec; 0.456 sec/batch)
2018-03-22 21:07:36.400858: step 68900, loss = 0.72 (251.9 examples/sec; 0.508 sec/batch)
2018-03-22 21:07:41.263426: step 68910, loss = 0.66 (263.2 examples/sec; 0.486 sec/batch)
2018-03-22 21:07:46.173871: step 68920, loss = 0.70 (260.7 examples/sec; 0.491 sec/batch)
2018-03-22 21:07:51.460341: step 68930, loss = 0.68 (242.1 examples/sec; 0.529 sec/batch)
2018-03-22 21:07:56.179376: step 68940, loss = 0.61 (271.2 examples/sec; 0.472 sec/batch)
2018-03-22 21:08:01.116933: step 68950, loss = 0.70 (259.2 examples/sec; 0.494 sec/batch)
2018-03-22 21:08:06.201394: step 68960, loss = 0.73 (251.7 examples/sec; 0.508 sec/batch)
2018-03-22 21:08:11.425490: step 68970, loss = 0.83 (245.0 examples/sec; 0.522 sec/batch)
2018-03-22 21:08:16.330374: step 68980, loss = 0.78 (261.0 examples/sec; 0.490 sec/batch)
2018-03-22 21:08:21.455392: step 68990, loss = 0.66 (249.8 examples/sec; 0.513 sec/batch)
2018-03-22 21:08:26.883265: step 69000, loss = 0.88 (235.8 examples/sec; 0.543 sec/batch)
2018-03-22 21:08:31.869186: step 69010, loss = 0.62 (256.7 examples/sec; 0.499 sec/batch)
2018-03-22 21:08:36.707371: step 69020, loss = 0.86 (264.6 examples/sec; 0.484 sec/batch)
2018-03-22 21:08:41.514398: step 69030, loss = 0.82 (266.3 examples/sec; 0.481 sec/batch)
2018-03-22 21:08:46.261847: step 69040, loss = 0.70 (269.6 examples/sec; 0.475 sec/batch)
2018-03-22 21:08:51.507900: step 69050, loss = 0.74 (244.0 examples/sec; 0.525 sec/batch)
2018-03-22 21:08:56.311415: step 69060, loss = 0.68 (266.5 examples/sec; 0.480 sec/batch)
2018-03-22 21:09:01.734025: step 69070, loss = 0.77 (236.0 examples/sec; 0.542 sec/batch)
2018-03-22 21:09:06.797736: step 69080, loss = 0.71 (252.8 examples/sec; 0.506 sec/batch)
2018-03-22 21:09:11.226223: step 69090, loss = 0.63 (289.0 examples/sec; 0.443 sec/batch)
2018-03-22 21:09:16.167020: step 69100, loss = 0.69 (259.1 examples/sec; 0.494 sec/batch)
2018-03-22 21:09:21.478433: step 69110, loss = 0.74 (241.0 examples/sec; 0.531 sec/batch)
2018-03-22 21:09:26.374538: step 69120, loss = 0.76 (261.4 examples/sec; 0.490 sec/batch)
2018-03-22 21:09:31.486264: step 69130, loss = 0.66 (250.4 examples/sec; 0.511 sec/batch)
2018-03-22 21:09:35.944761: step 69140, loss = 0.59 (287.1 examples/sec; 0.446 sec/batch)
2018-03-22 21:09:40.900365: step 69150, loss = 0.71 (258.3 examples/sec; 0.496 sec/batch)
2018-03-22 21:09:45.985734: step 69160, loss = 0.72 (251.7 examples/sec; 0.509 sec/batch)
2018-03-22 21:09:52.043307: step 69170, loss = 0.68 (211.3 examples/sec; 0.606 sec/batch)
2018-03-22 21:09:56.759351: step 69180, loss = 0.72 (271.4 examples/sec; 0.472 sec/batch)
2018-03-22 21:10:01.931992: step 69190, loss = 0.72 (247.5 examples/sec; 0.517 sec/batch)
2018-03-22 21:10:07.043473: step 69200, loss = 0.69 (250.4 examples/sec; 0.511 sec/batch)
2018-03-22 21:10:11.864497: step 69210, loss = 0.70 (265.5 examples/sec; 0.482 sec/batch)
2018-03-22 21:10:17.108484: step 69220, loss = 0.71 (244.1 examples/sec; 0.524 sec/batch)
2018-03-22 21:10:22.168398: step 69230, loss = 0.78 (253.0 examples/sec; 0.506 sec/batch)
2018-03-22 21:10:27.381361: step 69240, loss = 0.62 (245.5 examples/sec; 0.521 sec/batch)
2018-03-22 21:10:32.794383: step 69250, loss = 0.65 (236.5 examples/sec; 0.541 sec/batch)
2018-03-22 21:10:38.055361: step 69260, loss = 0.65 (243.3 examples/sec; 0.526 sec/batch)
2018-03-22 21:10:43.138973: step 69270, loss = 0.65 (251.8 examples/sec; 0.508 sec/batch)
2018-03-22 21:10:47.838548: step 69280, loss = 0.62 (272.4 examples/sec; 0.470 sec/batch)
2018-03-22 21:10:52.985232: step 69290, loss = 0.80 (248.7 examples/sec; 0.515 sec/batch)
2018-03-22 21:10:58.097536: step 69300, loss = 0.66 (250.4 examples/sec; 0.511 sec/batch)
2018-03-22 21:11:03.357902: step 69310, loss = 0.80 (243.3 examples/sec; 0.526 sec/batch)
2018-03-22 21:11:08.242323: step 69320, loss = 0.83 (262.1 examples/sec; 0.488 sec/batch)
2018-03-22 21:11:13.031369: step 69330, loss = 0.92 (267.3 examples/sec; 0.479 sec/batch)
2018-03-22 21:11:18.275619: step 69340, loss = 0.84 (244.1 examples/sec; 0.524 sec/batch)
2018-03-22 21:11:23.285648: step 69350, loss = 0.73 (255.5 examples/sec; 0.501 sec/batch)
2018-03-22 21:11:27.587488: step 69360, loss = 0.69 (297.5 examples/sec; 0.430 sec/batch)
2018-03-22 21:11:32.726404: step 69370, loss = 0.88 (249.1 examples/sec; 0.514 sec/batch)
2018-03-22 21:11:37.861292: step 69380, loss = 0.62 (249.3 examples/sec; 0.513 sec/batch)
2018-03-22 21:11:43.365465: step 69390, loss = 0.71 (232.6 examples/sec; 0.550 sec/batch)
2018-03-22 21:11:48.442989: step 69400, loss = 0.80 (252.1 examples/sec; 0.508 sec/batch)
2018-03-22 21:11:53.480294: step 69410, loss = 0.57 (254.1 examples/sec; 0.504 sec/batch)
2018-03-22 21:11:58.826344: step 69420, loss = 0.68 (239.4 examples/sec; 0.535 sec/batch)
2018-03-22 21:12:04.274436: step 69430, loss = 0.61 (234.9 examples/sec; 0.545 sec/batch)
2018-03-22 21:12:09.115437: step 69440, loss = 0.82 (264.4 examples/sec; 0.484 sec/batch)
2018-03-22 21:12:13.826397: step 69450, loss = 0.78 (271.7 examples/sec; 0.471 sec/batch)
2018-03-22 21:12:19.084386: step 69460, loss = 0.64 (243.4 examples/sec; 0.526 sec/batch)
2018-03-22 21:12:23.833416: step 69470, loss = 0.77 (269.5 examples/sec; 0.475 sec/batch)
2018-03-22 21:12:28.857324: step 69480, loss = 0.68 (254.8 examples/sec; 0.502 sec/batch)
2018-03-22 21:12:34.641386: step 69490, loss = 0.67 (221.3 examples/sec; 0.578 sec/batch)
2018-03-22 21:12:39.904960: step 69500, loss = 0.78 (243.2 examples/sec; 0.526 sec/batch)
2018-03-22 21:12:44.358916: step 69510, loss = 0.76 (287.4 examples/sec; 0.445 sec/batch)
2018-03-22 21:12:49.266816: step 69520, loss = 0.81 (260.8 examples/sec; 0.491 sec/batch)
2018-03-22 21:12:54.082255: step 69530, loss = 0.80 (265.8 examples/sec; 0.482 sec/batch)
2018-03-22 21:12:58.209114: step 69540, loss = 0.65 (310.2 examples/sec; 0.413 sec/batch)
2018-03-22 21:13:03.043411: step 69550, loss = 0.71 (264.8 examples/sec; 0.483 sec/batch)
2018-03-22 21:13:08.363393: step 69560, loss = 0.80 (240.6 examples/sec; 0.532 sec/batch)
2018-03-22 21:13:13.760391: step 69570, loss = 0.83 (237.2 examples/sec; 0.540 sec/batch)
2018-03-22 21:13:19.118350: step 69580, loss = 0.78 (238.9 examples/sec; 0.536 sec/batch)
2018-03-22 21:13:23.999110: step 69590, loss = 0.75 (262.3 examples/sec; 0.488 sec/batch)
2018-03-22 21:13:28.768998: step 69600, loss = 0.77 (268.4 examples/sec; 0.477 sec/batch)
2018-03-22 21:13:33.556483: step 69610, loss = 0.72 (267.4 examples/sec; 0.479 sec/batch)
2018-03-22 21:13:38.564912: step 69620, loss = 0.66 (255.6 examples/sec; 0.501 sec/batch)
2018-03-22 21:13:44.162398: step 69630, loss = 0.68 (228.7 examples/sec; 0.560 sec/batch)
2018-03-22 21:13:49.822133: step 69640, loss = 0.68 (226.2 examples/sec; 0.566 sec/batch)
2018-03-22 21:13:54.996336: step 69650, loss = 0.78 (247.4 examples/sec; 0.517 sec/batch)
2018-03-22 21:14:00.523571: step 69660, loss = 0.65 (231.6 examples/sec; 0.553 sec/batch)
2018-03-22 21:14:05.890457: step 69670, loss = 0.70 (238.5 examples/sec; 0.537 sec/batch)
2018-03-22 21:14:11.365379: step 69680, loss = 0.64 (233.8 examples/sec; 0.547 sec/batch)
2018-03-22 21:14:15.772617: step 69690, loss = 0.66 (290.4 examples/sec; 0.441 sec/batch)
2018-03-22 21:14:21.483197: step 69700, loss = 0.57 (224.2 examples/sec; 0.571 sec/batch)
2018-03-22 21:14:26.324247: step 69710, loss = 0.69 (264.4 examples/sec; 0.484 sec/batch)
2018-03-22 21:14:31.058890: step 69720, loss = 0.67 (270.3 examples/sec; 0.473 sec/batch)
2018-03-22 21:14:35.292338: step 69730, loss = 0.73 (302.4 examples/sec; 0.423 sec/batch)
2018-03-22 21:14:40.679422: step 69740, loss = 0.68 (237.6 examples/sec; 0.539 sec/batch)
2018-03-22 21:14:45.426641: step 69750, loss = 0.88 (269.6 examples/sec; 0.475 sec/batch)
2018-03-22 21:14:50.500880: step 69760, loss = 0.59 (252.3 examples/sec; 0.507 sec/batch)
2018-03-22 21:14:55.725922: step 69770, loss = 0.86 (245.0 examples/sec; 0.523 sec/batch)
2018-03-22 21:15:00.879391: step 69780, loss = 0.69 (248.4 examples/sec; 0.515 sec/batch)
2018-03-22 21:15:05.735516: step 69790, loss = 0.85 (263.6 examples/sec; 0.486 sec/batch)
2018-03-22 21:15:11.397532: step 69800, loss = 0.73 (226.1 examples/sec; 0.566 sec/batch)
2018-03-22 21:15:16.371415: step 69810, loss = 0.77 (257.3 examples/sec; 0.497 sec/batch)
2018-03-22 21:15:21.590361: step 69820, loss = 0.89 (245.3 examples/sec; 0.522 sec/batch)
2018-03-22 21:15:26.506482: step 69830, loss = 0.78 (260.4 examples/sec; 0.492 sec/batch)
2018-03-22 21:15:31.683408: step 69840, loss = 0.70 (247.3 examples/sec; 0.518 sec/batch)
2018-03-22 21:15:37.365187: step 69850, loss = 0.74 (225.3 examples/sec; 0.568 sec/batch)
2018-03-22 21:15:42.282519: step 69860, loss = 0.75 (260.3 examples/sec; 0.492 sec/batch)
2018-03-22 21:15:47.444394: step 69870, loss = 0.76 (248.0 examples/sec; 0.516 sec/batch)
2018-03-22 21:15:53.185438: step 69880, loss = 0.66 (223.0 examples/sec; 0.574 sec/batch)
2018-03-22 21:15:57.830696: step 69890, loss = 0.81 (275.5 examples/sec; 0.465 sec/batch)
2018-03-22 21:16:03.509023: step 69900, loss = 0.77 (225.4 examples/sec; 0.568 sec/batch)
2018-03-22 21:16:08.507448: step 69910, loss = 0.77 (256.1 examples/sec; 0.500 sec/batch)
2018-03-22 21:16:14.161410: step 69920, loss = 0.77 (226.4 examples/sec; 0.565 sec/batch)
2018-03-22 21:16:18.982526: step 69930, loss = 0.64 (265.5 examples/sec; 0.482 sec/batch)
2018-03-22 21:16:23.931983: step 69940, loss = 0.68 (258.6 examples/sec; 0.495 sec/batch)
2018-03-22 21:16:29.058252: step 69950, loss = 0.68 (249.7 examples/sec; 0.513 sec/batch)
2018-03-22 21:16:34.274394: step 69960, loss = 0.77 (245.4 examples/sec; 0.522 sec/batch)
2018-03-22 21:16:38.829911: step 69970, loss = 0.80 (281.0 examples/sec; 0.456 sec/batch)
2018-03-22 21:16:43.203185: step 69980, loss = 0.77 (292.7 examples/sec; 0.437 sec/batch)
2018-03-22 21:16:47.591301: step 69990, loss = 0.82 (291.7 examples/sec; 0.439 sec/batch)
2018-03-22 21:16:52.591263: step 70000, loss = 0.73 (256.0 examples/sec; 0.500 sec/batch)
2018-03-22 21:16:58.050389: step 70010, loss = 0.70 (234.5 examples/sec; 0.546 sec/batch)
2018-03-22 21:17:02.851350: step 70020, loss = 0.67 (266.6 examples/sec; 0.480 sec/batch)
2018-03-22 21:17:08.359395: step 70030, loss = 0.74 (232.4 examples/sec; 0.551 sec/batch)
2018-03-22 21:17:13.787362: step 70040, loss = 0.70 (235.8 examples/sec; 0.543 sec/batch)
2018-03-22 21:17:18.518657: step 70050, loss = 0.59 (270.5 examples/sec; 0.473 sec/batch)
2018-03-22 21:17:23.341422: step 70060, loss = 0.69 (265.4 examples/sec; 0.482 sec/batch)
2018-03-22 21:17:28.393414: step 70070, loss = 0.69 (253.4 examples/sec; 0.505 sec/batch)
2018-03-22 21:17:33.050356: step 70080, loss = 0.75 (274.9 examples/sec; 0.466 sec/batch)
2018-03-22 21:17:38.177765: step 70090, loss = 0.75 (249.6 examples/sec; 0.513 sec/batch)
2018-03-22 21:17:43.615583: step 70100, loss = 0.62 (235.4 examples/sec; 0.544 sec/batch)
2018-03-22 21:17:48.171673: step 70110, loss = 0.74 (280.9 examples/sec; 0.456 sec/batch)
2018-03-22 21:17:53.267375: step 70120, loss = 0.69 (251.2 examples/sec; 0.510 sec/batch)
2018-03-22 21:17:57.853432: step 70130, loss = 0.63 (279.1 examples/sec; 0.459 sec/batch)
2018-03-22 21:18:03.342442: step 70140, loss = 0.80 (233.2 examples/sec; 0.549 sec/batch)
2018-03-22 21:18:08.240602: step 70150, loss = 0.74 (261.3 examples/sec; 0.490 sec/batch)
2018-03-22 21:18:12.869035: step 70160, loss = 0.60 (276.6 examples/sec; 0.463 sec/batch)
2018-03-22 21:18:17.990405: step 70170, loss = 0.73 (249.9 examples/sec; 0.512 sec/batch)
2018-03-22 21:18:22.711344: step 70180, loss = 0.70 (271.1 examples/sec; 0.472 sec/batch)
2018-03-22 21:18:27.431656: step 70190, loss = 0.87 (271.2 examples/sec; 0.472 sec/batch)
2018-03-22 21:18:32.244741: step 70200, loss = 0.90 (265.9 examples/sec; 0.481 sec/batch)
2018-03-22 21:18:36.956432: step 70210, loss = 0.69 (271.7 examples/sec; 0.471 sec/batch)
2018-03-22 21:18:41.900429: step 70220, loss = 0.77 (258.9 examples/sec; 0.494 sec/batch)
2018-03-22 21:18:46.846396: step 70230, loss = 0.61 (258.8 examples/sec; 0.495 sec/batch)
2018-03-22 21:18:51.830057: step 70240, loss = 0.74 (256.8 examples/sec; 0.498 sec/batch)
2018-03-22 21:18:56.688086: step 70250, loss = 0.75 (263.5 examples/sec; 0.486 sec/batch)
2018-03-22 21:19:02.222071: step 70260, loss = 0.75 (231.3 examples/sec; 0.553 sec/batch)
2018-03-22 21:19:07.651673: step 70270, loss = 0.81 (235.7 examples/sec; 0.543 sec/batch)
2018-03-22 21:19:12.894491: step 70280, loss = 0.67 (244.1 examples/sec; 0.524 sec/batch)
2018-03-22 21:19:18.205635: step 70290, loss = 0.83 (241.0 examples/sec; 0.531 sec/batch)
2018-03-22 21:19:23.644388: step 70300, loss = 0.85 (235.3 examples/sec; 0.544 sec/batch)
2018-03-22 21:19:28.458150: step 70310, loss = 0.82 (265.9 examples/sec; 0.481 sec/batch)
2018-03-22 21:19:33.369371: step 70320, loss = 0.62 (260.6 examples/sec; 0.491 sec/batch)
2018-03-22 21:19:38.266408: step 70330, loss = 0.74 (261.4 examples/sec; 0.490 sec/batch)
2018-03-22 21:19:43.882211: step 70340, loss = 0.58 (227.9 examples/sec; 0.562 sec/batch)
2018-03-22 21:19:48.582948: step 70350, loss = 0.84 (272.3 examples/sec; 0.470 sec/batch)
2018-03-22 21:19:53.369906: step 70360, loss = 0.87 (267.4 examples/sec; 0.479 sec/batch)
2018-03-22 21:19:58.502775: step 70370, loss = 0.63 (249.4 examples/sec; 0.513 sec/batch)
2018-03-22 21:20:03.998067: step 70380, loss = 0.63 (232.9 examples/sec; 0.550 sec/batch)
2018-03-22 21:20:08.899887: step 70390, loss = 0.78 (261.1 examples/sec; 0.490 sec/batch)
2018-03-22 21:20:14.179313: step 70400, loss = 0.79 (242.5 examples/sec; 0.528 sec/batch)
2018-03-22 21:20:18.925350: step 70410, loss = 0.78 (269.7 examples/sec; 0.475 sec/batch)
2018-03-22 21:20:24.319379: step 70420, loss = 0.84 (237.3 examples/sec; 0.539 sec/batch)
2018-03-22 21:20:29.047458: step 70430, loss = 0.73 (270.7 examples/sec; 0.473 sec/batch)
2018-03-22 21:20:33.838412: step 70440, loss = 0.76 (267.2 examples/sec; 0.479 sec/batch)
2018-03-22 21:20:38.566360: step 70450, loss = 0.82 (270.7 examples/sec; 0.473 sec/batch)
2018-03-22 21:20:43.550617: step 70460, loss = 0.72 (256.8 examples/sec; 0.498 sec/batch)
2018-03-22 21:20:47.995995: step 70470, loss = 0.60 (287.9 examples/sec; 0.445 sec/batch)
2018-03-22 21:20:53.352708: step 70480, loss = 0.68 (239.0 examples/sec; 0.536 sec/batch)
2018-03-22 21:20:58.637357: step 70490, loss = 0.69 (242.2 examples/sec; 0.528 sec/batch)
2018-03-22 21:21:04.601867: step 70500, loss = 0.72 (214.6 examples/sec; 0.596 sec/batch)
2018-03-22 21:21:09.302978: step 70510, loss = 0.74 (272.3 examples/sec; 0.470 sec/batch)
2018-03-22 21:21:13.917430: step 70520, loss = 0.61 (277.4 examples/sec; 0.461 sec/batch)
2018-03-22 21:21:19.247333: step 70530, loss = 0.66 (240.2 examples/sec; 0.533 sec/batch)
2018-03-22 21:21:23.955682: step 70540, loss = 0.72 (271.9 examples/sec; 0.471 sec/batch)
2018-03-22 21:21:28.915369: step 70550, loss = 0.65 (258.1 examples/sec; 0.496 sec/batch)
2018-03-22 21:21:33.637601: step 70560, loss = 0.86 (271.1 examples/sec; 0.472 sec/batch)
2018-03-22 21:21:38.502824: step 70570, loss = 0.71 (263.1 examples/sec; 0.487 sec/batch)
2018-03-22 21:21:43.912364: step 70580, loss = 0.69 (236.6 examples/sec; 0.541 sec/batch)
2018-03-22 21:21:48.965421: step 70590, loss = 0.82 (253.3 examples/sec; 0.505 sec/batch)
2018-03-22 21:21:54.385898: step 70600, loss = 0.77 (236.1 examples/sec; 0.542 sec/batch)
2018-03-22 21:21:59.539059: step 70610, loss = 0.88 (248.4 examples/sec; 0.515 sec/batch)
2018-03-22 21:22:04.340404: step 70620, loss = 0.59 (266.6 examples/sec; 0.480 sec/batch)
2018-03-22 21:22:08.896468: step 70630, loss = 0.85 (280.9 examples/sec; 0.456 sec/batch)
2018-03-22 21:22:13.377654: step 70640, loss = 0.71 (285.6 examples/sec; 0.448 sec/batch)
2018-03-22 21:22:18.681440: step 70650, loss = 0.54 (241.3 examples/sec; 0.530 sec/batch)
2018-03-22 21:22:24.158490: step 70660, loss = 0.66 (233.7 examples/sec; 0.548 sec/batch)
2018-03-22 21:22:28.563629: step 70670, loss = 0.76 (290.6 examples/sec; 0.441 sec/batch)
2018-03-22 21:22:33.267329: step 70680, loss = 0.78 (272.1 examples/sec; 0.470 sec/batch)
2018-03-22 21:22:38.777878: step 70690, loss = 0.65 (232.3 examples/sec; 0.551 sec/batch)
2018-03-22 21:22:44.023694: step 70700, loss = 0.73 (244.0 examples/sec; 0.525 sec/batch)
2018-03-22 21:22:48.524492: step 70710, loss = 0.71 (284.4 examples/sec; 0.450 sec/batch)
2018-03-22 21:22:53.825181: step 70720, loss = 0.70 (241.5 examples/sec; 0.530 sec/batch)
2018-03-22 21:22:59.136327: step 70730, loss = 0.68 (241.0 examples/sec; 0.531 sec/batch)
2018-03-22 21:23:03.860437: step 70740, loss = 0.70 (271.0 examples/sec; 0.472 sec/batch)
2018-03-22 21:23:08.656370: step 70750, loss = 0.71 (266.9 examples/sec; 0.480 sec/batch)
2018-03-22 21:23:13.557410: step 70760, loss = 0.72 (261.2 examples/sec; 0.490 sec/batch)
2018-03-22 21:23:18.500295: step 70770, loss = 0.64 (259.0 examples/sec; 0.494 sec/batch)
2018-03-22 21:23:23.554379: step 70780, loss = 0.71 (253.3 examples/sec; 0.505 sec/batch)
2018-03-22 21:23:28.699380: step 70790, loss = 0.89 (248.8 examples/sec; 0.514 sec/batch)
2018-03-22 21:23:33.647052: step 70800, loss = 0.50 (258.7 examples/sec; 0.495 sec/batch)
2018-03-22 21:23:38.179312: step 70810, loss = 0.74 (282.4 examples/sec; 0.453 sec/batch)
2018-03-22 21:23:43.914845: step 70820, loss = 0.69 (223.2 examples/sec; 0.574 sec/batch)
2018-03-22 21:23:48.942436: step 70830, loss = 0.62 (254.6 examples/sec; 0.503 sec/batch)
2018-03-22 21:23:53.678439: step 70840, loss = 0.80 (270.3 examples/sec; 0.474 sec/batch)
2018-03-22 21:23:58.922612: step 70850, loss = 0.79 (244.1 examples/sec; 0.524 sec/batch)
2018-03-22 21:24:04.273709: step 70860, loss = 0.64 (239.2 examples/sec; 0.535 sec/batch)
2018-03-22 21:24:09.503465: step 70870, loss = 0.76 (244.8 examples/sec; 0.523 sec/batch)
2018-03-22 21:24:14.204499: step 70880, loss = 0.75 (272.3 examples/sec; 0.470 sec/batch)
2018-03-22 21:24:19.257758: step 70890, loss = 0.68 (253.3 examples/sec; 0.505 sec/batch)
2018-03-22 21:24:24.178207: step 70900, loss = 0.85 (260.1 examples/sec; 0.492 sec/batch)
2018-03-22 21:24:29.393341: step 70910, loss = 0.86 (245.4 examples/sec; 0.522 sec/batch)
2018-03-22 21:24:34.339519: step 70920, loss = 0.70 (258.8 examples/sec; 0.495 sec/batch)
2018-03-22 21:24:39.432223: step 70930, loss = 0.71 (251.3 examples/sec; 0.509 sec/batch)
2018-03-22 21:24:44.887382: step 70940, loss = 0.67 (234.6 examples/sec; 0.546 sec/batch)
2018-03-22 21:24:50.127412: step 70950, loss = 0.92 (244.3 examples/sec; 0.524 sec/batch)
2018-03-22 21:24:54.463989: step 70960, loss = 0.74 (295.2 examples/sec; 0.434 sec/batch)
2018-03-22 21:24:59.948282: step 70970, loss = 0.71 (233.4 examples/sec; 0.548 sec/batch)
2018-03-22 21:25:05.355405: step 70980, loss = 0.76 (236.7 examples/sec; 0.541 sec/batch)
2018-03-22 21:25:10.632379: step 70990, loss = 0.78 (242.6 examples/sec; 0.528 sec/batch)
2018-03-22 21:25:16.262566: step 71000, loss = 0.80 (227.3 examples/sec; 0.563 sec/batch)
2018-03-22 21:25:20.975496: step 71010, loss = 0.78 (271.6 examples/sec; 0.471 sec/batch)
2018-03-22 21:25:25.679626: step 71020, loss = 0.72 (272.1 examples/sec; 0.470 sec/batch)
2018-03-22 21:25:30.659453: step 71030, loss = 0.61 (257.0 examples/sec; 0.498 sec/batch)
2018-03-22 21:25:36.092368: step 71040, loss = 0.72 (235.6 examples/sec; 0.543 sec/batch)
2018-03-22 21:25:41.392409: step 71050, loss = 0.73 (241.5 examples/sec; 0.530 sec/batch)
2018-03-22 21:25:46.137346: step 71060, loss = 0.82 (269.8 examples/sec; 0.474 sec/batch)
2018-03-22 21:25:51.441623: step 71070, loss = 0.94 (241.3 examples/sec; 0.530 sec/batch)
2018-03-22 21:25:56.169419: step 71080, loss = 0.67 (270.7 examples/sec; 0.473 sec/batch)
2018-03-22 21:26:00.859284: step 71090, loss = 0.67 (272.9 examples/sec; 0.469 sec/batch)
2018-03-22 21:26:05.893188: step 71100, loss = 0.62 (254.3 examples/sec; 0.503 sec/batch)
2018-03-22 21:26:10.845315: step 71110, loss = 0.86 (258.5 examples/sec; 0.495 sec/batch)
2018-03-22 21:26:15.544792: step 71120, loss = 0.69 (272.4 examples/sec; 0.470 sec/batch)
2018-03-22 21:26:20.766354: step 71130, loss = 0.85 (245.1 examples/sec; 0.522 sec/batch)
2018-03-22 21:26:25.526345: step 71140, loss = 0.68 (268.9 examples/sec; 0.476 sec/batch)
2018-03-22 21:26:30.644417: step 71150, loss = 0.77 (250.1 examples/sec; 0.512 sec/batch)
2018-03-22 21:26:35.105419: step 71160, loss = 0.74 (286.9 examples/sec; 0.446 sec/batch)
2018-03-22 21:26:40.432902: step 71170, loss = 0.89 (240.3 examples/sec; 0.533 sec/batch)
2018-03-22 21:26:45.551445: step 71180, loss = 0.79 (250.1 examples/sec; 0.512 sec/batch)
2018-03-22 21:26:50.952661: step 71190, loss = 0.74 (237.0 examples/sec; 0.540 sec/batch)
2018-03-22 21:26:55.973955: step 71200, loss = 0.65 (254.9 examples/sec; 0.502 sec/batch)
2018-03-22 21:27:01.191369: step 71210, loss = 0.72 (245.3 examples/sec; 0.522 sec/batch)
2018-03-22 21:27:06.179470: step 71220, loss = 0.68 (256.6 examples/sec; 0.499 sec/batch)
2018-03-22 21:27:11.858376: step 71230, loss = 0.57 (225.4 examples/sec; 0.568 sec/batch)
2018-03-22 21:27:16.462399: step 71240, loss = 0.65 (278.0 examples/sec; 0.460 sec/batch)
2018-03-22 21:27:21.590370: step 71250, loss = 0.61 (249.6 examples/sec; 0.513 sec/batch)
2018-03-22 21:27:26.465393: step 71260, loss = 0.73 (262.6 examples/sec; 0.488 sec/batch)
2018-03-22 21:27:31.750389: step 71270, loss = 0.79 (242.2 examples/sec; 0.528 sec/batch)
2018-03-22 21:27:36.162348: step 71280, loss = 0.60 (290.1 examples/sec; 0.441 sec/batch)
2018-03-22 21:27:41.342398: step 71290, loss = 0.60 (247.1 examples/sec; 0.518 sec/batch)
2018-03-22 21:27:46.255685: step 71300, loss = 0.61 (260.5 examples/sec; 0.491 sec/batch)
2018-03-22 21:27:51.410557: step 71310, loss = 0.78 (248.3 examples/sec; 0.515 sec/batch)
2018-03-22 21:27:55.801288: step 71320, loss = 0.64 (291.5 examples/sec; 0.439 sec/batch)
2018-03-22 21:28:00.676406: step 71330, loss = 0.75 (262.6 examples/sec; 0.488 sec/batch)
2018-03-22 21:28:05.506395: step 71340, loss = 0.68 (265.0 examples/sec; 0.483 sec/batch)
2018-03-22 21:28:10.582404: step 71350, loss = 0.73 (252.2 examples/sec; 0.508 sec/batch)
2018-03-22 21:28:15.004611: step 71360, loss = 0.68 (289.4 examples/sec; 0.442 sec/batch)
2018-03-22 21:28:19.536394: step 71370, loss = 0.67 (282.4 examples/sec; 0.453 sec/batch)
2018-03-22 21:28:24.229152: step 71380, loss = 0.64 (272.8 examples/sec; 0.469 sec/batch)
2018-03-22 21:28:29.705880: step 71390, loss = 0.89 (233.7 examples/sec; 0.548 sec/batch)
2018-03-22 21:28:35.369346: step 71400, loss = 0.64 (226.0 examples/sec; 0.566 sec/batch)
2018-03-22 21:28:40.593390: step 71410, loss = 0.58 (245.0 examples/sec; 0.522 sec/batch)
2018-03-22 21:28:45.242699: step 71420, loss = 0.59 (275.3 examples/sec; 0.465 sec/batch)
2018-03-22 21:28:50.375007: step 71430, loss = 0.68 (249.4 examples/sec; 0.513 sec/batch)
2018-03-22 21:28:55.282397: step 71440, loss = 0.79 (260.8 examples/sec; 0.491 sec/batch)
2018-03-22 21:29:00.758176: step 71450, loss = 0.85 (233.8 examples/sec; 0.548 sec/batch)
2018-03-22 21:29:06.241191: step 71460, loss = 0.71 (233.4 examples/sec; 0.548 sec/batch)
2018-03-22 21:29:11.292638: step 71470, loss = 0.75 (253.4 examples/sec; 0.505 sec/batch)
2018-03-22 21:29:15.865689: step 71480, loss = 0.73 (279.9 examples/sec; 0.457 sec/batch)
2018-03-22 21:29:21.137344: step 71490, loss = 0.78 (242.8 examples/sec; 0.527 sec/batch)
2018-03-22 21:29:26.692157: step 71500, loss = 0.79 (230.4 examples/sec; 0.555 sec/batch)
2018-03-22 21:29:31.976834: step 71510, loss = 0.66 (242.2 examples/sec; 0.528 sec/batch)
2018-03-22 21:29:36.581262: step 71520, loss = 0.75 (278.0 examples/sec; 0.460 sec/batch)
2018-03-22 21:29:41.766389: step 71530, loss = 0.77 (246.9 examples/sec; 0.519 sec/batch)
2018-03-22 21:29:46.507333: step 71540, loss = 0.69 (270.0 examples/sec; 0.474 sec/batch)
2018-03-22 21:29:51.093186: step 71550, loss = 0.59 (279.1 examples/sec; 0.459 sec/batch)
2018-03-22 21:29:55.700409: step 71560, loss = 0.76 (277.8 examples/sec; 0.461 sec/batch)
2018-03-22 21:30:00.715844: step 71570, loss = 0.84 (255.2 examples/sec; 0.502 sec/batch)
2018-03-22 21:30:05.698779: step 71580, loss = 0.87 (256.9 examples/sec; 0.498 sec/batch)
2018-03-22 21:30:10.995279: step 71590, loss = 0.76 (241.7 examples/sec; 0.530 sec/batch)
2018-03-22 21:30:16.655395: step 71600, loss = 0.86 (226.1 examples/sec; 0.566 sec/batch)
2018-03-22 21:30:21.397414: step 71610, loss = 0.75 (269.9 examples/sec; 0.474 sec/batch)
2018-03-22 21:30:26.693349: step 71620, loss = 0.82 (241.7 examples/sec; 0.530 sec/batch)
2018-03-22 21:30:31.912549: step 71630, loss = 0.71 (245.2 examples/sec; 0.522 sec/batch)
2018-03-22 21:30:37.242347: step 71640, loss = 0.86 (240.2 examples/sec; 0.533 sec/batch)
2018-03-22 21:30:41.934956: step 71650, loss = 0.69 (272.8 examples/sec; 0.469 sec/batch)
2018-03-22 21:30:46.717841: step 71660, loss = 0.70 (267.6 examples/sec; 0.478 sec/batch)
2018-03-22 21:30:51.834421: step 71670, loss = 0.75 (250.2 examples/sec; 0.512 sec/batch)
2018-03-22 21:30:56.927383: step 71680, loss = 0.73 (251.3 examples/sec; 0.509 sec/batch)
2018-03-22 21:31:01.869808: step 71690, loss = 0.72 (259.0 examples/sec; 0.494 sec/batch)
2018-03-22 21:31:06.744349: step 71700, loss = 0.68 (262.6 examples/sec; 0.487 sec/batch)
2018-03-22 21:31:12.014393: step 71710, loss = 0.62 (242.9 examples/sec; 0.527 sec/batch)
2018-03-22 21:31:17.198395: step 71720, loss = 0.75 (246.9 examples/sec; 0.518 sec/batch)
2018-03-22 21:31:22.225425: step 71730, loss = 0.88 (254.6 examples/sec; 0.503 sec/batch)
2018-03-22 21:31:27.215357: step 71740, loss = 0.79 (256.5 examples/sec; 0.499 sec/batch)
2018-03-22 21:31:32.462369: step 71750, loss = 0.72 (243.9 examples/sec; 0.525 sec/batch)
2018-03-22 21:31:37.394925: step 71760, loss = 0.68 (259.5 examples/sec; 0.493 sec/batch)
2018-03-22 21:31:42.551326: step 71770, loss = 0.80 (248.2 examples/sec; 0.516 sec/batch)
2018-03-22 21:31:47.629559: step 71780, loss = 0.82 (252.1 examples/sec; 0.508 sec/batch)
2018-03-22 21:31:52.222344: step 71790, loss = 0.62 (278.7 examples/sec; 0.459 sec/batch)
2018-03-22 21:31:56.641271: step 71800, loss = 0.91 (289.7 examples/sec; 0.442 sec/batch)
2018-03-22 21:32:01.893820: step 71810, loss = 0.72 (243.7 examples/sec; 0.525 sec/batch)
2018-03-22 21:32:06.843372: step 71820, loss = 0.74 (258.6 examples/sec; 0.495 sec/batch)
2018-03-22 21:32:11.784430: step 71830, loss = 0.72 (259.1 examples/sec; 0.494 sec/batch)
2018-03-22 21:32:16.343360: step 71840, loss = 0.77 (280.8 examples/sec; 0.456 sec/batch)
2018-03-22 21:32:21.686376: step 71850, loss = 0.88 (239.6 examples/sec; 0.534 sec/batch)
2018-03-22 21:32:26.758348: step 71860, loss = 0.62 (252.4 examples/sec; 0.507 sec/batch)
2018-03-22 21:32:31.616234: step 71870, loss = 0.65 (263.5 examples/sec; 0.486 sec/batch)
2018-03-22 21:32:35.893295: step 71880, loss = 0.77 (299.3 examples/sec; 0.428 sec/batch)
2018-03-22 21:32:41.205348: step 71890, loss = 0.71 (241.0 examples/sec; 0.531 sec/batch)
2018-03-22 21:32:46.562600: step 71900, loss = 0.70 (238.9 examples/sec; 0.536 sec/batch)
2018-03-22 21:32:51.812038: step 71910, loss = 0.64 (243.8 examples/sec; 0.525 sec/batch)
2018-03-22 21:32:56.363750: step 71920, loss = 0.88 (281.2 examples/sec; 0.455 sec/batch)
2018-03-22 21:33:01.885353: step 71930, loss = 0.76 (231.8 examples/sec; 0.552 sec/batch)
2018-03-22 21:33:06.989350: step 71940, loss = 0.71 (250.8 examples/sec; 0.510 sec/batch)
2018-03-22 21:33:11.927376: step 71950, loss = 0.70 (259.2 examples/sec; 0.494 sec/batch)
2018-03-22 21:33:17.064755: step 71960, loss = 0.77 (249.2 examples/sec; 0.514 sec/batch)
2018-03-22 21:33:22.224645: step 71970, loss = 0.62 (248.1 examples/sec; 0.516 sec/batch)
2018-03-22 21:33:27.160269: step 71980, loss = 0.68 (259.3 examples/sec; 0.494 sec/batch)
2018-03-22 21:33:31.679329: step 71990, loss = 0.90 (283.2 examples/sec; 0.452 sec/batch)
2018-03-22 21:33:36.070326: step 72000, loss = 0.67 (291.5 examples/sec; 0.439 sec/batch)
2018-03-22 21:33:41.177365: step 72010, loss = 0.79 (250.6 examples/sec; 0.511 sec/batch)
2018-03-22 21:33:46.014377: step 72020, loss = 0.79 (264.6 examples/sec; 0.484 sec/batch)
2018-03-22 21:33:51.340351: step 72030, loss = 0.82 (240.3 examples/sec; 0.533 sec/batch)
2018-03-22 21:33:56.155420: step 72040, loss = 0.77 (265.8 examples/sec; 0.482 sec/batch)
2018-03-22 21:34:01.369353: step 72050, loss = 0.64 (245.5 examples/sec; 0.521 sec/batch)
2018-03-22 21:34:06.303361: step 72060, loss = 0.72 (259.4 examples/sec; 0.493 sec/batch)
2018-03-22 21:34:11.180922: step 72070, loss = 0.69 (262.4 examples/sec; 0.488 sec/batch)
2018-03-22 21:34:16.128920: step 72080, loss = 0.72 (258.7 examples/sec; 0.495 sec/batch)
2018-03-22 21:34:21.407481: step 72090, loss = 0.83 (242.5 examples/sec; 0.528 sec/batch)
2018-03-22 21:34:26.444151: step 72100, loss = 0.79 (254.1 examples/sec; 0.504 sec/batch)
2018-03-22 21:34:31.476108: step 72110, loss = 0.71 (254.4 examples/sec; 0.503 sec/batch)
2018-03-22 21:34:36.879340: step 72120, loss = 0.73 (236.9 examples/sec; 0.540 sec/batch)
2018-03-22 21:34:41.784368: step 72130, loss = 0.71 (261.0 examples/sec; 0.491 sec/batch)
2018-03-22 21:34:46.842465: step 72140, loss = 0.68 (253.1 examples/sec; 0.506 sec/batch)
2018-03-22 21:34:52.116336: step 72150, loss = 0.79 (242.7 examples/sec; 0.527 sec/batch)
2018-03-22 21:34:56.830979: step 72160, loss = 0.77 (271.5 examples/sec; 0.471 sec/batch)
2018-03-22 21:35:01.883339: step 72170, loss = 0.78 (253.3 examples/sec; 0.505 sec/batch)
2018-03-22 21:35:06.929270: step 72180, loss = 0.59 (253.7 examples/sec; 0.505 sec/batch)
2018-03-22 21:35:12.226361: step 72190, loss = 0.58 (241.6 examples/sec; 0.530 sec/batch)
2018-03-22 21:35:17.677950: step 72200, loss = 0.83 (234.8 examples/sec; 0.545 sec/batch)
2018-03-22 21:35:22.837379: step 72210, loss = 0.68 (248.1 examples/sec; 0.516 sec/batch)
2018-03-22 21:35:28.149824: step 72220, loss = 0.72 (240.9 examples/sec; 0.531 sec/batch)
2018-03-22 21:35:33.562405: step 72230, loss = 0.68 (236.5 examples/sec; 0.541 sec/batch)
2018-03-22 21:35:38.005358: step 72240, loss = 0.80 (288.1 examples/sec; 0.444 sec/batch)
2018-03-22 21:35:42.646707: step 72250, loss = 0.65 (275.8 examples/sec; 0.464 sec/batch)
2018-03-22 21:35:47.166419: step 72260, loss = 0.74 (283.2 examples/sec; 0.452 sec/batch)
2018-03-22 21:35:51.779372: step 72270, loss = 0.65 (277.5 examples/sec; 0.461 sec/batch)
2018-03-22 21:35:56.672383: step 72280, loss = 0.81 (261.6 examples/sec; 0.489 sec/batch)
2018-03-22 21:36:01.572358: step 72290, loss = 0.72 (261.2 examples/sec; 0.490 sec/batch)
2018-03-22 21:36:06.432582: step 72300, loss = 0.68 (263.4 examples/sec; 0.486 sec/batch)
2018-03-22 21:36:12.092961: step 72310, loss = 0.83 (226.1 examples/sec; 0.566 sec/batch)
2018-03-22 21:36:16.998443: step 72320, loss = 0.64 (260.9 examples/sec; 0.491 sec/batch)
2018-03-22 21:36:21.709378: step 72330, loss = 0.74 (271.7 examples/sec; 0.471 sec/batch)
2018-03-22 21:36:26.504422: step 72340, loss = 0.79 (266.9 examples/sec; 0.480 sec/batch)
2018-03-22 21:36:31.810416: step 72350, loss = 0.73 (241.2 examples/sec; 0.531 sec/batch)
2018-03-22 21:36:36.735371: step 72360, loss = 0.61 (259.9 examples/sec; 0.492 sec/batch)
2018-03-22 21:36:41.757548: step 72370, loss = 0.63 (254.9 examples/sec; 0.502 sec/batch)
2018-03-22 21:36:46.183355: step 72380, loss = 0.75 (289.2 examples/sec; 0.443 sec/batch)
2018-03-22 21:36:51.076555: step 72390, loss = 0.62 (261.6 examples/sec; 0.489 sec/batch)
2018-03-22 21:36:56.655270: step 72400, loss = 0.66 (229.4 examples/sec; 0.558 sec/batch)
2018-03-22 21:37:02.424357: step 72410, loss = 0.74 (221.9 examples/sec; 0.577 sec/batch)
2018-03-22 21:37:06.991400: step 72420, loss = 0.65 (280.3 examples/sec; 0.457 sec/batch)
2018-03-22 21:37:11.887694: step 72430, loss = 0.69 (261.4 examples/sec; 0.490 sec/batch)
2018-03-22 21:37:16.606382: step 72440, loss = 0.63 (271.3 examples/sec; 0.472 sec/batch)
2018-03-22 21:37:21.258604: step 72450, loss = 0.53 (275.1 examples/sec; 0.465 sec/batch)
2018-03-22 21:37:25.728752: step 72460, loss = 0.64 (286.3 examples/sec; 0.447 sec/batch)
2018-03-22 21:37:30.740839: step 72470, loss = 0.83 (255.4 examples/sec; 0.501 sec/batch)
2018-03-22 21:37:35.602356: step 72480, loss = 0.64 (263.3 examples/sec; 0.486 sec/batch)
2018-03-22 21:37:41.228205: step 72490, loss = 0.76 (227.5 examples/sec; 0.563 sec/batch)
2018-03-22 21:37:46.418368: step 72500, loss = 0.55 (246.6 examples/sec; 0.519 sec/batch)
2018-03-22 21:37:51.436413: step 72510, loss = 0.72 (255.1 examples/sec; 0.502 sec/batch)
2018-03-22 21:37:55.565293: step 72520, loss = 0.89 (310.0 examples/sec; 0.413 sec/batch)
2018-03-22 21:38:01.614477: step 72530, loss = 0.64 (211.6 examples/sec; 0.605 sec/batch)
2018-03-22 21:38:06.723365: step 72540, loss = 1.21 (250.5 examples/sec; 0.511 sec/batch)
2018-03-22 21:38:12.032133: step 72550, loss = 0.77 (241.1 examples/sec; 0.531 sec/batch)
2018-03-22 21:38:16.565367: step 72560, loss = 0.68 (282.4 examples/sec; 0.453 sec/batch)
2018-03-22 21:38:21.481547: step 72570, loss = 0.62 (260.4 examples/sec; 0.492 sec/batch)
2018-03-22 21:38:26.306330: step 72580, loss = 0.66 (265.3 examples/sec; 0.482 sec/batch)
2018-03-22 21:38:31.204189: step 72590, loss = 0.73 (261.3 examples/sec; 0.490 sec/batch)
2018-03-22 21:38:35.990611: step 72600, loss = 0.79 (267.4 examples/sec; 0.479 sec/batch)
2018-03-22 21:38:41.248247: step 72610, loss = 0.64 (243.5 examples/sec; 0.526 sec/batch)
2018-03-22 21:38:46.442403: step 72620, loss = 0.71 (246.4 examples/sec; 0.519 sec/batch)
2018-03-22 21:38:51.744373: step 72630, loss = 0.65 (241.4 examples/sec; 0.530 sec/batch)
2018-03-22 21:38:56.582687: step 72640, loss = 0.68 (264.6 examples/sec; 0.484 sec/batch)
2018-03-22 21:39:01.863873: step 72650, loss = 0.77 (242.4 examples/sec; 0.528 sec/batch)
2018-03-22 21:39:07.284362: step 72660, loss = 0.82 (236.1 examples/sec; 0.542 sec/batch)
2018-03-22 21:39:12.161226: step 72670, loss = 0.75 (262.5 examples/sec; 0.488 sec/batch)
2018-03-22 21:39:17.060677: step 72680, loss = 0.66 (261.3 examples/sec; 0.490 sec/batch)
2018-03-22 21:39:22.206396: step 72690, loss = 0.81 (248.8 examples/sec; 0.515 sec/batch)
2018-03-22 21:39:27.067249: step 72700, loss = 0.65 (263.3 examples/sec; 0.486 sec/batch)
2018-03-22 21:39:32.101337: step 72710, loss = 0.68 (254.3 examples/sec; 0.503 sec/batch)
2018-03-22 21:39:36.810158: step 72720, loss = 0.70 (271.8 examples/sec; 0.471 sec/batch)
2018-03-22 21:39:41.535443: step 72730, loss = 0.59 (270.9 examples/sec; 0.473 sec/batch)
2018-03-22 21:39:45.795592: step 72740, loss = 0.73 (300.5 examples/sec; 0.426 sec/batch)
2018-03-22 21:39:51.295345: step 72750, loss = 0.78 (232.7 examples/sec; 0.550 sec/batch)
2018-03-22 21:39:56.992046: step 72760, loss = 0.79 (224.7 examples/sec; 0.570 sec/batch)
2018-03-22 21:40:02.380353: step 72770, loss = 0.77 (237.6 examples/sec; 0.539 sec/batch)
2018-03-22 21:40:06.966467: step 72780, loss = 0.60 (279.1 examples/sec; 0.459 sec/batch)
2018-03-22 21:40:11.190854: step 72790, loss = 0.83 (303.0 examples/sec; 0.422 sec/batch)
2018-03-22 21:40:16.760860: step 72800, loss = 0.74 (229.8 examples/sec; 0.557 sec/batch)
2018-03-22 21:40:22.201396: step 72810, loss = 0.72 (235.3 examples/sec; 0.544 sec/batch)
2018-03-22 21:40:27.128096: step 72820, loss = 0.71 (259.8 examples/sec; 0.493 sec/batch)
2018-03-22 21:40:32.039680: step 72830, loss = 0.68 (260.6 examples/sec; 0.491 sec/batch)
2018-03-22 21:40:37.065384: step 72840, loss = 0.82 (254.7 examples/sec; 0.503 sec/batch)
2018-03-22 21:40:42.425427: step 72850, loss = 0.68 (238.8 examples/sec; 0.536 sec/batch)
2018-03-22 21:40:47.035335: step 72860, loss = 0.72 (277.7 examples/sec; 0.461 sec/batch)
2018-03-22 21:40:52.466463: step 72870, loss = 0.79 (235.7 examples/sec; 0.543 sec/batch)
2018-03-22 21:40:57.200696: step 72880, loss = 0.66 (270.4 examples/sec; 0.473 sec/batch)
2018-03-22 21:41:02.550855: step 72890, loss = 1.01 (239.2 examples/sec; 0.535 sec/batch)
2018-03-22 21:41:07.231007: step 72900, loss = 0.77 (273.5 examples/sec; 0.468 sec/batch)
2018-03-22 21:41:12.057371: step 72910, loss = 0.71 (265.2 examples/sec; 0.483 sec/batch)
2018-03-22 21:41:16.949180: step 72920, loss = 0.76 (261.7 examples/sec; 0.489 sec/batch)
2018-03-22 21:41:21.969521: step 72930, loss = 0.83 (255.0 examples/sec; 0.502 sec/batch)
2018-03-22 21:41:26.662372: step 72940, loss = 0.86 (272.8 examples/sec; 0.469 sec/batch)
2018-03-22 21:41:32.018283: step 72950, loss = 0.64 (239.0 examples/sec; 0.536 sec/batch)
2018-03-22 21:41:36.828382: step 72960, loss = 0.73 (266.1 examples/sec; 0.481 sec/batch)
2018-03-22 21:41:42.095302: step 72970, loss = 0.85 (243.0 examples/sec; 0.527 sec/batch)
2018-03-22 21:41:46.500391: step 72980, loss = 0.94 (290.6 examples/sec; 0.441 sec/batch)
2018-03-22 21:41:51.861308: step 72990, loss = 0.70 (238.8 examples/sec; 0.536 sec/batch)
2018-03-22 21:41:57.296226: step 73000, loss = 0.61 (235.5 examples/sec; 0.543 sec/batch)
2018-03-22 21:42:02.874306: step 73010, loss = 0.67 (229.5 examples/sec; 0.558 sec/batch)
2018-03-22 21:42:07.701345: step 73020, loss = 0.65 (265.2 examples/sec; 0.483 sec/batch)
2018-03-22 21:42:12.698283: step 73030, loss = 0.74 (256.2 examples/sec; 0.500 sec/batch)
2018-03-22 21:42:17.422357: step 73040, loss = 0.75 (271.0 examples/sec; 0.472 sec/batch)
2018-03-22 21:42:22.726357: step 73050, loss = 0.85 (241.3 examples/sec; 0.530 sec/batch)
2018-03-22 21:42:27.903362: step 73060, loss = 0.66 (247.2 examples/sec; 0.518 sec/batch)
2018-03-22 21:42:32.803360: step 73070, loss = 0.77 (261.2 examples/sec; 0.490 sec/batch)
2018-03-22 21:42:37.506375: step 73080, loss = 0.63 (272.2 examples/sec; 0.470 sec/batch)
2018-03-22 21:42:43.131368: step 73090, loss = 0.67 (227.6 examples/sec; 0.562 sec/batch)
2018-03-22 21:42:48.031837: step 73100, loss = 0.77 (261.2 examples/sec; 0.490 sec/batch)
2018-03-22 21:42:53.062419: step 73110, loss = 0.69 (254.4 examples/sec; 0.503 sec/batch)
2018-03-22 21:42:57.777437: step 73120, loss = 0.83 (271.5 examples/sec; 0.471 sec/batch)
2018-03-22 21:43:03.271320: step 73130, loss = 0.69 (233.0 examples/sec; 0.549 sec/batch)
2018-03-22 21:43:07.960302: step 73140, loss = 0.74 (273.0 examples/sec; 0.469 sec/batch)
2018-03-22 21:43:12.974328: step 73150, loss = 0.85 (255.3 examples/sec; 0.501 sec/batch)
2018-03-22 21:43:18.077391: step 73160, loss = 0.67 (250.8 examples/sec; 0.510 sec/batch)
2018-03-22 21:43:23.226351: step 73170, loss = 0.93 (248.6 examples/sec; 0.515 sec/batch)
2018-03-22 21:43:27.793347: step 73180, loss = 0.71 (280.3 examples/sec; 0.457 sec/batch)
2018-03-22 21:43:32.400287: step 73190, loss = 0.66 (277.8 examples/sec; 0.461 sec/batch)
2018-03-22 21:43:37.489504: step 73200, loss = 0.67 (251.5 examples/sec; 0.509 sec/batch)
2018-03-22 21:43:42.460780: step 73210, loss = 0.65 (257.5 examples/sec; 0.497 sec/batch)
2018-03-22 21:43:47.692187: step 73220, loss = 0.73 (244.7 examples/sec; 0.523 sec/batch)
2018-03-22 21:43:52.516420: step 73230, loss = 0.78 (265.3 examples/sec; 0.482 sec/batch)
2018-03-22 21:43:57.495923: step 73240, loss = 0.67 (257.1 examples/sec; 0.498 sec/batch)
2018-03-22 21:44:02.198347: step 73250, loss = 0.71 (272.2 examples/sec; 0.470 sec/batch)
2018-03-22 21:44:06.854766: step 73260, loss = 0.70 (274.9 examples/sec; 0.466 sec/batch)
2018-03-22 21:44:11.798377: step 73270, loss = 0.76 (258.9 examples/sec; 0.494 sec/batch)
2018-03-22 21:44:16.846690: step 73280, loss = 0.72 (253.5 examples/sec; 0.505 sec/batch)
2018-03-22 21:44:22.457539: step 73290, loss = 0.85 (228.1 examples/sec; 0.561 sec/batch)
2018-03-22 21:44:27.282883: step 73300, loss = 0.83 (265.3 examples/sec; 0.483 sec/batch)
2018-03-22 21:44:32.459810: step 73310, loss = 0.71 (247.3 examples/sec; 0.518 sec/batch)
2018-03-22 21:44:37.574396: step 73320, loss = 0.55 (250.3 examples/sec; 0.511 sec/batch)
2018-03-22 21:44:43.056099: step 73330, loss = 0.60 (233.5 examples/sec; 0.548 sec/batch)
2018-03-22 21:44:47.748362: step 73340, loss = 0.69 (272.8 examples/sec; 0.469 sec/batch)
2018-03-22 21:44:53.114338: step 73350, loss = 0.62 (238.5 examples/sec; 0.537 sec/batch)
2018-03-22 21:44:58.031658: step 73360, loss = 0.74 (260.3 examples/sec; 0.492 sec/batch)
2018-03-22 21:45:04.377479: step 73370, loss = 0.57 (201.7 examples/sec; 0.635 sec/batch)
2018-03-22 21:45:09.435368: step 73380, loss = 0.70 (253.1 examples/sec; 0.506 sec/batch)
2018-03-22 21:45:14.448348: step 73390, loss = 0.53 (255.3 examples/sec; 0.501 sec/batch)
2018-03-22 21:45:19.971372: step 73400, loss = 0.86 (231.8 examples/sec; 0.552 sec/batch)
2018-03-22 21:45:25.149313: step 73410, loss = 0.72 (247.2 examples/sec; 0.518 sec/batch)
2018-03-22 21:45:30.029384: step 73420, loss = 0.64 (262.3 examples/sec; 0.488 sec/batch)
2018-03-22 21:45:34.953388: step 73430, loss = 0.68 (260.0 examples/sec; 0.492 sec/batch)
2018-03-22 21:45:40.729518: step 73440, loss = 0.73 (221.6 examples/sec; 0.578 sec/batch)
2018-03-22 21:45:45.545416: step 73450, loss = 0.74 (265.8 examples/sec; 0.482 sec/batch)
2018-03-22 21:45:50.537498: step 73460, loss = 0.61 (256.4 examples/sec; 0.499 sec/batch)
2018-03-22 21:45:54.719347: step 73470, loss = 0.61 (306.1 examples/sec; 0.418 sec/batch)
2018-03-22 21:45:59.723411: step 73480, loss = 0.70 (255.8 examples/sec; 0.500 sec/batch)
2018-03-22 21:46:04.536510: step 73490, loss = 0.78 (265.9 examples/sec; 0.481 sec/batch)
2018-03-22 21:46:09.749935: step 73500, loss = 0.88 (245.5 examples/sec; 0.521 sec/batch)
2018-03-22 21:46:14.271402: step 73510, loss = 0.62 (283.1 examples/sec; 0.452 sec/batch)
2018-03-22 21:46:19.032922: step 73520, loss = 0.78 (268.8 examples/sec; 0.476 sec/batch)
2018-03-22 21:46:23.860333: step 73530, loss = 0.69 (265.2 examples/sec; 0.483 sec/batch)
2018-03-22 21:46:28.849447: step 73540, loss = 1.04 (256.6 examples/sec; 0.499 sec/batch)
2018-03-22 21:46:34.392352: step 73550, loss = 0.71 (230.9 examples/sec; 0.554 sec/batch)
2018-03-22 21:46:39.581432: step 73560, loss = 0.76 (246.7 examples/sec; 0.519 sec/batch)
2018-03-22 21:46:45.037226: step 73570, loss = 0.75 (234.6 examples/sec; 0.546 sec/batch)
2018-03-22 21:46:50.230369: step 73580, loss = 0.69 (246.5 examples/sec; 0.519 sec/batch)
2018-03-22 21:46:55.424442: step 73590, loss = 0.90 (246.4 examples/sec; 0.519 sec/batch)
2018-03-22 21:47:00.687050: step 73600, loss = 0.60 (243.2 examples/sec; 0.526 sec/batch)
2018-03-22 21:47:05.444417: step 73610, loss = 0.70 (269.1 examples/sec; 0.476 sec/batch)
2018-03-22 21:47:10.398692: step 73620, loss = 0.63 (258.4 examples/sec; 0.495 sec/batch)
2018-03-22 21:47:14.576688: step 73630, loss = 0.86 (306.4 examples/sec; 0.418 sec/batch)
2018-03-22 21:47:19.801736: step 73640, loss = 0.61 (245.0 examples/sec; 0.523 sec/batch)
2018-03-22 21:47:24.823067: step 73650, loss = 0.88 (254.9 examples/sec; 0.502 sec/batch)
2018-03-22 21:47:30.071764: step 73660, loss = 0.73 (243.9 examples/sec; 0.525 sec/batch)
2018-03-22 21:47:35.350413: step 73670, loss = 0.85 (242.5 examples/sec; 0.528 sec/batch)
2018-03-22 21:47:39.725967: step 73680, loss = 0.70 (292.6 examples/sec; 0.438 sec/batch)
2018-03-22 21:47:44.157371: step 73690, loss = 0.66 (288.8 examples/sec; 0.443 sec/batch)
2018-03-22 21:47:49.540289: step 73700, loss = 0.65 (237.8 examples/sec; 0.538 sec/batch)
2018-03-22 21:47:54.771648: step 73710, loss = 0.83 (244.7 examples/sec; 0.523 sec/batch)
2018-03-22 21:47:59.299372: step 73720, loss = 0.76 (282.7 examples/sec; 0.453 sec/batch)
2018-03-22 21:48:04.142422: step 73730, loss = 0.80 (264.3 examples/sec; 0.484 sec/batch)
2018-03-22 21:48:09.432643: step 73740, loss = 0.72 (242.0 examples/sec; 0.529 sec/batch)
2018-03-22 21:48:14.560517: step 73750, loss = 0.71 (249.6 examples/sec; 0.513 sec/batch)
2018-03-22 21:48:19.614413: step 73760, loss = 0.70 (253.3 examples/sec; 0.505 sec/batch)
2018-03-22 21:48:24.319289: step 73770, loss = 0.71 (272.1 examples/sec; 0.470 sec/batch)
2018-03-22 21:48:29.250502: step 73780, loss = 0.59 (259.6 examples/sec; 0.493 sec/batch)
2018-03-22 21:48:34.185446: step 73790, loss = 0.67 (259.4 examples/sec; 0.493 sec/batch)
2018-03-22 21:48:39.006429: step 73800, loss = 0.76 (265.5 examples/sec; 0.482 sec/batch)
2018-03-22 21:48:43.925334: step 73810, loss = 0.82 (260.2 examples/sec; 0.492 sec/batch)
2018-03-22 21:48:49.466007: step 73820, loss = 0.78 (231.0 examples/sec; 0.554 sec/batch)
2018-03-22 21:48:54.756451: step 73830, loss = 0.70 (241.9 examples/sec; 0.529 sec/batch)
2018-03-22 21:48:59.568415: step 73840, loss = 0.73 (266.0 examples/sec; 0.481 sec/batch)
2018-03-22 21:49:04.528309: step 73850, loss = 0.79 (258.1 examples/sec; 0.496 sec/batch)
2018-03-22 21:49:09.547062: step 73860, loss = 0.58 (255.0 examples/sec; 0.502 sec/batch)
2018-03-22 21:49:15.242401: step 73870, loss = 0.82 (224.7 examples/sec; 0.570 sec/batch)
2018-03-22 21:49:20.474369: step 73880, loss = 0.61 (244.6 examples/sec; 0.523 sec/batch)
2018-03-22 21:49:25.087353: step 73890, loss = 0.63 (277.5 examples/sec; 0.461 sec/batch)
2018-03-22 21:49:30.809697: step 73900, loss = 0.72 (223.7 examples/sec; 0.572 sec/batch)
2018-03-22 21:49:35.558366: step 73910, loss = 0.66 (269.5 examples/sec; 0.475 sec/batch)
2018-03-22 21:49:41.065379: step 73920, loss = 0.68 (232.4 examples/sec; 0.551 sec/batch)
2018-03-22 21:49:45.592598: step 73930, loss = 0.58 (282.7 examples/sec; 0.453 sec/batch)
2018-03-22 21:49:50.676398: step 73940, loss = 0.80 (251.8 examples/sec; 0.508 sec/batch)
2018-03-22 21:49:55.718388: step 73950, loss = 0.89 (253.9 examples/sec; 0.504 sec/batch)
2018-03-22 21:50:00.623099: step 73960, loss = 0.94 (261.0 examples/sec; 0.490 sec/batch)
2018-03-22 21:50:05.351716: step 73970, loss = 0.83 (270.7 examples/sec; 0.473 sec/batch)
2018-03-22 21:50:10.514701: step 73980, loss = 0.65 (247.9 examples/sec; 0.516 sec/batch)
2018-03-22 21:50:15.445423: step 73990, loss = 0.79 (259.6 examples/sec; 0.493 sec/batch)
2018-03-22 21:50:20.839989: step 74000, loss = 0.76 (237.3 examples/sec; 0.539 sec/batch)
2018-03-22 21:50:26.064403: step 74010, loss = 0.69 (245.0 examples/sec; 0.522 sec/batch)
2018-03-22 21:50:31.051607: step 74020, loss = 0.70 (256.7 examples/sec; 0.499 sec/batch)
2018-03-22 21:50:35.962395: step 74030, loss = 0.66 (260.7 examples/sec; 0.491 sec/batch)
2018-03-22 21:50:41.367400: step 74040, loss = 0.79 (236.8 examples/sec; 0.541 sec/batch)
2018-03-22 21:50:46.594371: step 74050, loss = 0.78 (244.9 examples/sec; 0.523 sec/batch)
2018-03-22 21:50:51.318390: step 74060, loss = 0.67 (271.0 examples/sec; 0.472 sec/batch)
2018-03-22 21:50:55.923342: step 74070, loss = 0.84 (278.0 examples/sec; 0.460 sec/batch)
2018-03-22 21:51:00.578635: step 74080, loss = 0.75 (275.0 examples/sec; 0.466 sec/batch)
2018-03-22 21:51:05.564330: step 74090, loss = 0.60 (256.7 examples/sec; 0.499 sec/batch)
2018-03-22 21:51:10.824983: step 74100, loss = 0.69 (243.3 examples/sec; 0.526 sec/batch)
2018-03-22 21:51:15.478407: step 74110, loss = 0.75 (275.1 examples/sec; 0.465 sec/batch)
2018-03-22 21:51:20.606374: step 74120, loss = 0.61 (249.6 examples/sec; 0.513 sec/batch)
2018-03-22 21:51:25.256809: step 74130, loss = 0.77 (275.2 examples/sec; 0.465 sec/batch)
2018-03-22 21:51:29.538883: step 74140, loss = 0.60 (298.9 examples/sec; 0.428 sec/batch)
2018-03-22 21:51:34.757405: step 74150, loss = 0.77 (245.3 examples/sec; 0.522 sec/batch)
2018-03-22 21:51:40.734412: step 74160, loss = 0.66 (214.2 examples/sec; 0.598 sec/batch)
2018-03-22 21:51:46.151355: step 74170, loss = 0.73 (236.3 examples/sec; 0.542 sec/batch)
2018-03-22 21:51:51.045471: step 74180, loss = 0.76 (261.5 examples/sec; 0.489 sec/batch)
2018-03-22 21:51:55.276329: step 74190, loss = 0.69 (302.5 examples/sec; 0.423 sec/batch)
2018-03-22 21:52:00.727491: step 74200, loss = 0.78 (234.8 examples/sec; 0.545 sec/batch)
2018-03-22 21:52:05.677171: step 74210, loss = 0.83 (258.6 examples/sec; 0.495 sec/batch)
2018-03-22 21:52:10.626270: step 74220, loss = 0.60 (258.6 examples/sec; 0.495 sec/batch)
2018-03-22 21:52:15.259656: step 74230, loss = 0.67 (276.3 examples/sec; 0.463 sec/batch)
2018-03-22 21:52:20.446381: step 74240, loss = 0.59 (246.8 examples/sec; 0.519 sec/batch)
2018-03-22 21:52:25.073379: step 74250, loss = 0.74 (276.6 examples/sec; 0.463 sec/batch)
2018-03-22 21:52:30.137971: step 74260, loss = 0.86 (252.7 examples/sec; 0.506 sec/batch)
2018-03-22 21:52:34.830829: step 74270, loss = 0.82 (272.8 examples/sec; 0.469 sec/batch)
2018-03-22 21:52:39.589350: step 74280, loss = 0.51 (269.0 examples/sec; 0.476 sec/batch)
2018-03-22 21:52:44.972229: step 74290, loss = 0.56 (237.8 examples/sec; 0.538 sec/batch)
2018-03-22 21:52:50.306491: step 74300, loss = 0.60 (240.0 examples/sec; 0.533 sec/batch)
2018-03-22 21:52:55.241355: step 74310, loss = 0.80 (259.4 examples/sec; 0.493 sec/batch)
2018-03-22 21:53:00.442938: step 74320, loss = 0.66 (246.1 examples/sec; 0.520 sec/batch)
2018-03-22 21:53:04.895017: step 74330, loss = 0.68 (287.5 examples/sec; 0.445 sec/batch)
2018-03-22 21:53:09.651316: step 74340, loss = 0.73 (269.1 examples/sec; 0.476 sec/batch)
2018-03-22 21:53:14.409354: step 74350, loss = 0.80 (269.0 examples/sec; 0.476 sec/batch)
2018-03-22 21:53:19.221462: step 74360, loss = 0.65 (266.0 examples/sec; 0.481 sec/batch)
2018-03-22 21:53:24.068929: step 74370, loss = 0.73 (264.1 examples/sec; 0.485 sec/batch)
2018-03-22 21:53:29.045376: step 74380, loss = 0.69 (257.2 examples/sec; 0.498 sec/batch)
2018-03-22 21:53:33.622534: step 74390, loss = 0.61 (279.6 examples/sec; 0.458 sec/batch)
2018-03-22 21:53:38.560382: step 74400, loss = 0.69 (259.2 examples/sec; 0.494 sec/batch)
2018-03-22 21:53:42.995688: step 74410, loss = 0.70 (288.6 examples/sec; 0.444 sec/batch)
2018-03-22 21:53:47.624402: step 74420, loss = 0.65 (276.5 examples/sec; 0.463 sec/batch)
2018-03-22 21:53:52.558039: step 74430, loss = 0.69 (259.4 examples/sec; 0.493 sec/batch)
2018-03-22 21:53:57.445414: step 74440, loss = 0.66 (261.9 examples/sec; 0.489 sec/batch)
2018-03-22 21:54:02.309381: step 74450, loss = 0.74 (263.2 examples/sec; 0.486 sec/batch)
2018-03-22 21:54:07.032671: step 74460, loss = 0.82 (271.0 examples/sec; 0.472 sec/batch)
2018-03-22 21:54:11.677163: step 74470, loss = 0.72 (275.6 examples/sec; 0.464 sec/batch)
2018-03-22 21:54:16.321405: step 74480, loss = 0.79 (275.6 examples/sec; 0.464 sec/batch)
2018-03-22 21:54:21.439583: step 74490, loss = 0.58 (250.1 examples/sec; 0.512 sec/batch)
2018-03-22 21:54:26.924902: step 74500, loss = 0.76 (233.3 examples/sec; 0.549 sec/batch)
2018-03-22 21:54:31.852819: step 74510, loss = 0.78 (259.7 examples/sec; 0.493 sec/batch)
2018-03-22 21:54:36.347922: step 74520, loss = 0.67 (284.8 examples/sec; 0.450 sec/batch)
2018-03-22 21:54:41.342318: step 74530, loss = 0.67 (256.3 examples/sec; 0.499 sec/batch)
2018-03-22 21:54:45.515298: step 74540, loss = 0.66 (306.7 examples/sec; 0.417 sec/batch)
2018-03-22 21:54:49.977692: step 74550, loss = 0.80 (286.8 examples/sec; 0.446 sec/batch)
2018-03-22 21:54:54.790353: step 74560, loss = 0.66 (266.0 examples/sec; 0.481 sec/batch)
2018-03-22 21:55:00.061447: step 74570, loss = 0.70 (242.8 examples/sec; 0.527 sec/batch)
2018-03-22 21:55:04.805817: step 74580, loss = 0.74 (269.8 examples/sec; 0.474 sec/batch)
2018-03-22 21:55:09.991381: step 74590, loss = 0.68 (246.8 examples/sec; 0.519 sec/batch)
2018-03-22 21:55:14.913252: step 74600, loss = 0.74 (260.1 examples/sec; 0.492 sec/batch)
2018-03-22 21:55:18.801940: step 74610, loss = 0.63 (329.2 examples/sec; 0.389 sec/batch)
2018-03-22 21:55:23.567537: step 74620, loss = 0.80 (268.6 examples/sec; 0.477 sec/batch)
2018-03-22 21:55:28.337317: step 74630, loss = 0.64 (268.4 examples/sec; 0.477 sec/batch)
2018-03-22 21:55:33.417257: step 74640, loss = 0.65 (252.0 examples/sec; 0.508 sec/batch)
2018-03-22 21:55:38.429329: step 74650, loss = 0.78 (255.4 examples/sec; 0.501 sec/batch)
2018-03-22 21:55:42.942751: step 74660, loss = 0.60 (283.6 examples/sec; 0.451 sec/batch)
2018-03-22 21:55:47.282700: step 74670, loss = 0.62 (294.9 examples/sec; 0.434 sec/batch)
2018-03-22 21:55:52.202343: step 74680, loss = 0.79 (260.2 examples/sec; 0.492 sec/batch)
2018-03-22 21:55:56.528295: step 74690, loss = 0.87 (295.9 examples/sec; 0.433 sec/batch)
2018-03-22 21:56:01.507839: step 74700, loss = 0.75 (257.1 examples/sec; 0.498 sec/batch)
2018-03-22 21:56:06.123380: step 74710, loss = 0.74 (277.3 examples/sec; 0.462 sec/batch)
2018-03-22 21:56:11.148424: step 74720, loss = 0.70 (254.7 examples/sec; 0.503 sec/batch)
2018-03-22 21:56:15.576546: step 74730, loss = 0.60 (289.1 examples/sec; 0.443 sec/batch)
2018-03-22 21:56:20.534806: step 74740, loss = 0.74 (258.2 examples/sec; 0.496 sec/batch)
2018-03-22 21:56:25.177306: step 74750, loss = 0.69 (275.7 examples/sec; 0.464 sec/batch)
2018-03-22 21:56:30.107432: step 74760, loss = 0.72 (259.6 examples/sec; 0.493 sec/batch)
2018-03-22 21:56:34.692467: step 74770, loss = 0.67 (279.2 examples/sec; 0.459 sec/batch)
2018-03-22 21:56:39.704906: step 74780, loss = 0.77 (255.4 examples/sec; 0.501 sec/batch)
2018-03-22 21:56:44.618955: step 74790, loss = 0.61 (260.5 examples/sec; 0.491 sec/batch)
2018-03-22 21:56:50.168721: step 74800, loss = 0.83 (230.6 examples/sec; 0.555 sec/batch)
2018-03-22 21:56:55.001346: step 74810, loss = 0.78 (264.9 examples/sec; 0.483 sec/batch)
2018-03-22 21:57:00.634400: step 74820, loss = 0.69 (227.2 examples/sec; 0.563 sec/batch)
2018-03-22 21:57:05.104399: step 74830, loss = 0.82 (286.4 examples/sec; 0.447 sec/batch)
2018-03-22 21:57:10.392406: step 74840, loss = 0.81 (242.1 examples/sec; 0.529 sec/batch)
2018-03-22 21:57:15.272269: step 74850, loss = 0.77 (262.3 examples/sec; 0.488 sec/batch)
2018-03-22 21:57:20.443810: step 74860, loss = 0.62 (247.5 examples/sec; 0.517 sec/batch)
2018-03-22 21:57:25.261648: step 74870, loss = 0.78 (265.7 examples/sec; 0.482 sec/batch)
2018-03-22 21:57:30.120333: step 74880, loss = 0.69 (263.4 examples/sec; 0.486 sec/batch)
2018-03-22 21:57:34.938016: step 74890, loss = 0.67 (265.7 examples/sec; 0.482 sec/batch)
2018-03-22 21:57:40.090808: step 74900, loss = 0.68 (248.4 examples/sec; 0.515 sec/batch)
2018-03-22 21:57:45.163205: step 74910, loss = 0.67 (252.3 examples/sec; 0.507 sec/batch)
2018-03-22 21:57:50.461364: step 74920, loss = 0.70 (241.6 examples/sec; 0.530 sec/batch)
2018-03-22 21:57:55.423105: step 74930, loss = 0.62 (258.0 examples/sec; 0.496 sec/batch)
2018-03-22 21:58:00.510437: step 74940, loss = 0.62 (251.6 examples/sec; 0.509 sec/batch)
2018-03-22 21:58:05.292350: step 74950, loss = 0.63 (267.7 examples/sec; 0.478 sec/batch)
2018-03-22 21:58:09.736398: step 74960, loss = 0.72 (288.0 examples/sec; 0.444 sec/batch)
2018-03-22 21:58:14.468392: step 74970, loss = 0.69 (270.5 examples/sec; 0.473 sec/batch)
2018-03-22 21:58:18.749383: step 74980, loss = 0.71 (299.0 examples/sec; 0.428 sec/batch)
2018-03-22 21:58:23.843389: step 74990, loss = 0.90 (251.3 examples/sec; 0.509 sec/batch)
2018-03-22 21:58:28.777357: step 75000, loss = 0.68 (259.4 examples/sec; 0.493 sec/batch)
2018-03-22 21:58:33.594546: step 75010, loss = 0.76 (265.7 examples/sec; 0.482 sec/batch)
2018-03-22 21:58:37.974500: step 75020, loss = 0.67 (292.2 examples/sec; 0.438 sec/batch)
2018-03-22 21:58:42.926538: step 75030, loss = 0.59 (258.5 examples/sec; 0.495 sec/batch)
2018-03-22 21:58:47.526319: step 75040, loss = 0.62 (278.3 examples/sec; 0.460 sec/batch)
2018-03-22 21:58:52.744334: step 75050, loss = 0.87 (245.3 examples/sec; 0.522 sec/batch)
2018-03-22 21:58:57.529330: step 75060, loss = 0.68 (267.5 examples/sec; 0.478 sec/batch)
2018-03-22 21:59:02.914324: step 75070, loss = 0.75 (237.7 examples/sec; 0.538 sec/batch)
2018-03-22 21:59:07.279242: step 75080, loss = 0.77 (293.2 examples/sec; 0.436 sec/batch)
2018-03-22 21:59:11.469330: step 75090, loss = 0.95 (305.5 examples/sec; 0.419 sec/batch)
2018-03-22 21:59:16.328322: step 75100, loss = 0.70 (263.4 examples/sec; 0.486 sec/batch)
2018-03-22 21:59:21.228300: step 75110, loss = 0.74 (261.2 examples/sec; 0.490 sec/batch)
2018-03-22 21:59:26.254139: step 75120, loss = 0.75 (254.7 examples/sec; 0.503 sec/batch)
2018-03-22 21:59:30.942308: step 75130, loss = 0.64 (273.0 examples/sec; 0.469 sec/batch)
2018-03-22 21:59:35.590701: step 75140, loss = 0.56 (275.4 examples/sec; 0.465 sec/batch)
2018-03-22 21:59:40.490288: step 75150, loss = 0.58 (261.2 examples/sec; 0.490 sec/batch)
2018-03-22 21:59:44.879344: step 75160, loss = 0.66 (291.6 examples/sec; 0.439 sec/batch)
2018-03-22 21:59:49.351406: step 75170, loss = 0.88 (286.2 examples/sec; 0.447 sec/batch)
2018-03-22 21:59:54.145151: step 75180, loss = 0.87 (267.0 examples/sec; 0.479 sec/batch)
2018-03-22 21:59:58.810119: step 75190, loss = 0.68 (274.4 examples/sec; 0.466 sec/batch)
2018-03-22 22:00:04.120666: step 75200, loss = 0.58 (241.0 examples/sec; 0.531 sec/batch)
2018-03-22 22:00:08.521373: step 75210, loss = 0.72 (290.9 examples/sec; 0.440 sec/batch)
2018-03-22 22:00:13.752548: step 75220, loss = 0.80 (244.7 examples/sec; 0.523 sec/batch)
2018-03-22 22:00:18.099472: step 75230, loss = 0.72 (294.5 examples/sec; 0.435 sec/batch)
2018-03-22 22:00:23.058352: step 75240, loss = 0.71 (258.1 examples/sec; 0.496 sec/batch)
2018-03-22 22:00:27.800576: step 75250, loss = 0.81 (269.9 examples/sec; 0.474 sec/batch)
2018-03-22 22:00:32.565371: step 75260, loss = 0.85 (268.6 examples/sec; 0.476 sec/batch)
2018-03-22 22:00:37.233991: step 75270, loss = 0.79 (274.2 examples/sec; 0.467 sec/batch)
2018-03-22 22:00:42.201407: step 75280, loss = 0.76 (257.7 examples/sec; 0.497 sec/batch)
2018-03-22 22:00:46.824457: step 75290, loss = 0.85 (276.9 examples/sec; 0.462 sec/batch)
2018-03-22 22:00:52.186376: step 75300, loss = 0.67 (238.7 examples/sec; 0.536 sec/batch)
2018-03-22 22:00:56.675136: step 75310, loss = 0.63 (285.2 examples/sec; 0.449 sec/batch)
2018-03-22 22:01:01.691040: step 75320, loss = 0.81 (255.2 examples/sec; 0.502 sec/batch)
2018-03-22 22:01:06.153338: step 75330, loss = 0.79 (286.8 examples/sec; 0.446 sec/batch)
2018-03-22 22:01:11.574317: step 75340, loss = 0.85 (236.1 examples/sec; 0.542 sec/batch)
2018-03-22 22:01:16.078036: step 75350, loss = 0.75 (284.2 examples/sec; 0.450 sec/batch)
2018-03-22 22:01:20.910319: step 75360, loss = 0.69 (264.9 examples/sec; 0.483 sec/batch)
2018-03-22 22:01:25.717373: step 75370, loss = 0.76 (266.3 examples/sec; 0.481 sec/batch)
2018-03-22 22:01:30.704326: step 75380, loss = 0.85 (256.7 examples/sec; 0.499 sec/batch)
2018-03-22 22:01:35.544515: step 75390, loss = 0.71 (264.5 examples/sec; 0.484 sec/batch)
2018-03-22 22:01:40.847352: step 75400, loss = 0.75 (241.4 examples/sec; 0.530 sec/batch)
2018-03-22 22:01:45.564663: step 75410, loss = 0.65 (271.3 examples/sec; 0.472 sec/batch)
2018-03-22 22:01:50.192543: step 75420, loss = 0.71 (276.6 examples/sec; 0.463 sec/batch)
2018-03-22 22:01:54.875372: step 75430, loss = 0.68 (273.3 examples/sec; 0.468 sec/batch)
2018-03-22 22:01:59.722092: step 75440, loss = 0.70 (264.1 examples/sec; 0.485 sec/batch)
2018-03-22 22:02:04.588439: step 75450, loss = 0.71 (263.0 examples/sec; 0.487 sec/batch)
2018-03-22 22:02:09.533351: step 75460, loss = 0.83 (258.9 examples/sec; 0.494 sec/batch)
2018-03-22 22:02:14.613465: step 75470, loss = 0.67 (252.0 examples/sec; 0.508 sec/batch)
2018-03-22 22:02:19.569851: step 75480, loss = 0.67 (258.3 examples/sec; 0.496 sec/batch)
2018-03-22 22:02:24.129920: step 75490, loss = 0.75 (280.7 examples/sec; 0.456 sec/batch)
2018-03-22 22:02:28.494615: step 75500, loss = 0.74 (293.3 examples/sec; 0.436 sec/batch)
2018-03-22 22:02:32.807718: step 75510, loss = 0.63 (296.8 examples/sec; 0.431 sec/batch)
2018-03-22 22:02:36.980098: step 75520, loss = 0.82 (306.8 examples/sec; 0.417 sec/batch)
2018-03-22 22:02:42.001336: step 75530, loss = 0.64 (254.9 examples/sec; 0.502 sec/batch)
2018-03-22 22:02:46.738293: step 75540, loss = 0.80 (270.2 examples/sec; 0.474 sec/batch)
2018-03-22 22:02:51.879319: step 75550, loss = 0.63 (249.0 examples/sec; 0.514 sec/batch)
2018-03-22 22:02:56.677432: step 75560, loss = 0.67 (266.8 examples/sec; 0.480 sec/batch)
2018-03-22 22:03:01.820244: step 75570, loss = 0.61 (248.9 examples/sec; 0.514 sec/batch)
2018-03-22 22:03:06.309417: step 75580, loss = 0.57 (285.1 examples/sec; 0.449 sec/batch)
2018-03-22 22:03:11.315355: step 75590, loss = 0.68 (255.7 examples/sec; 0.501 sec/batch)
2018-03-22 22:03:16.097566: step 75600, loss = 0.80 (267.7 examples/sec; 0.478 sec/batch)
2018-03-22 22:03:21.443380: step 75610, loss = 0.87 (239.4 examples/sec; 0.535 sec/batch)
2018-03-22 22:03:26.521299: step 75620, loss = 0.83 (252.1 examples/sec; 0.508 sec/batch)
2018-03-22 22:03:31.340405: step 75630, loss = 0.75 (265.6 examples/sec; 0.482 sec/batch)
2018-03-22 22:03:35.777693: step 75640, loss = 0.67 (288.5 examples/sec; 0.444 sec/batch)
2018-03-22 22:03:40.030332: step 75650, loss = 0.83 (301.0 examples/sec; 0.425 sec/batch)
2018-03-22 22:03:44.634387: step 75660, loss = 0.56 (278.0 examples/sec; 0.460 sec/batch)
2018-03-22 22:03:49.077971: step 75670, loss = 0.63 (288.1 examples/sec; 0.444 sec/batch)
2018-03-22 22:03:54.105398: step 75680, loss = 0.59 (254.6 examples/sec; 0.503 sec/batch)
2018-03-22 22:03:58.728352: step 75690, loss = 0.59 (276.9 examples/sec; 0.462 sec/batch)
2018-03-22 22:04:03.925882: step 75700, loss = 0.79 (246.3 examples/sec; 0.520 sec/batch)
2018-03-22 22:04:08.619412: step 75710, loss = 0.76 (272.7 examples/sec; 0.469 sec/batch)
2018-03-22 22:04:13.857333: step 75720, loss = 0.73 (244.4 examples/sec; 0.524 sec/batch)
2018-03-22 22:04:18.502897: step 75730, loss = 0.68 (275.5 examples/sec; 0.465 sec/batch)
2018-03-22 22:04:23.733430: step 75740, loss = 0.71 (244.7 examples/sec; 0.523 sec/batch)
2018-03-22 22:04:28.298343: step 75750, loss = 0.73 (280.4 examples/sec; 0.456 sec/batch)
2018-03-22 22:04:33.630477: step 75760, loss = 0.59 (240.1 examples/sec; 0.533 sec/batch)
2018-03-22 22:04:38.326365: step 75770, loss = 0.57 (272.6 examples/sec; 0.470 sec/batch)
2018-03-22 22:04:43.332346: step 75780, loss = 0.88 (255.7 examples/sec; 0.501 sec/batch)
2018-03-22 22:04:47.626467: step 75790, loss = 0.68 (298.1 examples/sec; 0.429 sec/batch)
2018-03-22 22:04:52.759362: step 75800, loss = 0.77 (249.4 examples/sec; 0.513 sec/batch)
2018-03-22 22:04:57.263877: step 75810, loss = 0.79 (284.2 examples/sec; 0.450 sec/batch)
2018-03-22 22:05:02.283434: step 75820, loss = 0.93 (255.0 examples/sec; 0.502 sec/batch)
2018-03-22 22:05:06.967101: step 75830, loss = 0.69 (273.3 examples/sec; 0.468 sec/batch)
2018-03-22 22:05:12.026336: step 75840, loss = 0.70 (253.0 examples/sec; 0.506 sec/batch)
2018-03-22 22:05:16.614551: step 75850, loss = 0.61 (279.0 examples/sec; 0.459 sec/batch)
2018-03-22 22:05:21.365966: step 75860, loss = 0.70 (269.4 examples/sec; 0.475 sec/batch)
2018-03-22 22:05:25.566357: step 75870, loss = 0.67 (304.7 examples/sec; 0.420 sec/batch)
2018-03-22 22:05:30.446386: step 75880, loss = 0.68 (262.3 examples/sec; 0.488 sec/batch)
2018-03-22 22:05:35.204385: step 75890, loss = 0.67 (269.0 examples/sec; 0.476 sec/batch)
2018-03-22 22:05:40.608244: step 75900, loss = 0.58 (236.9 examples/sec; 0.540 sec/batch)
2018-03-22 22:05:45.014344: step 75910, loss = 0.90 (290.5 examples/sec; 0.441 sec/batch)
2018-03-22 22:05:50.030398: step 75920, loss = 0.73 (255.2 examples/sec; 0.502 sec/batch)
2018-03-22 22:05:54.376419: step 75930, loss = 0.64 (294.5 examples/sec; 0.435 sec/batch)
2018-03-22 22:05:59.094382: step 75940, loss = 0.74 (271.3 examples/sec; 0.472 sec/batch)
2018-03-22 22:06:03.937343: step 75950, loss = 0.72 (264.3 examples/sec; 0.484 sec/batch)
2018-03-22 22:06:08.551403: step 75960, loss = 0.69 (277.4 examples/sec; 0.461 sec/batch)
2018-03-22 22:06:13.418034: step 75970, loss = 0.79 (263.0 examples/sec; 0.487 sec/batch)
2018-03-22 22:06:18.101439: step 75980, loss = 0.76 (273.3 examples/sec; 0.468 sec/batch)
2018-03-22 22:06:23.080390: step 75990, loss = 0.60 (257.1 examples/sec; 0.498 sec/batch)
2018-03-22 22:06:27.784137: step 76000, loss = 0.62 (272.1 examples/sec; 0.470 sec/batch)
2018-03-22 22:06:32.434947: step 76010, loss = 0.61 (275.2 examples/sec; 0.465 sec/batch)
2018-03-22 22:06:36.967287: step 76020, loss = 0.72 (282.4 examples/sec; 0.453 sec/batch)
2018-03-22 22:06:42.374433: step 76030, loss = 0.64 (236.7 examples/sec; 0.541 sec/batch)
2018-03-22 22:06:47.374459: step 76040, loss = 0.72 (256.0 examples/sec; 0.500 sec/batch)
2018-03-22 22:06:52.221499: step 76050, loss = 0.74 (264.1 examples/sec; 0.485 sec/batch)
2018-03-22 22:06:56.912490: step 76060, loss = 0.62 (272.9 examples/sec; 0.469 sec/batch)
2018-03-22 22:07:01.743286: step 76070, loss = 0.65 (265.0 examples/sec; 0.483 sec/batch)
2018-03-22 22:07:06.404219: step 76080, loss = 0.69 (274.6 examples/sec; 0.466 sec/batch)
2018-03-22 22:07:11.550466: step 76090, loss = 0.84 (248.7 examples/sec; 0.515 sec/batch)
2018-03-22 22:07:16.456887: step 76100, loss = 0.58 (260.9 examples/sec; 0.491 sec/batch)
2018-03-22 22:07:21.595382: step 76110, loss = 0.76 (249.1 examples/sec; 0.514 sec/batch)
2018-03-22 22:07:26.419038: step 76120, loss = 0.69 (265.4 examples/sec; 0.482 sec/batch)
2018-03-22 22:07:31.354399: step 76130, loss = 0.81 (259.4 examples/sec; 0.494 sec/batch)
2018-03-22 22:07:35.584427: step 76140, loss = 0.63 (302.6 examples/sec; 0.423 sec/batch)
2018-03-22 22:07:40.740386: step 76150, loss = 0.63 (248.3 examples/sec; 0.516 sec/batch)
2018-03-22 22:07:44.864857: step 76160, loss = 0.84 (310.3 examples/sec; 0.412 sec/batch)
2018-03-22 22:07:49.783177: step 76170, loss = 0.59 (260.3 examples/sec; 0.492 sec/batch)
2018-03-22 22:07:54.466314: step 76180, loss = 0.62 (273.3 examples/sec; 0.468 sec/batch)
2018-03-22 22:07:59.934435: step 76190, loss = 0.64 (234.1 examples/sec; 0.547 sec/batch)
2018-03-22 22:08:05.306607: step 76200, loss = 0.75 (238.3 examples/sec; 0.537 sec/batch)
2018-03-22 22:08:09.988491: step 76210, loss = 0.67 (273.4 examples/sec; 0.468 sec/batch)
2018-03-22 22:08:14.883560: step 76220, loss = 0.84 (261.5 examples/sec; 0.490 sec/batch)
2018-03-22 22:08:19.694376: step 76230, loss = 0.64 (266.1 examples/sec; 0.481 sec/batch)
2018-03-22 22:08:24.679687: step 76240, loss = 0.69 (256.8 examples/sec; 0.499 sec/batch)
2018-03-22 22:08:29.354618: step 76250, loss = 0.77 (273.8 examples/sec; 0.467 sec/batch)
2018-03-22 22:08:34.265456: step 76260, loss = 0.67 (260.6 examples/sec; 0.491 sec/batch)
2018-03-22 22:08:38.883590: step 76270, loss = 0.57 (277.2 examples/sec; 0.462 sec/batch)
2018-03-22 22:08:43.459381: step 76280, loss = 0.85 (279.7 examples/sec; 0.458 sec/batch)
2018-03-22 22:08:48.041418: step 76290, loss = 0.78 (279.4 examples/sec; 0.458 sec/batch)
2018-03-22 22:08:52.963503: step 76300, loss = 0.62 (260.1 examples/sec; 0.492 sec/batch)
2018-03-22 22:08:57.520363: step 76310, loss = 0.91 (280.9 examples/sec; 0.456 sec/batch)
2018-03-22 22:09:02.722400: step 76320, loss = 0.78 (246.1 examples/sec; 0.520 sec/batch)
2018-03-22 22:09:07.364588: step 76330, loss = 0.90 (275.7 examples/sec; 0.464 sec/batch)
2018-03-22 22:09:12.829979: step 76340, loss = 0.78 (234.2 examples/sec; 0.547 sec/batch)
2018-03-22 22:09:17.354428: step 76350, loss = 0.73 (282.9 examples/sec; 0.452 sec/batch)
2018-03-22 22:09:22.555425: step 76360, loss = 0.74 (246.1 examples/sec; 0.520 sec/batch)
2018-03-22 22:09:27.409694: step 76370, loss = 0.64 (263.7 examples/sec; 0.485 sec/batch)
2018-03-22 22:09:32.130318: step 76380, loss = 0.56 (271.2 examples/sec; 0.472 sec/batch)
2018-03-22 22:09:36.910329: step 76390, loss = 0.96 (267.8 examples/sec; 0.478 sec/batch)
2018-03-22 22:09:42.204055: step 76400, loss = 0.59 (241.8 examples/sec; 0.529 sec/batch)
2018-03-22 22:09:47.217419: step 76410, loss = 0.77 (255.3 examples/sec; 0.501 sec/batch)
2018-03-22 22:09:52.567394: step 76420, loss = 0.76 (239.3 examples/sec; 0.535 sec/batch)
2018-03-22 22:09:57.310357: step 76430, loss = 0.67 (269.9 examples/sec; 0.474 sec/batch)
2018-03-22 22:10:02.319033: step 76440, loss = 0.62 (255.6 examples/sec; 0.501 sec/batch)
2018-03-22 22:10:06.983366: step 76450, loss = 0.82 (274.4 examples/sec; 0.466 sec/batch)
2018-03-22 22:10:12.565194: step 76460, loss = 0.95 (229.3 examples/sec; 0.558 sec/batch)
2018-03-22 22:10:17.017658: step 76470, loss = 0.78 (287.5 examples/sec; 0.445 sec/batch)
2018-03-22 22:10:21.476479: step 76480, loss = 0.75 (287.1 examples/sec; 0.446 sec/batch)
2018-03-22 22:10:25.772888: step 76490, loss = 0.67 (297.9 examples/sec; 0.430 sec/batch)
2018-03-22 22:10:30.803479: step 76500, loss = 0.75 (254.4 examples/sec; 0.503 sec/batch)
2018-03-22 22:10:35.547346: step 76510, loss = 0.74 (269.8 examples/sec; 0.474 sec/batch)
2018-03-22 22:10:40.696491: step 76520, loss = 0.77 (248.6 examples/sec; 0.515 sec/batch)
2018-03-22 22:10:45.278980: step 76530, loss = 0.66 (279.3 examples/sec; 0.458 sec/batch)
2018-03-22 22:10:50.149240: step 76540, loss = 0.77 (262.8 examples/sec; 0.487 sec/batch)
2018-03-22 22:10:54.383277: step 76550, loss = 0.81 (302.3 examples/sec; 0.423 sec/batch)
2018-03-22 22:10:59.370024: step 76560, loss = 0.56 (256.7 examples/sec; 0.499 sec/batch)
2018-03-22 22:11:04.238386: step 76570, loss = 0.76 (262.9 examples/sec; 0.487 sec/batch)
2018-03-22 22:11:09.181609: step 76580, loss = 0.85 (258.9 examples/sec; 0.494 sec/batch)
2018-03-22 22:11:13.851875: step 76590, loss = 0.66 (274.1 examples/sec; 0.467 sec/batch)
2018-03-22 22:11:18.830549: step 76600, loss = 0.65 (257.1 examples/sec; 0.498 sec/batch)
2018-03-22 22:11:23.657575: step 76610, loss = 0.77 (265.2 examples/sec; 0.483 sec/batch)
2018-03-22 22:11:28.035341: step 76620, loss = 0.72 (292.4 examples/sec; 0.438 sec/batch)
2018-03-22 22:11:32.811419: step 76630, loss = 0.68 (268.0 examples/sec; 0.478 sec/batch)
2018-03-22 22:11:37.270327: step 76640, loss = 0.76 (287.1 examples/sec; 0.446 sec/batch)
2018-03-22 22:11:42.396887: step 76650, loss = 0.81 (249.7 examples/sec; 0.513 sec/batch)
2018-03-22 22:11:47.157446: step 76660, loss = 0.82 (268.9 examples/sec; 0.476 sec/batch)
2018-03-22 22:11:51.927548: step 76670, loss = 0.73 (268.3 examples/sec; 0.477 sec/batch)
2018-03-22 22:11:56.648351: step 76680, loss = 0.91 (271.1 examples/sec; 0.472 sec/batch)
2018-03-22 22:12:02.008938: step 76690, loss = 0.79 (238.8 examples/sec; 0.536 sec/batch)
2018-03-22 22:12:06.776366: step 76700, loss = 0.75 (268.5 examples/sec; 0.477 sec/batch)
2018-03-22 22:12:11.761374: step 76710, loss = 0.71 (256.8 examples/sec; 0.499 sec/batch)
2018-03-22 22:12:16.176394: step 76720, loss = 0.79 (289.9 examples/sec; 0.442 sec/batch)
2018-03-22 22:12:21.194419: step 76730, loss = 0.71 (255.1 examples/sec; 0.502 sec/batch)
2018-03-22 22:12:26.095394: step 76740, loss = 0.72 (261.2 examples/sec; 0.490 sec/batch)
2018-03-22 22:12:31.418099: step 76750, loss = 0.81 (240.5 examples/sec; 0.532 sec/batch)
2018-03-22 22:12:35.676328: step 76760, loss = 0.70 (300.6 examples/sec; 0.426 sec/batch)
2018-03-22 22:12:41.157480: step 76770, loss = 0.64 (233.5 examples/sec; 0.548 sec/batch)
2018-03-22 22:12:45.516309: step 76780, loss = 0.89 (293.7 examples/sec; 0.436 sec/batch)
2018-03-22 22:12:50.193496: step 76790, loss = 0.76 (273.7 examples/sec; 0.468 sec/batch)
2018-03-22 22:12:55.547891: step 76800, loss = 0.69 (239.1 examples/sec; 0.535 sec/batch)
2018-03-22 22:13:00.536445: step 76810, loss = 0.83 (256.6 examples/sec; 0.499 sec/batch)
2018-03-22 22:13:05.385433: step 76820, loss = 0.69 (264.0 examples/sec; 0.485 sec/batch)
2018-03-22 22:13:10.070408: step 76830, loss = 0.90 (273.2 examples/sec; 0.468 sec/batch)
2018-03-22 22:13:14.945117: step 76840, loss = 0.64 (262.6 examples/sec; 0.487 sec/batch)
2018-03-22 22:13:19.786093: step 76850, loss = 0.63 (264.4 examples/sec; 0.484 sec/batch)
2018-03-22 22:13:24.729390: step 76860, loss = 0.69 (258.9 examples/sec; 0.494 sec/batch)
2018-03-22 22:13:29.719806: step 76870, loss = 0.75 (256.5 examples/sec; 0.499 sec/batch)
2018-03-22 22:13:34.147608: step 76880, loss = 0.81 (289.1 examples/sec; 0.443 sec/batch)
2018-03-22 22:13:38.786896: step 76890, loss = 0.65 (275.9 examples/sec; 0.464 sec/batch)
2018-03-22 22:13:43.864677: step 76900, loss = 0.70 (252.1 examples/sec; 0.508 sec/batch)
2018-03-22 22:13:48.366412: step 76910, loss = 0.67 (284.3 examples/sec; 0.450 sec/batch)
2018-03-22 22:13:53.409654: step 76920, loss = 0.67 (253.8 examples/sec; 0.504 sec/batch)
2018-03-22 22:13:57.751417: step 76930, loss = 0.64 (294.8 examples/sec; 0.434 sec/batch)
2018-03-22 22:14:02.783348: step 76940, loss = 0.65 (254.4 examples/sec; 0.503 sec/batch)
2018-03-22 22:14:07.300915: step 76950, loss = 0.74 (283.3 examples/sec; 0.452 sec/batch)
2018-03-22 22:14:12.505933: step 76960, loss = 0.73 (245.9 examples/sec; 0.521 sec/batch)
2018-03-22 22:14:16.798355: step 76970, loss = 0.70 (298.2 examples/sec; 0.429 sec/batch)
2018-03-22 22:14:21.567088: step 76980, loss = 0.63 (268.4 examples/sec; 0.477 sec/batch)
2018-03-22 22:14:26.326420: step 76990, loss = 0.68 (268.9 examples/sec; 0.476 sec/batch)
2018-03-22 22:14:31.437715: step 77000, loss = 0.76 (250.4 examples/sec; 0.511 sec/batch)
2018-03-22 22:14:36.233395: step 77010, loss = 0.69 (266.9 examples/sec; 0.480 sec/batch)
2018-03-22 22:14:41.465383: step 77020, loss = 0.64 (244.6 examples/sec; 0.523 sec/batch)
2018-03-22 22:14:46.272415: step 77030, loss = 0.72 (266.3 examples/sec; 0.481 sec/batch)
2018-03-22 22:14:50.859784: step 77040, loss = 0.84 (279.0 examples/sec; 0.459 sec/batch)
2018-03-22 22:14:55.232342: step 77050, loss = 0.75 (292.7 examples/sec; 0.437 sec/batch)
2018-03-22 22:15:00.064396: step 77060, loss = 0.89 (264.9 examples/sec; 0.483 sec/batch)
2018-03-22 22:15:05.019264: step 77070, loss = 0.76 (258.3 examples/sec; 0.495 sec/batch)
2018-03-22 22:15:09.542906: step 77080, loss = 0.77 (283.0 examples/sec; 0.452 sec/batch)
2018-03-22 22:15:14.189443: step 77090, loss = 0.69 (275.5 examples/sec; 0.465 sec/batch)
2018-03-22 22:15:19.017376: step 77100, loss = 0.78 (265.1 examples/sec; 0.483 sec/batch)
2018-03-22 22:15:23.524078: step 77110, loss = 0.79 (284.0 examples/sec; 0.451 sec/batch)
2018-03-22 22:15:28.079341: step 77120, loss = 0.73 (281.0 examples/sec; 0.456 sec/batch)
2018-03-22 22:15:32.685128: step 77130, loss = 0.89 (277.9 examples/sec; 0.461 sec/batch)
2018-03-22 22:15:37.162397: step 77140, loss = 0.68 (285.9 examples/sec; 0.448 sec/batch)
2018-03-22 22:15:42.207440: step 77150, loss = 0.64 (253.7 examples/sec; 0.505 sec/batch)
2018-03-22 22:15:46.904386: step 77160, loss = 0.79 (272.5 examples/sec; 0.470 sec/batch)
2018-03-22 22:15:51.789505: step 77170, loss = 0.80 (262.0 examples/sec; 0.489 sec/batch)
2018-03-22 22:15:56.591393: step 77180, loss = 0.67 (266.6 examples/sec; 0.480 sec/batch)
2018-03-22 22:16:02.106455: step 77190, loss = 0.80 (232.1 examples/sec; 0.552 sec/batch)
2018-03-22 22:16:06.774308: step 77200, loss = 0.70 (274.2 examples/sec; 0.467 sec/batch)
2018-03-22 22:16:11.885797: step 77210, loss = 0.79 (250.4 examples/sec; 0.511 sec/batch)
2018-03-22 22:16:16.227383: step 77220, loss = 0.66 (294.8 examples/sec; 0.434 sec/batch)
2018-03-22 22:16:21.220395: step 77230, loss = 0.74 (256.4 examples/sec; 0.499 sec/batch)
2018-03-22 22:16:25.972578: step 77240, loss = 0.84 (269.3 examples/sec; 0.475 sec/batch)
2018-03-22 22:16:31.105716: step 77250, loss = 0.66 (249.4 examples/sec; 0.513 sec/batch)
2018-03-22 22:16:35.622384: step 77260, loss = 0.74 (283.4 examples/sec; 0.452 sec/batch)
2018-03-22 22:16:40.853191: step 77270, loss = 0.75 (244.7 examples/sec; 0.523 sec/batch)
2018-03-22 22:16:45.480989: step 77280, loss = 0.69 (276.6 examples/sec; 0.463 sec/batch)
2018-03-22 22:16:50.580766: step 77290, loss = 0.72 (251.0 examples/sec; 0.510 sec/batch)
2018-03-22 22:16:55.512223: step 77300, loss = 0.82 (259.6 examples/sec; 0.493 sec/batch)
2018-03-22 22:17:00.643361: step 77310, loss = 0.63 (249.5 examples/sec; 0.513 sec/batch)
2018-03-22 22:17:05.287339: step 77320, loss = 0.62 (275.6 examples/sec; 0.464 sec/batch)
2018-03-22 22:17:10.402323: step 77330, loss = 0.90 (250.2 examples/sec; 0.511 sec/batch)
2018-03-22 22:17:15.268429: step 77340, loss = 0.74 (263.0 examples/sec; 0.487 sec/batch)
2018-03-22 22:17:20.006830: step 77350, loss = 0.83 (270.1 examples/sec; 0.474 sec/batch)
2018-03-22 22:17:24.831790: step 77360, loss = 0.73 (265.3 examples/sec; 0.482 sec/batch)
2018-03-22 22:17:29.742427: step 77370, loss = 0.69 (260.7 examples/sec; 0.491 sec/batch)
2018-03-22 22:17:34.743407: step 77380, loss = 0.75 (255.9 examples/sec; 0.500 sec/batch)
2018-03-22 22:17:39.646327: step 77390, loss = 0.70 (261.1 examples/sec; 0.490 sec/batch)
2018-03-22 22:17:44.524771: step 77400, loss = 0.62 (262.4 examples/sec; 0.488 sec/batch)
2018-03-22 22:17:49.215589: step 77410, loss = 0.61 (272.9 examples/sec; 0.469 sec/batch)
2018-03-22 22:17:54.283416: step 77420, loss = 0.68 (252.6 examples/sec; 0.507 sec/batch)
2018-03-22 22:17:58.779101: step 77430, loss = 0.71 (284.7 examples/sec; 0.450 sec/batch)
2018-03-22 22:18:03.715360: step 77440, loss = 0.63 (259.3 examples/sec; 0.494 sec/batch)
2018-03-22 22:18:08.290341: step 77450, loss = 0.66 (279.8 examples/sec; 0.457 sec/batch)
2018-03-22 22:18:13.411622: step 77460, loss = 0.68 (249.9 examples/sec; 0.512 sec/batch)
2018-03-22 22:18:17.890004: step 77470, loss = 0.72 (285.8 examples/sec; 0.448 sec/batch)
2018-03-22 22:18:23.152350: step 77480, loss = 0.64 (243.2 examples/sec; 0.526 sec/batch)
2018-03-22 22:18:27.934821: step 77490, loss = 0.59 (267.6 examples/sec; 0.478 sec/batch)
2018-03-22 22:18:33.280598: step 77500, loss = 0.62 (239.4 examples/sec; 0.535 sec/batch)
2018-03-22 22:18:38.093361: step 77510, loss = 0.76 (266.0 examples/sec; 0.481 sec/batch)
2018-03-22 22:18:43.128490: step 77520, loss = 0.71 (254.2 examples/sec; 0.504 sec/batch)
2018-03-22 22:18:47.247902: step 77530, loss = 0.68 (310.7 examples/sec; 0.412 sec/batch)
2018-03-22 22:18:52.567617: step 77540, loss = 0.59 (240.6 examples/sec; 0.532 sec/batch)
2018-03-22 22:18:57.458636: step 77550, loss = 0.66 (261.7 examples/sec; 0.489 sec/batch)
2018-03-22 22:19:02.374626: step 77560, loss = 0.79 (260.4 examples/sec; 0.492 sec/batch)
2018-03-22 22:19:06.887387: step 77570, loss = 0.83 (283.6 examples/sec; 0.451 sec/batch)
2018-03-22 22:19:11.922353: step 77580, loss = 0.72 (254.2 examples/sec; 0.503 sec/batch)
2018-03-22 22:19:16.277403: step 77590, loss = 0.71 (293.9 examples/sec; 0.436 sec/batch)
2018-03-22 22:19:21.497019: step 77600, loss = 0.82 (245.2 examples/sec; 0.522 sec/batch)
2018-03-22 22:19:25.720327: step 77610, loss = 0.73 (303.1 examples/sec; 0.422 sec/batch)
2018-03-22 22:19:30.185442: step 77620, loss = 0.77 (286.7 examples/sec; 0.447 sec/batch)
2018-03-22 22:19:35.310348: step 77630, loss = 0.68 (249.8 examples/sec; 0.512 sec/batch)
2018-03-22 22:19:40.785392: step 77640, loss = 0.80 (233.8 examples/sec; 0.548 sec/batch)
2018-03-22 22:19:45.486796: step 77650, loss = 0.74 (272.3 examples/sec; 0.470 sec/batch)
2018-03-22 22:19:50.622363: step 77660, loss = 0.75 (249.2 examples/sec; 0.514 sec/batch)
2018-03-22 22:19:55.544630: step 77670, loss = 0.74 (260.0 examples/sec; 0.492 sec/batch)
2018-03-22 22:20:00.212423: step 77680, loss = 0.70 (274.2 examples/sec; 0.467 sec/batch)
2018-03-22 22:20:04.738771: step 77690, loss = 0.68 (282.8 examples/sec; 0.453 sec/batch)
2018-03-22 22:20:09.359847: step 77700, loss = 0.68 (277.0 examples/sec; 0.462 sec/batch)
2018-03-22 22:20:14.573424: step 77710, loss = 0.59 (245.5 examples/sec; 0.521 sec/batch)
2018-03-22 22:20:19.534099: step 77720, loss = 0.65 (258.0 examples/sec; 0.496 sec/batch)
2018-03-22 22:20:24.016625: step 77730, loss = 0.71 (285.6 examples/sec; 0.448 sec/batch)
2018-03-22 22:20:28.701385: step 77740, loss = 0.72 (273.2 examples/sec; 0.468 sec/batch)
2018-03-22 22:20:33.876346: step 77750, loss = 0.63 (247.3 examples/sec; 0.517 sec/batch)
2018-03-22 22:20:38.810365: step 77760, loss = 0.53 (259.4 examples/sec; 0.493 sec/batch)
2018-03-22 22:20:44.051375: step 77770, loss = 0.77 (244.2 examples/sec; 0.524 sec/batch)
2018-03-22 22:20:48.778790: step 77780, loss = 0.70 (270.8 examples/sec; 0.473 sec/batch)
2018-03-22 22:20:53.719388: step 77790, loss = 0.61 (259.1 examples/sec; 0.494 sec/batch)
2018-03-22 22:20:58.704162: step 77800, loss = 0.61 (256.8 examples/sec; 0.498 sec/batch)
2018-03-22 22:21:03.657386: step 77810, loss = 0.91 (258.4 examples/sec; 0.495 sec/batch)
2018-03-22 22:21:08.547286: step 77820, loss = 0.73 (261.8 examples/sec; 0.489 sec/batch)
2018-03-22 22:21:13.513412: step 77830, loss = 0.58 (257.7 examples/sec; 0.497 sec/batch)
2018-03-22 22:21:17.975201: step 77840, loss = 0.98 (286.9 examples/sec; 0.446 sec/batch)
2018-03-22 22:21:22.790362: step 77850, loss = 0.81 (265.8 examples/sec; 0.482 sec/batch)
2018-03-22 22:21:27.469449: step 77860, loss = 0.93 (273.6 examples/sec; 0.468 sec/batch)
2018-03-22 22:21:32.272009: step 77870, loss = 0.63 (266.5 examples/sec; 0.480 sec/batch)
2018-03-22 22:21:37.166362: step 77880, loss = 0.57 (261.5 examples/sec; 0.489 sec/batch)
2018-03-22 22:21:42.103357: step 77890, loss = 0.60 (259.3 examples/sec; 0.494 sec/batch)
2018-03-22 22:21:47.243712: step 77900, loss = 0.71 (249.0 examples/sec; 0.514 sec/batch)
2018-03-22 22:21:52.798298: step 77910, loss = 0.76 (230.4 examples/sec; 0.555 sec/batch)
2018-03-22 22:21:57.409621: step 77920, loss = 0.64 (277.6 examples/sec; 0.461 sec/batch)
2018-03-22 22:22:02.796380: step 77930, loss = 0.68 (237.6 examples/sec; 0.539 sec/batch)
2018-03-22 22:22:07.214314: step 77940, loss = 0.91 (289.7 examples/sec; 0.442 sec/batch)
2018-03-22 22:22:11.831149: step 77950, loss = 0.73 (277.2 examples/sec; 0.462 sec/batch)
2018-03-22 22:22:16.459048: step 77960, loss = 0.65 (276.6 examples/sec; 0.463 sec/batch)
2018-03-22 22:22:21.678405: step 77970, loss = 0.64 (245.2 examples/sec; 0.522 sec/batch)
2018-03-22 22:22:26.507215: step 77980, loss = 0.77 (265.1 examples/sec; 0.483 sec/batch)
2018-03-22 22:22:31.338932: step 77990, loss = 0.85 (264.9 examples/sec; 0.483 sec/batch)
2018-03-22 22:22:35.914357: step 78000, loss = 0.74 (279.8 examples/sec; 0.458 sec/batch)
2018-03-22 22:22:40.627418: step 78010, loss = 0.71 (271.6 examples/sec; 0.471 sec/batch)
2018-03-22 22:22:45.028343: step 78020, loss = 0.66 (290.8 examples/sec; 0.440 sec/batch)
2018-03-22 22:22:50.445356: step 78030, loss = 0.76 (236.3 examples/sec; 0.542 sec/batch)
2018-03-22 22:22:55.823769: step 78040, loss = 0.70 (238.0 examples/sec; 0.538 sec/batch)
2018-03-22 22:23:00.579359: step 78050, loss = 0.65 (269.2 examples/sec; 0.476 sec/batch)
2018-03-22 22:23:04.892283: step 78060, loss = 0.66 (296.8 examples/sec; 0.431 sec/batch)
2018-03-22 22:23:09.020264: step 78070, loss = 0.58 (310.1 examples/sec; 0.413 sec/batch)
2018-03-22 22:23:13.358290: step 78080, loss = 0.74 (295.1 examples/sec; 0.434 sec/batch)
2018-03-22 22:23:17.597893: step 78090, loss = 0.77 (301.9 examples/sec; 0.424 sec/batch)
2018-03-22 22:23:22.335022: step 78100, loss = 0.57 (270.2 examples/sec; 0.474 sec/batch)
2018-03-22 22:23:27.320317: step 78110, loss = 0.73 (256.8 examples/sec; 0.499 sec/batch)
2018-03-22 22:23:32.843406: step 78120, loss = 0.83 (231.8 examples/sec; 0.552 sec/batch)
2018-03-22 22:23:37.625746: step 78130, loss = 0.80 (267.7 examples/sec; 0.478 sec/batch)
2018-03-22 22:23:42.564562: step 78140, loss = 0.67 (259.2 examples/sec; 0.494 sec/batch)
2018-03-22 22:23:47.389078: step 78150, loss = 0.69 (265.3 examples/sec; 0.482 sec/batch)
2018-03-22 22:23:52.621428: step 78160, loss = 0.53 (244.6 examples/sec; 0.523 sec/batch)
2018-03-22 22:23:56.963559: step 78170, loss = 0.74 (294.8 examples/sec; 0.434 sec/batch)
2018-03-22 22:24:02.438387: step 78180, loss = 0.84 (233.8 examples/sec; 0.547 sec/batch)
2018-03-22 22:24:07.332423: step 78190, loss = 0.61 (261.5 examples/sec; 0.489 sec/batch)
2018-03-22 22:24:12.478961: step 78200, loss = 0.71 (248.7 examples/sec; 0.515 sec/batch)
2018-03-22 22:24:16.981492: step 78210, loss = 0.63 (284.3 examples/sec; 0.450 sec/batch)
2018-03-22 22:24:22.217465: step 78220, loss = 0.76 (244.5 examples/sec; 0.524 sec/batch)
2018-03-22 22:24:27.046809: step 78230, loss = 0.63 (265.0 examples/sec; 0.483 sec/batch)
2018-03-22 22:24:32.176372: step 78240, loss = 0.75 (249.5 examples/sec; 0.513 sec/batch)
2018-03-22 22:24:36.728780: step 78250, loss = 0.77 (281.2 examples/sec; 0.455 sec/batch)
2018-03-22 22:24:41.851403: step 78260, loss = 0.78 (249.9 examples/sec; 0.512 sec/batch)
2018-03-22 22:24:46.536200: step 78270, loss = 0.90 (273.2 examples/sec; 0.468 sec/batch)
2018-03-22 22:24:51.686439: step 78280, loss = 0.73 (248.5 examples/sec; 0.515 sec/batch)
2018-03-22 22:24:56.666338: step 78290, loss = 0.70 (257.0 examples/sec; 0.498 sec/batch)
2018-03-22 22:25:01.804466: step 78300, loss = 0.75 (249.1 examples/sec; 0.514 sec/batch)
2018-03-22 22:25:06.424062: step 78310, loss = 0.91 (277.1 examples/sec; 0.462 sec/batch)
2018-03-22 22:25:11.666737: step 78320, loss = 0.61 (244.2 examples/sec; 0.524 sec/batch)
2018-03-22 22:25:16.164424: step 78330, loss = 0.66 (284.6 examples/sec; 0.450 sec/batch)
2018-03-22 22:25:21.554412: step 78340, loss = 0.86 (237.5 examples/sec; 0.539 sec/batch)
2018-03-22 22:25:26.579383: step 78350, loss = 0.78 (254.7 examples/sec; 0.502 sec/batch)
2018-03-22 22:25:31.380344: step 78360, loss = 0.76 (266.6 examples/sec; 0.480 sec/batch)
2018-03-22 22:25:36.145116: step 78370, loss = 0.60 (268.6 examples/sec; 0.476 sec/batch)
2018-03-22 22:25:40.959503: step 78380, loss = 0.62 (265.9 examples/sec; 0.481 sec/batch)
2018-03-22 22:25:45.790292: step 78390, loss = 0.71 (265.0 examples/sec; 0.483 sec/batch)
2018-03-22 22:25:50.842161: step 78400, loss = 0.77 (253.4 examples/sec; 0.505 sec/batch)
2018-03-22 22:25:55.258409: step 78410, loss = 0.63 (289.8 examples/sec; 0.442 sec/batch)
2018-03-22 22:26:00.516393: step 78420, loss = 0.97 (243.4 examples/sec; 0.526 sec/batch)
2018-03-22 22:26:04.609116: step 78430, loss = 0.68 (312.7 examples/sec; 0.409 sec/batch)
2018-03-22 22:26:09.223412: step 78440, loss = 0.54 (277.4 examples/sec; 0.461 sec/batch)
2018-03-22 22:26:14.082108: step 78450, loss = 0.84 (263.4 examples/sec; 0.486 sec/batch)
2018-03-22 22:26:19.065387: step 78460, loss = 0.72 (256.9 examples/sec; 0.498 sec/batch)
2018-03-22 22:26:24.071445: step 78470, loss = 0.74 (255.7 examples/sec; 0.501 sec/batch)
2018-03-22 22:26:28.778640: step 78480, loss = 0.72 (271.9 examples/sec; 0.471 sec/batch)
2018-03-22 22:26:33.556705: step 78490, loss = 0.72 (267.9 examples/sec; 0.478 sec/batch)
2018-03-22 22:26:38.355351: step 78500, loss = 0.76 (266.7 examples/sec; 0.480 sec/batch)
2018-03-22 22:26:43.604434: step 78510, loss = 0.68 (243.9 examples/sec; 0.525 sec/batch)
2018-03-22 22:26:48.097359: step 78520, loss = 0.62 (284.9 examples/sec; 0.449 sec/batch)
2018-03-22 22:26:53.117897: step 78530, loss = 0.74 (255.0 examples/sec; 0.502 sec/batch)
2018-03-22 22:26:57.770330: step 78540, loss = 0.71 (275.1 examples/sec; 0.465 sec/batch)
2018-03-22 22:27:02.190651: step 78550, loss = 0.64 (289.6 examples/sec; 0.442 sec/batch)
2018-03-22 22:27:06.255283: step 78560, loss = 0.63 (314.9 examples/sec; 0.406 sec/batch)
2018-03-22 22:27:10.707324: step 78570, loss = 0.65 (287.5 examples/sec; 0.445 sec/batch)
2018-03-22 22:27:15.778401: step 78580, loss = 0.76 (252.4 examples/sec; 0.507 sec/batch)
2018-03-22 22:27:20.739742: step 78590, loss = 0.70 (258.0 examples/sec; 0.496 sec/batch)
2018-03-22 22:27:25.696657: step 78600, loss = 0.70 (258.2 examples/sec; 0.496 sec/batch)
2018-03-22 22:27:30.762780: step 78610, loss = 0.72 (252.7 examples/sec; 0.507 sec/batch)
2018-03-22 22:27:35.542332: step 78620, loss = 0.78 (267.8 examples/sec; 0.478 sec/batch)
2018-03-22 22:27:40.232127: step 78630, loss = 0.77 (272.9 examples/sec; 0.469 sec/batch)
2018-03-22 22:27:44.657413: step 78640, loss = 0.73 (289.2 examples/sec; 0.443 sec/batch)
2018-03-22 22:27:49.502339: step 78650, loss = 0.61 (264.2 examples/sec; 0.484 sec/batch)
2018-03-22 22:27:54.631340: step 78660, loss = 0.73 (249.6 examples/sec; 0.513 sec/batch)
2018-03-22 22:27:59.815456: step 78670, loss = 0.62 (246.9 examples/sec; 0.518 sec/batch)
2018-03-22 22:28:04.843362: step 78680, loss = 0.68 (254.6 examples/sec; 0.503 sec/batch)
2018-03-22 22:28:09.273987: step 78690, loss = 0.89 (288.9 examples/sec; 0.443 sec/batch)
2018-03-22 22:28:14.452758: step 78700, loss = 0.76 (247.2 examples/sec; 0.518 sec/batch)
2018-03-22 22:28:18.913886: step 78710, loss = 0.61 (286.9 examples/sec; 0.446 sec/batch)
2018-03-22 22:28:24.159549: step 78720, loss = 0.73 (244.0 examples/sec; 0.525 sec/batch)
2018-03-22 22:28:28.713302: step 78730, loss = 0.67 (281.1 examples/sec; 0.455 sec/batch)
2018-03-22 22:28:33.627256: step 78740, loss = 0.67 (260.5 examples/sec; 0.491 sec/batch)
2018-03-22 22:28:38.108866: step 78750, loss = 0.65 (285.6 examples/sec; 0.448 sec/batch)
2018-03-22 22:28:43.149353: step 78760, loss = 0.68 (253.9 examples/sec; 0.504 sec/batch)
2018-03-22 22:28:47.860760: step 78770, loss = 0.67 (271.7 examples/sec; 0.471 sec/batch)
2018-03-22 22:28:53.303320: step 78780, loss = 0.75 (235.2 examples/sec; 0.544 sec/batch)
2018-03-22 22:28:58.277376: step 78790, loss = 0.83 (257.3 examples/sec; 0.497 sec/batch)
2018-03-22 22:29:03.647098: step 78800, loss = 0.73 (238.4 examples/sec; 0.537 sec/batch)
2018-03-22 22:29:08.424199: step 78810, loss = 0.78 (267.9 examples/sec; 0.478 sec/batch)
2018-03-22 22:29:13.413739: step 78820, loss = 0.62 (256.5 examples/sec; 0.499 sec/batch)
2018-03-22 22:29:17.735075: step 78830, loss = 0.96 (296.2 examples/sec; 0.432 sec/batch)
2018-03-22 22:29:22.988373: step 78840, loss = 0.66 (243.7 examples/sec; 0.525 sec/batch)
2018-03-22 22:29:27.277544: step 78850, loss = 0.65 (298.4 examples/sec; 0.429 sec/batch)
2018-03-22 22:29:32.485905: step 78860, loss = 0.61 (245.8 examples/sec; 0.521 sec/batch)
2018-03-22 22:29:37.191321: step 78870, loss = 0.82 (272.0 examples/sec; 0.471 sec/batch)
2018-03-22 22:29:42.399964: step 78880, loss = 0.72 (245.7 examples/sec; 0.521 sec/batch)
2018-03-22 22:29:46.935365: step 78890, loss = 0.74 (282.2 examples/sec; 0.454 sec/batch)
2018-03-22 22:29:52.113881: step 78900, loss = 0.67 (247.2 examples/sec; 0.518 sec/batch)
2018-03-22 22:29:56.459407: step 78910, loss = 0.66 (294.6 examples/sec; 0.435 sec/batch)
2018-03-22 22:30:01.710403: step 78920, loss = 0.75 (243.8 examples/sec; 0.525 sec/batch)
2018-03-22 22:30:06.263404: step 78930, loss = 0.71 (281.1 examples/sec; 0.455 sec/batch)
2018-03-22 22:30:11.633082: step 78940, loss = 0.73 (238.4 examples/sec; 0.537 sec/batch)
2018-03-22 22:30:15.973383: step 78950, loss = 0.74 (294.9 examples/sec; 0.434 sec/batch)
2018-03-22 22:30:20.516557: step 78960, loss = 0.77 (281.7 examples/sec; 0.454 sec/batch)
2018-03-22 22:30:25.414436: step 78970, loss = 0.68 (261.3 examples/sec; 0.490 sec/batch)
2018-03-22 22:30:30.611423: step 78980, loss = 0.70 (246.3 examples/sec; 0.520 sec/batch)
2018-03-22 22:30:35.546433: step 78990, loss = 0.62 (259.4 examples/sec; 0.494 sec/batch)
2018-03-22 22:30:40.696019: step 79000, loss = 0.65 (248.6 examples/sec; 0.515 sec/batch)
2018-03-22 22:30:45.234334: step 79010, loss = 0.88 (282.0 examples/sec; 0.454 sec/batch)
2018-03-22 22:30:50.133662: step 79020, loss = 0.77 (261.3 examples/sec; 0.490 sec/batch)
2018-03-22 22:30:55.035712: step 79030, loss = 0.80 (261.1 examples/sec; 0.490 sec/batch)
2018-03-22 22:30:59.988644: step 79040, loss = 0.73 (258.4 examples/sec; 0.495 sec/batch)
2018-03-22 22:31:04.840351: step 79050, loss = 0.62 (263.8 examples/sec; 0.485 sec/batch)
2018-03-22 22:31:09.971300: step 79060, loss = 0.53 (249.5 examples/sec; 0.513 sec/batch)
2018-03-22 22:31:14.680459: step 79070, loss = 0.65 (271.8 examples/sec; 0.471 sec/batch)
2018-03-22 22:31:19.631327: step 79080, loss = 0.68 (258.5 examples/sec; 0.495 sec/batch)
2018-03-22 22:31:24.416386: step 79090, loss = 0.82 (267.5 examples/sec; 0.479 sec/batch)
2018-03-22 22:31:29.242369: step 79100, loss = 0.55 (265.2 examples/sec; 0.483 sec/batch)
2018-03-22 22:31:34.136368: step 79110, loss = 0.56 (261.5 examples/sec; 0.489 sec/batch)
2018-03-22 22:31:38.927192: step 79120, loss = 0.66 (267.2 examples/sec; 0.479 sec/batch)
2018-03-22 22:31:43.704876: step 79130, loss = 0.84 (267.9 examples/sec; 0.478 sec/batch)
2018-03-22 22:31:48.557071: step 79140, loss = 0.66 (263.8 examples/sec; 0.485 sec/batch)
2018-03-22 22:31:53.310386: step 79150, loss = 0.63 (269.3 examples/sec; 0.475 sec/batch)
2018-03-22 22:31:58.080709: step 79160, loss = 0.59 (268.3 examples/sec; 0.477 sec/batch)
2018-03-22 22:32:03.063388: step 79170, loss = 0.72 (256.9 examples/sec; 0.498 sec/batch)
2018-03-22 22:32:08.038404: step 79180, loss = 0.69 (257.3 examples/sec; 0.498 sec/batch)
2018-03-22 22:32:13.252635: step 79190, loss = 0.70 (245.5 examples/sec; 0.521 sec/batch)
2018-03-22 22:32:18.435741: step 79200, loss = 0.71 (247.0 examples/sec; 0.518 sec/batch)
2018-03-22 22:32:23.573097: step 79210, loss = 0.76 (249.2 examples/sec; 0.514 sec/batch)
2018-03-22 22:32:28.360355: step 79220, loss = 0.89 (267.4 examples/sec; 0.479 sec/batch)
2018-03-22 22:32:33.333809: step 79230, loss = 0.69 (257.4 examples/sec; 0.497 sec/batch)
2018-03-22 22:32:37.956522: step 79240, loss = 0.62 (276.9 examples/sec; 0.462 sec/batch)
2018-03-22 22:32:42.835354: step 79250, loss = 0.64 (262.4 examples/sec; 0.488 sec/batch)
2018-03-22 22:32:47.387410: step 79260, loss = 0.73 (281.2 examples/sec; 0.455 sec/batch)
2018-03-22 22:32:52.281356: step 79270, loss = 0.86 (261.5 examples/sec; 0.489 sec/batch)
2018-03-22 22:32:57.348390: step 79280, loss = 0.71 (252.6 examples/sec; 0.507 sec/batch)
2018-03-22 22:33:02.668388: step 79290, loss = 0.68 (240.6 examples/sec; 0.532 sec/batch)
2018-03-22 22:33:07.634652: step 79300, loss = 0.87 (257.7 examples/sec; 0.497 sec/batch)
2018-03-22 22:33:13.103378: step 79310, loss = 0.54 (234.1 examples/sec; 0.547 sec/batch)
2018-03-22 22:33:17.866853: step 79320, loss = 0.75 (268.7 examples/sec; 0.476 sec/batch)
2018-03-22 22:33:22.914088: step 79330, loss = 0.76 (253.6 examples/sec; 0.505 sec/batch)
2018-03-22 22:33:27.633995: step 79340, loss = 0.65 (271.2 examples/sec; 0.472 sec/batch)
2018-03-22 22:33:32.325579: step 79350, loss = 0.71 (272.8 examples/sec; 0.469 sec/batch)
2018-03-22 22:33:36.953361: step 79360, loss = 0.84 (276.6 examples/sec; 0.463 sec/batch)
2018-03-22 22:33:41.822308: step 79370, loss = 0.72 (262.9 examples/sec; 0.487 sec/batch)
2018-03-22 22:33:46.561420: step 79380, loss = 0.87 (270.1 examples/sec; 0.474 sec/batch)
2018-03-22 22:33:51.587317: step 79390, loss = 0.65 (254.7 examples/sec; 0.503 sec/batch)
2018-03-22 22:33:56.923606: step 79400, loss = 0.80 (239.9 examples/sec; 0.534 sec/batch)
2018-03-22 22:34:01.809963: step 79410, loss = 0.65 (262.0 examples/sec; 0.489 sec/batch)
2018-03-22 22:34:05.939373: step 79420, loss = 0.79 (310.0 examples/sec; 0.413 sec/batch)
2018-03-22 22:34:10.653215: step 79430, loss = 0.62 (271.5 examples/sec; 0.471 sec/batch)
2018-03-22 22:34:15.263388: step 79440, loss = 0.73 (277.6 examples/sec; 0.461 sec/batch)
2018-03-22 22:34:19.893411: step 79450, loss = 0.60 (276.5 examples/sec; 0.463 sec/batch)
2018-03-22 22:34:24.731389: step 79460, loss = 0.63 (264.6 examples/sec; 0.484 sec/batch)
2018-03-22 22:34:29.642444: step 79470, loss = 0.89 (260.6 examples/sec; 0.491 sec/batch)
2018-03-22 22:34:34.402168: step 79480, loss = 0.68 (268.9 examples/sec; 0.476 sec/batch)
2018-03-22 22:34:39.051912: step 79490, loss = 0.67 (275.3 examples/sec; 0.465 sec/batch)
2018-03-22 22:34:44.012566: step 79500, loss = 0.59 (258.0 examples/sec; 0.496 sec/batch)
2018-03-22 22:34:48.651416: step 79510, loss = 0.77 (275.9 examples/sec; 0.464 sec/batch)
2018-03-22 22:34:53.902311: step 79520, loss = 0.70 (243.8 examples/sec; 0.525 sec/batch)
2018-03-22 22:34:58.430875: step 79530, loss = 0.74 (282.7 examples/sec; 0.453 sec/batch)
2018-03-22 22:35:03.286079: step 79540, loss = 0.83 (263.6 examples/sec; 0.486 sec/batch)
2018-03-22 22:35:08.322420: step 79550, loss = 0.63 (254.2 examples/sec; 0.504 sec/batch)
2018-03-22 22:35:13.429333: step 79560, loss = 0.67 (250.6 examples/sec; 0.511 sec/batch)
2018-03-22 22:35:18.474337: step 79570, loss = 0.66 (253.7 examples/sec; 0.505 sec/batch)
2018-03-22 22:35:23.477590: step 79580, loss = 0.61 (255.8 examples/sec; 0.500 sec/batch)
2018-03-22 22:35:27.906182: step 79590, loss = 0.72 (289.0 examples/sec; 0.443 sec/batch)
2018-03-22 22:35:33.061320: step 79600, loss = 0.56 (248.3 examples/sec; 0.516 sec/batch)
2018-03-22 22:35:37.785357: step 79610, loss = 0.71 (271.0 examples/sec; 0.472 sec/batch)
2018-03-22 22:35:42.530344: step 79620, loss = 0.70 (269.8 examples/sec; 0.474 sec/batch)
2018-03-22 22:35:47.357406: step 79630, loss = 0.65 (265.2 examples/sec; 0.483 sec/batch)
2018-03-22 22:35:52.658368: step 79640, loss = 0.63 (241.5 examples/sec; 0.530 sec/batch)
2018-03-22 22:35:57.226268: step 79650, loss = 0.74 (280.2 examples/sec; 0.457 sec/batch)
2018-03-22 22:36:02.307408: step 79660, loss = 0.95 (251.9 examples/sec; 0.508 sec/batch)
2018-03-22 22:36:06.903425: step 79670, loss = 0.62 (278.5 examples/sec; 0.460 sec/batch)
2018-03-22 22:36:11.772234: step 79680, loss = 0.73 (262.9 examples/sec; 0.487 sec/batch)
2018-03-22 22:36:16.307665: step 79690, loss = 0.72 (282.2 examples/sec; 0.454 sec/batch)
2018-03-22 22:36:21.550600: step 79700, loss = 0.71 (244.1 examples/sec; 0.524 sec/batch)
2018-03-22 22:36:25.879101: step 79710, loss = 0.53 (295.7 examples/sec; 0.433 sec/batch)
2018-03-22 22:36:31.042291: step 79720, loss = 0.82 (247.9 examples/sec; 0.516 sec/batch)
2018-03-22 22:36:35.995417: step 79730, loss = 0.65 (258.4 examples/sec; 0.495 sec/batch)
2018-03-22 22:36:41.186246: step 79740, loss = 0.58 (246.6 examples/sec; 0.519 sec/batch)
2018-03-22 22:36:45.765819: step 79750, loss = 0.85 (279.5 examples/sec; 0.458 sec/batch)
2018-03-22 22:36:50.930447: step 79760, loss = 0.85 (247.8 examples/sec; 0.516 sec/batch)
2018-03-22 22:36:55.653393: step 79770, loss = 0.76 (271.0 examples/sec; 0.472 sec/batch)
2018-03-22 22:37:00.666104: step 79780, loss = 0.64 (255.4 examples/sec; 0.501 sec/batch)
2018-03-22 22:37:05.564252: step 79790, loss = 0.61 (261.3 examples/sec; 0.490 sec/batch)
2018-03-22 22:37:10.743042: step 79800, loss = 0.79 (247.2 examples/sec; 0.518 sec/batch)
2018-03-22 22:37:15.281601: step 79810, loss = 0.66 (282.0 examples/sec; 0.454 sec/batch)
2018-03-22 22:37:20.056364: step 79820, loss = 0.81 (268.1 examples/sec; 0.477 sec/batch)
2018-03-22 22:37:24.336088: step 79830, loss = 0.70 (299.1 examples/sec; 0.428 sec/batch)
2018-03-22 22:37:28.420248: step 79840, loss = 0.90 (313.4 examples/sec; 0.408 sec/batch)
2018-03-22 22:37:33.378114: step 79850, loss = 0.64 (258.2 examples/sec; 0.496 sec/batch)
2018-03-22 22:37:38.323335: step 79860, loss = 0.72 (258.8 examples/sec; 0.495 sec/batch)
2018-03-22 22:37:43.410338: step 79870, loss = 0.67 (251.6 examples/sec; 0.509 sec/batch)
2018-03-22 22:37:48.101411: step 79880, loss = 0.75 (272.9 examples/sec; 0.469 sec/batch)
2018-03-22 22:37:53.195866: step 79890, loss = 0.75 (251.3 examples/sec; 0.509 sec/batch)
2018-03-22 22:37:57.953544: step 79900, loss = 0.68 (269.0 examples/sec; 0.476 sec/batch)
2018-03-22 22:38:02.959931: step 79910, loss = 0.75 (255.7 examples/sec; 0.501 sec/batch)
2018-03-22 22:38:07.549650: step 79920, loss = 0.70 (278.9 examples/sec; 0.459 sec/batch)
2018-03-22 22:38:12.405398: step 79930, loss = 0.69 (263.6 examples/sec; 0.486 sec/batch)
2018-03-22 22:38:16.975375: step 79940, loss = 0.67 (280.1 examples/sec; 0.457 sec/batch)
2018-03-22 22:38:22.120375: step 79950, loss = 0.84 (248.8 examples/sec; 0.514 sec/batch)
2018-03-22 22:38:26.618209: step 79960, loss = 0.78 (284.6 examples/sec; 0.450 sec/batch)
2018-03-22 22:38:31.631591: step 79970, loss = 0.57 (255.3 examples/sec; 0.501 sec/batch)
2018-03-22 22:38:36.191328: step 79980, loss = 0.85 (280.7 examples/sec; 0.456 sec/batch)
2018-03-22 22:38:41.259840: step 79990, loss = 0.65 (252.5 examples/sec; 0.507 sec/batch)
2018-03-22 22:38:46.230583: step 80000, loss = 0.75 (257.5 examples/sec; 0.497 sec/batch)
2018-03-22 22:38:51.404529: step 80010, loss = 0.73 (247.4 examples/sec; 0.517 sec/batch)
2018-03-22 22:38:56.002357: step 80020, loss = 0.78 (278.4 examples/sec; 0.460 sec/batch)
2018-03-22 22:39:00.880079: step 80030, loss = 0.68 (262.4 examples/sec; 0.488 sec/batch)
2018-03-22 22:39:05.859421: step 80040, loss = 0.77 (257.1 examples/sec; 0.498 sec/batch)
2018-03-22 22:39:10.432074: step 80050, loss = 0.79 (279.9 examples/sec; 0.457 sec/batch)
2018-03-22 22:39:15.363368: step 80060, loss = 0.69 (259.6 examples/sec; 0.493 sec/batch)
2018-03-22 22:39:20.415317: step 80070, loss = 0.60 (253.4 examples/sec; 0.505 sec/batch)
2018-03-22 22:39:25.086106: step 80080, loss = 0.69 (274.0 examples/sec; 0.467 sec/batch)
2018-03-22 22:39:30.495416: step 80090, loss = 0.79 (236.6 examples/sec; 0.541 sec/batch)
2018-03-22 22:39:35.293720: step 80100, loss = 0.58 (266.8 examples/sec; 0.480 sec/batch)
2018-03-22 22:39:40.403759: step 80110, loss = 0.72 (250.5 examples/sec; 0.511 sec/batch)
2018-03-22 22:39:45.094342: step 80120, loss = 0.74 (272.9 examples/sec; 0.469 sec/batch)
2018-03-22 22:39:49.982364: step 80130, loss = 0.69 (261.9 examples/sec; 0.489 sec/batch)
2018-03-22 22:39:54.540968: step 80140, loss = 0.67 (280.8 examples/sec; 0.456 sec/batch)
2018-03-22 22:39:59.206415: step 80150, loss = 0.76 (274.4 examples/sec; 0.467 sec/batch)
2018-03-22 22:40:03.948429: step 80160, loss = 0.74 (269.9 examples/sec; 0.474 sec/batch)
2018-03-22 22:40:08.708385: step 80170, loss = 0.71 (268.9 examples/sec; 0.476 sec/batch)
2018-03-22 22:40:13.900457: step 80180, loss = 0.67 (246.5 examples/sec; 0.519 sec/batch)
2018-03-22 22:40:18.529425: step 80190, loss = 0.64 (276.5 examples/sec; 0.463 sec/batch)
2018-03-22 22:40:23.562684: step 80200, loss = 0.89 (254.3 examples/sec; 0.503 sec/batch)
2018-03-22 22:40:28.360377: step 80210, loss = 0.67 (266.8 examples/sec; 0.480 sec/batch)
2018-03-22 22:40:33.695407: step 80220, loss = 0.62 (239.9 examples/sec; 0.534 sec/batch)
2018-03-22 22:40:38.509431: step 80230, loss = 0.83 (265.9 examples/sec; 0.481 sec/batch)
2018-03-22 22:40:43.387252: step 80240, loss = 0.66 (262.4 examples/sec; 0.488 sec/batch)
2018-03-22 22:40:48.125399: step 80250, loss = 0.80 (270.1 examples/sec; 0.474 sec/batch)
2018-03-22 22:40:53.106371: step 80260, loss = 0.76 (257.0 examples/sec; 0.498 sec/batch)
2018-03-22 22:40:57.832415: step 80270, loss = 0.69 (270.8 examples/sec; 0.473 sec/batch)
2018-03-22 22:41:03.223347: step 80280, loss = 0.61 (237.4 examples/sec; 0.539 sec/batch)
2018-03-22 22:41:07.843100: step 80290, loss = 0.76 (277.1 examples/sec; 0.462 sec/batch)
2018-03-22 22:41:12.697921: step 80300, loss = 0.83 (263.7 examples/sec; 0.485 sec/batch)
2018-03-22 22:41:16.986423: step 80310, loss = 0.60 (298.5 examples/sec; 0.429 sec/batch)
2018-03-22 22:41:21.915482: step 80320, loss = 0.82 (259.7 examples/sec; 0.493 sec/batch)
2018-03-22 22:41:26.832417: step 80330, loss = 0.88 (260.3 examples/sec; 0.492 sec/batch)
2018-03-22 22:41:31.964513: step 80340, loss = 0.75 (249.4 examples/sec; 0.513 sec/batch)
2018-03-22 22:41:36.852308: step 80350, loss = 0.64 (261.9 examples/sec; 0.489 sec/batch)
2018-03-22 22:41:41.771489: step 80360, loss = 0.68 (260.2 examples/sec; 0.492 sec/batch)
2018-03-22 22:41:46.338313: step 80370, loss = 0.63 (280.3 examples/sec; 0.457 sec/batch)
2018-03-22 22:41:51.329363: step 80380, loss = 0.65 (256.5 examples/sec; 0.499 sec/batch)
2018-03-22 22:41:56.236576: step 80390, loss = 0.93 (260.8 examples/sec; 0.491 sec/batch)
2018-03-22 22:42:01.336468: step 80400, loss = 0.57 (251.0 examples/sec; 0.510 sec/batch)
2018-03-22 22:42:06.046847: step 80410, loss = 0.62 (271.7 examples/sec; 0.471 sec/batch)
2018-03-22 22:42:11.208946: step 80420, loss = 0.68 (248.0 examples/sec; 0.516 sec/batch)
2018-03-22 22:42:16.123678: step 80430, loss = 0.69 (260.4 examples/sec; 0.491 sec/batch)
2018-03-22 22:42:21.490353: step 80440, loss = 0.60 (238.5 examples/sec; 0.537 sec/batch)
2018-03-22 22:42:26.012367: step 80450, loss = 0.77 (283.1 examples/sec; 0.452 sec/batch)
2018-03-22 22:42:30.943191: step 80460, loss = 0.65 (259.6 examples/sec; 0.493 sec/batch)
2018-03-22 22:42:35.623461: step 80470, loss = 0.71 (273.5 examples/sec; 0.468 sec/batch)
2018-03-22 22:42:40.515592: step 80480, loss = 0.72 (261.6 examples/sec; 0.489 sec/batch)
2018-03-22 22:42:45.169330: step 80490, loss = 0.66 (275.0 examples/sec; 0.465 sec/batch)
2018-03-22 22:42:50.018191: step 80500, loss = 0.67 (264.0 examples/sec; 0.485 sec/batch)
2018-03-22 22:42:55.086076: step 80510, loss = 0.67 (252.6 examples/sec; 0.507 sec/batch)
2018-03-22 22:42:59.729235: step 80520, loss = 0.67 (275.7 examples/sec; 0.464 sec/batch)
2018-03-22 22:43:04.448127: step 80530, loss = 0.72 (271.3 examples/sec; 0.472 sec/batch)
2018-03-22 22:43:09.255368: step 80540, loss = 0.65 (266.3 examples/sec; 0.481 sec/batch)
2018-03-22 22:43:14.280272: step 80550, loss = 0.68 (254.7 examples/sec; 0.502 sec/batch)
2018-03-22 22:43:18.950441: step 80560, loss = 0.68 (274.1 examples/sec; 0.467 sec/batch)
2018-03-22 22:43:23.856296: step 80570, loss = 0.64 (260.9 examples/sec; 0.491 sec/batch)
2018-03-22 22:43:28.479413: step 80580, loss = 0.77 (276.9 examples/sec; 0.462 sec/batch)
2018-03-22 22:43:33.452355: step 80590, loss = 0.76 (257.4 examples/sec; 0.497 sec/batch)
2018-03-22 22:43:37.842943: step 80600, loss = 0.74 (291.5 examples/sec; 0.439 sec/batch)
2018-03-22 22:43:43.157361: step 80610, loss = 0.71 (240.9 examples/sec; 0.531 sec/batch)
2018-03-22 22:43:47.826377: step 80620, loss = 0.78 (274.1 examples/sec; 0.467 sec/batch)
2018-03-22 22:43:52.962156: step 80630, loss = 0.71 (249.2 examples/sec; 0.514 sec/batch)
2018-03-22 22:43:57.959345: step 80640, loss = 0.67 (256.1 examples/sec; 0.500 sec/batch)
2018-03-22 22:44:02.320069: step 80650, loss = 0.69 (293.5 examples/sec; 0.436 sec/batch)
2018-03-22 22:44:06.800351: step 80660, loss = 0.67 (285.7 examples/sec; 0.448 sec/batch)
2018-03-22 22:44:11.952441: step 80670, loss = 0.70 (248.4 examples/sec; 0.515 sec/batch)
2018-03-22 22:44:16.416326: step 80680, loss = 0.79 (286.7 examples/sec; 0.446 sec/batch)
2018-03-22 22:44:21.285564: step 80690, loss = 0.78 (262.9 examples/sec; 0.487 sec/batch)
2018-03-22 22:44:25.982105: step 80700, loss = 0.71 (272.5 examples/sec; 0.470 sec/batch)
2018-03-22 22:44:31.128183: step 80710, loss = 0.61 (248.7 examples/sec; 0.515 sec/batch)
2018-03-22 22:44:36.092426: step 80720, loss = 0.72 (257.8 examples/sec; 0.496 sec/batch)
2018-03-22 22:44:41.255858: step 80730, loss = 0.64 (247.9 examples/sec; 0.516 sec/batch)
2018-03-22 22:44:45.982040: step 80740, loss = 0.88 (270.8 examples/sec; 0.473 sec/batch)
2018-03-22 22:44:50.861083: step 80750, loss = 0.78 (262.3 examples/sec; 0.488 sec/batch)
2018-03-22 22:44:55.171384: step 80760, loss = 0.63 (297.0 examples/sec; 0.431 sec/batch)
2018-03-22 22:44:59.448778: step 80770, loss = 0.97 (299.2 examples/sec; 0.428 sec/batch)
2018-03-22 22:45:03.672654: step 80780, loss = 0.77 (303.0 examples/sec; 0.422 sec/batch)
2018-03-22 22:45:08.199358: step 80790, loss = 0.66 (282.8 examples/sec; 0.453 sec/batch)
2018-03-22 22:45:13.469334: step 80800, loss = 0.68 (242.9 examples/sec; 0.527 sec/batch)
2018-03-22 22:45:18.157606: step 80810, loss = 0.70 (273.0 examples/sec; 0.469 sec/batch)
2018-03-22 22:45:23.217412: step 80820, loss = 0.74 (253.0 examples/sec; 0.506 sec/batch)
2018-03-22 22:45:27.943353: step 80830, loss = 0.88 (270.8 examples/sec; 0.473 sec/batch)
2018-03-22 22:45:32.538368: step 80840, loss = 0.63 (278.6 examples/sec; 0.460 sec/batch)
2018-03-22 22:45:37.071358: step 80850, loss = 0.75 (282.4 examples/sec; 0.453 sec/batch)
2018-03-22 22:45:42.135435: step 80860, loss = 0.56 (252.8 examples/sec; 0.506 sec/batch)
2018-03-22 22:45:46.842400: step 80870, loss = 0.74 (271.9 examples/sec; 0.471 sec/batch)
2018-03-22 22:45:52.041419: step 80880, loss = 0.76 (246.2 examples/sec; 0.520 sec/batch)
2018-03-22 22:45:57.116383: step 80890, loss = 0.63 (252.2 examples/sec; 0.507 sec/batch)
2018-03-22 22:46:02.643296: step 80900, loss = 0.79 (231.6 examples/sec; 0.553 sec/batch)
2018-03-22 22:46:07.591399: step 80910, loss = 0.62 (258.7 examples/sec; 0.495 sec/batch)
2018-03-22 22:46:12.310667: step 80920, loss = 0.67 (271.2 examples/sec; 0.472 sec/batch)
2018-03-22 22:46:17.006364: step 80930, loss = 0.72 (272.6 examples/sec; 0.470 sec/batch)
2018-03-22 22:46:22.150391: step 80940, loss = 0.70 (248.8 examples/sec; 0.514 sec/batch)
2018-03-22 22:46:26.748715: step 80950, loss = 0.72 (278.4 examples/sec; 0.460 sec/batch)
2018-03-22 22:46:31.997749: step 80960, loss = 0.65 (243.9 examples/sec; 0.525 sec/batch)
2018-03-22 22:46:36.820341: step 80970, loss = 0.65 (265.4 examples/sec; 0.482 sec/batch)
2018-03-22 22:46:42.041364: step 80980, loss = 0.75 (245.2 examples/sec; 0.522 sec/batch)
2018-03-22 22:46:46.580570: step 80990, loss = 0.82 (282.0 examples/sec; 0.454 sec/batch)
2018-03-22 22:46:51.931437: step 81000, loss = 0.66 (239.2 examples/sec; 0.535 sec/batch)
2018-03-22 22:46:56.698377: step 81010, loss = 0.70 (268.5 examples/sec; 0.477 sec/batch)
2018-03-22 22:47:01.644370: step 81020, loss = 0.59 (258.8 examples/sec; 0.495 sec/batch)
2018-03-22 22:47:06.079306: step 81030, loss = 0.75 (288.6 examples/sec; 0.443 sec/batch)
2018-03-22 22:47:11.064406: step 81040, loss = 0.70 (256.8 examples/sec; 0.499 sec/batch)
2018-03-22 22:47:15.722484: step 81050, loss = 0.82 (274.8 examples/sec; 0.466 sec/batch)
2018-03-22 22:47:20.464305: step 81060, loss = 0.67 (269.9 examples/sec; 0.474 sec/batch)
2018-03-22 22:47:25.040384: step 81070, loss = 0.78 (279.7 examples/sec; 0.458 sec/batch)
2018-03-22 22:47:30.479605: step 81080, loss = 0.68 (235.3 examples/sec; 0.544 sec/batch)
2018-03-22 22:47:34.995402: step 81090, loss = 0.59 (283.4 examples/sec; 0.452 sec/batch)
2018-03-22 22:47:40.203132: step 81100, loss = 0.56 (245.8 examples/sec; 0.521 sec/batch)
2018-03-22 22:47:45.052594: step 81110, loss = 0.69 (263.9 examples/sec; 0.485 sec/batch)
2018-03-22 22:47:49.956359: step 81120, loss = 0.65 (261.0 examples/sec; 0.490 sec/batch)
2018-03-22 22:47:54.801343: step 81130, loss = 0.68 (264.2 examples/sec; 0.484 sec/batch)
2018-03-22 22:47:59.174361: step 81140, loss = 0.90 (292.7 examples/sec; 0.437 sec/batch)
2018-03-22 22:48:03.726364: step 81150, loss = 0.72 (281.2 examples/sec; 0.455 sec/batch)
2018-03-22 22:48:08.654396: step 81160, loss = 0.69 (259.7 examples/sec; 0.493 sec/batch)
2018-03-22 22:48:13.256373: step 81170, loss = 0.60 (278.1 examples/sec; 0.460 sec/batch)
2018-03-22 22:48:18.038344: step 81180, loss = 0.69 (267.7 examples/sec; 0.478 sec/batch)
2018-03-22 22:48:22.713172: step 81190, loss = 0.67 (273.8 examples/sec; 0.467 sec/batch)
2018-03-22 22:48:27.721503: step 81200, loss = 0.71 (255.6 examples/sec; 0.501 sec/batch)
2018-03-22 22:48:32.866036: step 81210, loss = 0.72 (248.8 examples/sec; 0.514 sec/batch)
2018-03-22 22:48:37.484382: step 81220, loss = 0.72 (277.2 examples/sec; 0.462 sec/batch)
2018-03-22 22:48:42.495368: step 81230, loss = 0.56 (255.4 examples/sec; 0.501 sec/batch)
2018-03-22 22:48:46.916341: step 81240, loss = 0.66 (289.5 examples/sec; 0.442 sec/batch)
2018-03-22 22:48:51.244375: step 81250, loss = 0.63 (295.7 examples/sec; 0.433 sec/batch)
2018-03-22 22:48:55.823383: step 81260, loss = 0.67 (279.5 examples/sec; 0.458 sec/batch)
2018-03-22 22:49:01.127652: step 81270, loss = 0.74 (241.3 examples/sec; 0.530 sec/batch)
2018-03-22 22:49:06.150016: step 81280, loss = 0.71 (254.9 examples/sec; 0.502 sec/batch)
2018-03-22 22:49:11.032369: step 81290, loss = 0.68 (262.2 examples/sec; 0.488 sec/batch)
2018-03-22 22:49:16.078306: step 81300, loss = 0.67 (253.7 examples/sec; 0.505 sec/batch)
2018-03-22 22:49:21.081374: step 81310, loss = 0.66 (255.8 examples/sec; 0.500 sec/batch)
2018-03-22 22:49:25.534182: step 81320, loss = 0.73 (287.5 examples/sec; 0.445 sec/batch)
2018-03-22 22:49:30.064558: step 81330, loss = 0.77 (282.5 examples/sec; 0.453 sec/batch)
2018-03-22 22:49:34.548469: step 81340, loss = 0.76 (285.5 examples/sec; 0.448 sec/batch)
2018-03-22 22:49:39.211254: step 81350, loss = 0.60 (274.5 examples/sec; 0.466 sec/batch)
2018-03-22 22:49:44.317619: step 81360, loss = 0.83 (250.7 examples/sec; 0.511 sec/batch)
2018-03-22 22:49:48.597760: step 81370, loss = 0.70 (299.1 examples/sec; 0.428 sec/batch)
2018-03-22 22:49:53.735345: step 81380, loss = 0.67 (249.1 examples/sec; 0.514 sec/batch)
2018-03-22 22:49:58.367698: step 81390, loss = 0.82 (276.3 examples/sec; 0.463 sec/batch)
2018-03-22 22:50:03.662010: step 81400, loss = 0.60 (241.8 examples/sec; 0.529 sec/batch)
2018-03-22 22:50:08.232301: step 81410, loss = 0.85 (280.1 examples/sec; 0.457 sec/batch)
2018-03-22 22:50:13.311430: step 81420, loss = 0.84 (252.0 examples/sec; 0.508 sec/batch)
2018-03-22 22:50:18.228405: step 81430, loss = 0.65 (260.3 examples/sec; 0.492 sec/batch)
2018-03-22 22:50:23.146402: step 81440, loss = 0.63 (260.3 examples/sec; 0.492 sec/batch)
2018-03-22 22:50:27.976418: step 81450, loss = 0.72 (265.0 examples/sec; 0.483 sec/batch)
2018-03-22 22:50:33.071519: step 81460, loss = 0.67 (251.2 examples/sec; 0.510 sec/batch)
2018-03-22 22:50:37.710981: step 81470, loss = 0.57 (275.9 examples/sec; 0.464 sec/batch)
2018-03-22 22:50:42.982456: step 81480, loss = 0.62 (242.8 examples/sec; 0.527 sec/batch)
2018-03-22 22:50:47.749362: step 81490, loss = 0.74 (268.5 examples/sec; 0.477 sec/batch)
2018-03-22 22:50:53.270447: step 81500, loss = 0.73 (231.8 examples/sec; 0.552 sec/batch)
2018-03-22 22:50:58.170474: step 81510, loss = 0.62 (261.2 examples/sec; 0.490 sec/batch)
2018-03-22 22:51:03.067304: step 81520, loss = 0.74 (261.4 examples/sec; 0.490 sec/batch)
2018-03-22 22:51:07.445396: step 81530, loss = 0.66 (292.4 examples/sec; 0.438 sec/batch)
2018-03-22 22:51:12.453326: step 81540, loss = 0.73 (255.6 examples/sec; 0.501 sec/batch)
2018-03-22 22:51:17.060380: step 81550, loss = 0.75 (277.8 examples/sec; 0.461 sec/batch)
2018-03-22 22:51:21.768530: step 81560, loss = 0.83 (271.9 examples/sec; 0.471 sec/batch)
2018-03-22 22:51:26.204022: step 81570, loss = 0.70 (288.6 examples/sec; 0.444 sec/batch)
2018-03-22 22:51:31.222493: step 81580, loss = 0.64 (255.1 examples/sec; 0.502 sec/batch)
2018-03-22 22:51:35.691757: step 81590, loss = 0.66 (286.4 examples/sec; 0.447 sec/batch)
2018-03-22 22:51:40.678203: step 81600, loss = 0.72 (256.7 examples/sec; 0.499 sec/batch)
2018-03-22 22:51:45.209366: step 81610, loss = 0.66 (282.5 examples/sec; 0.453 sec/batch)
2018-03-22 22:51:50.199792: step 81620, loss = 0.69 (256.5 examples/sec; 0.499 sec/batch)
2018-03-22 22:51:54.797760: step 81630, loss = 0.59 (278.4 examples/sec; 0.460 sec/batch)
2018-03-22 22:51:59.636275: step 81640, loss = 0.75 (264.5 examples/sec; 0.484 sec/batch)
2018-03-22 22:52:04.336368: step 81650, loss = 0.70 (272.3 examples/sec; 0.470 sec/batch)
2018-03-22 22:52:09.018319: step 81660, loss = 0.65 (273.4 examples/sec; 0.468 sec/batch)
2018-03-22 22:52:14.395664: step 81670, loss = 0.70 (238.0 examples/sec; 0.538 sec/batch)
2018-03-22 22:52:19.170458: step 81680, loss = 0.78 (268.1 examples/sec; 0.477 sec/batch)
2018-03-22 22:52:23.938201: step 81690, loss = 0.62 (268.5 examples/sec; 0.477 sec/batch)
2018-03-22 22:52:28.687828: step 81700, loss = 0.65 (269.5 examples/sec; 0.475 sec/batch)
2018-03-22 22:52:33.442362: step 81710, loss = 0.72 (269.2 examples/sec; 0.475 sec/batch)
2018-03-22 22:52:37.718130: step 81720, loss = 0.59 (299.4 examples/sec; 0.428 sec/batch)
2018-03-22 22:52:42.456765: step 81730, loss = 0.74 (270.1 examples/sec; 0.474 sec/batch)
2018-03-22 22:52:46.931362: step 81740, loss = 0.67 (286.1 examples/sec; 0.447 sec/batch)
2018-03-22 22:52:51.484122: step 81750, loss = 0.73 (281.1 examples/sec; 0.455 sec/batch)
2018-03-22 22:52:56.054385: step 81760, loss = 0.77 (280.1 examples/sec; 0.457 sec/batch)
2018-03-22 22:53:01.196413: step 81770, loss = 0.78 (248.9 examples/sec; 0.514 sec/batch)
2018-03-22 22:53:06.062833: step 81780, loss = 0.68 (263.0 examples/sec; 0.487 sec/batch)
2018-03-22 22:53:10.942674: step 81790, loss = 0.69 (262.3 examples/sec; 0.488 sec/batch)
2018-03-22 22:53:15.598445: step 81800, loss = 0.91 (274.9 examples/sec; 0.466 sec/batch)
2018-03-22 22:53:20.229405: step 81810, loss = 0.62 (276.4 examples/sec; 0.463 sec/batch)
2018-03-22 22:53:24.707384: step 81820, loss = 0.70 (285.8 examples/sec; 0.448 sec/batch)
2018-03-22 22:53:29.124374: step 81830, loss = 0.80 (289.8 examples/sec; 0.442 sec/batch)
2018-03-22 22:53:34.076392: step 81840, loss = 0.83 (258.5 examples/sec; 0.495 sec/batch)
2018-03-22 22:53:39.186418: step 81850, loss = 0.66 (250.5 examples/sec; 0.511 sec/batch)
2018-03-22 22:53:44.301422: step 81860, loss = 0.73 (250.2 examples/sec; 0.512 sec/batch)
2018-03-22 22:53:49.363460: step 81870, loss = 0.69 (252.9 examples/sec; 0.506 sec/batch)
2018-03-22 22:53:54.423483: step 81880, loss = 0.73 (253.0 examples/sec; 0.506 sec/batch)
2018-03-22 22:53:58.890478: step 81890, loss = 0.67 (286.5 examples/sec; 0.447 sec/batch)
2018-03-22 22:54:04.102777: step 81900, loss = 0.69 (245.6 examples/sec; 0.521 sec/batch)
2018-03-22 22:54:08.896414: step 81910, loss = 0.82 (267.0 examples/sec; 0.479 sec/batch)
2018-03-22 22:54:13.711323: step 81920, loss = 0.72 (265.8 examples/sec; 0.481 sec/batch)
2018-03-22 22:54:18.539288: step 81930, loss = 0.77 (265.1 examples/sec; 0.483 sec/batch)
2018-03-22 22:54:23.253323: step 81940, loss = 0.76 (271.5 examples/sec; 0.471 sec/batch)
2018-03-22 22:54:27.505866: step 81950, loss = 0.67 (301.0 examples/sec; 0.425 sec/batch)
2018-03-22 22:54:32.447530: step 81960, loss = 0.63 (259.0 examples/sec; 0.494 sec/batch)
2018-03-22 22:54:36.985786: step 81970, loss = 0.79 (282.0 examples/sec; 0.454 sec/batch)
2018-03-22 22:54:42.180308: step 81980, loss = 0.60 (246.4 examples/sec; 0.519 sec/batch)
2018-03-22 22:54:47.144226: step 81990, loss = 0.78 (257.9 examples/sec; 0.496 sec/batch)
2018-03-22 22:54:52.283222: step 82000, loss = 0.73 (249.1 examples/sec; 0.514 sec/batch)
2018-03-22 22:54:57.089371: step 82010, loss = 0.72 (266.3 examples/sec; 0.481 sec/batch)
2018-03-22 22:55:01.643430: step 82020, loss = 0.74 (281.1 examples/sec; 0.455 sec/batch)
2018-03-22 22:55:06.841408: step 82030, loss = 0.63 (246.2 examples/sec; 0.520 sec/batch)
2018-03-22 22:55:12.154420: step 82040, loss = 0.71 (240.9 examples/sec; 0.531 sec/batch)
2018-03-22 22:55:16.643308: step 82050, loss = 0.72 (285.1 examples/sec; 0.449 sec/batch)
2018-03-22 22:55:21.648444: step 82060, loss = 0.71 (255.7 examples/sec; 0.501 sec/batch)
2018-03-22 22:55:26.359005: step 82070, loss = 0.81 (271.7 examples/sec; 0.471 sec/batch)
2018-03-22 22:55:31.526347: step 82080, loss = 0.78 (247.7 examples/sec; 0.517 sec/batch)
2018-03-22 22:55:36.103312: step 82090, loss = 0.63 (279.7 examples/sec; 0.458 sec/batch)
2018-03-22 22:55:41.210882: step 82100, loss = 0.58 (250.6 examples/sec; 0.511 sec/batch)
2018-03-22 22:55:45.591905: step 82110, loss = 0.77 (292.2 examples/sec; 0.438 sec/batch)
2018-03-22 22:55:50.477381: step 82120, loss = 0.70 (262.0 examples/sec; 0.489 sec/batch)
2018-03-22 22:55:54.977315: step 82130, loss = 0.65 (284.4 examples/sec; 0.450 sec/batch)
2018-03-22 22:55:59.812427: step 82140, loss = 0.79 (264.7 examples/sec; 0.484 sec/batch)
2018-03-22 22:56:04.588395: step 82150, loss = 0.85 (268.0 examples/sec; 0.478 sec/batch)
2018-03-22 22:56:09.179243: step 82160, loss = 0.72 (278.8 examples/sec; 0.459 sec/batch)
2018-03-22 22:56:14.068579: step 82170, loss = 0.79 (261.8 examples/sec; 0.489 sec/batch)
2018-03-22 22:56:18.501321: step 82180, loss = 0.71 (288.8 examples/sec; 0.443 sec/batch)
2018-03-22 22:56:23.715143: step 82190, loss = 0.67 (245.5 examples/sec; 0.521 sec/batch)
2018-03-22 22:56:28.712666: step 82200, loss = 0.82 (256.1 examples/sec; 0.500 sec/batch)
2018-03-22 22:56:33.323359: step 82210, loss = 0.61 (277.6 examples/sec; 0.461 sec/batch)
2018-03-22 22:56:37.419061: step 82220, loss = 0.70 (312.5 examples/sec; 0.410 sec/batch)
2018-03-22 22:56:42.660277: step 82230, loss = 0.81 (244.2 examples/sec; 0.524 sec/batch)
2018-03-22 22:56:47.916564: step 82240, loss = 0.69 (243.5 examples/sec; 0.526 sec/batch)
2018-03-22 22:56:52.897391: step 82250, loss = 0.85 (257.0 examples/sec; 0.498 sec/batch)
2018-03-22 22:56:57.848318: step 82260, loss = 0.99 (258.5 examples/sec; 0.495 sec/batch)
2018-03-22 22:57:02.988328: step 82270, loss = 0.74 (249.0 examples/sec; 0.514 sec/batch)
2018-03-22 22:57:07.113462: step 82280, loss = 0.77 (310.3 examples/sec; 0.413 sec/batch)
2018-03-22 22:57:11.860515: step 82290, loss = 0.69 (269.6 examples/sec; 0.475 sec/batch)
2018-03-22 22:57:16.199783: step 82300, loss = 0.90 (295.0 examples/sec; 0.434 sec/batch)
2018-03-22 22:57:21.108145: step 82310, loss = 0.55 (260.8 examples/sec; 0.491 sec/batch)
2018-03-22 22:57:25.826257: step 82320, loss = 0.60 (271.3 examples/sec; 0.472 sec/batch)
2018-03-22 22:57:30.681342: step 82330, loss = 0.69 (263.6 examples/sec; 0.486 sec/batch)
2018-03-22 22:57:35.317415: step 82340, loss = 0.84 (276.1 examples/sec; 0.464 sec/batch)
2018-03-22 22:57:40.401401: step 82350, loss = 0.77 (251.8 examples/sec; 0.508 sec/batch)
2018-03-22 22:57:45.417649: step 82360, loss = 0.63 (255.2 examples/sec; 0.502 sec/batch)
2018-03-22 22:57:50.759391: step 82370, loss = 0.64 (239.6 examples/sec; 0.534 sec/batch)
2018-03-22 22:57:55.173349: step 82380, loss = 0.85 (290.0 examples/sec; 0.441 sec/batch)
2018-03-22 22:58:00.463357: step 82390, loss = 0.80 (242.0 examples/sec; 0.529 sec/batch)
2018-03-22 22:58:05.395129: step 82400, loss = 0.60 (259.5 examples/sec; 0.493 sec/batch)
2018-03-22 22:58:10.604325: step 82410, loss = 0.65 (245.7 examples/sec; 0.521 sec/batch)
2018-03-22 22:58:15.314035: step 82420, loss = 0.76 (271.8 examples/sec; 0.471 sec/batch)
2018-03-22 22:58:20.607891: step 82430, loss = 0.74 (241.8 examples/sec; 0.529 sec/batch)
2018-03-22 22:58:25.538125: step 82440, loss = 0.74 (259.6 examples/sec; 0.493 sec/batch)
2018-03-22 22:58:30.551396: step 82450, loss = 0.78 (255.3 examples/sec; 0.501 sec/batch)
2018-03-22 22:58:35.077381: step 82460, loss = 0.69 (282.8 examples/sec; 0.453 sec/batch)
2018-03-22 22:58:39.970610: step 82470, loss = 0.80 (261.6 examples/sec; 0.489 sec/batch)
2018-03-22 22:58:44.418969: step 82480, loss = 0.67 (287.7 examples/sec; 0.445 sec/batch)
2018-03-22 22:58:49.168537: step 82490, loss = 0.69 (269.5 examples/sec; 0.475 sec/batch)
2018-03-22 22:58:54.755580: step 82500, loss = 0.78 (229.1 examples/sec; 0.559 sec/batch)
2018-03-22 22:58:59.792376: step 82510, loss = 0.63 (254.1 examples/sec; 0.504 sec/batch)
2018-03-22 22:59:04.918383: step 82520, loss = 0.71 (249.7 examples/sec; 0.513 sec/batch)
2018-03-22 22:59:09.870652: step 82530, loss = 0.71 (258.5 examples/sec; 0.495 sec/batch)
2018-03-22 22:59:14.790391: step 82540, loss = 0.72 (260.2 examples/sec; 0.492 sec/batch)
2018-03-22 22:59:19.690882: step 82550, loss = 0.61 (261.2 examples/sec; 0.490 sec/batch)
2018-03-22 22:59:24.536232: step 82560, loss = 0.84 (264.2 examples/sec; 0.485 sec/batch)
2018-03-22 22:59:29.331402: step 82570, loss = 0.87 (266.9 examples/sec; 0.480 sec/batch)
2018-03-22 22:59:34.295080: step 82580, loss = 0.80 (257.9 examples/sec; 0.496 sec/batch)
2018-03-22 22:59:39.325371: step 82590, loss = 0.75 (254.5 examples/sec; 0.503 sec/batch)
2018-03-22 22:59:44.329319: step 82600, loss = 0.70 (255.8 examples/sec; 0.500 sec/batch)
2018-03-22 22:59:49.432421: step 82610, loss = 0.70 (250.8 examples/sec; 0.510 sec/batch)
2018-03-22 22:59:54.173437: step 82620, loss = 0.79 (270.0 examples/sec; 0.474 sec/batch)
2018-03-22 22:59:59.079528: step 82630, loss = 0.69 (260.9 examples/sec; 0.491 sec/batch)
2018-03-22 23:00:03.409491: step 82640, loss = 0.84 (295.6 examples/sec; 0.433 sec/batch)
2018-03-22 23:00:07.711041: step 82650, loss = 0.87 (297.6 examples/sec; 0.430 sec/batch)
2018-03-22 23:00:12.807231: step 82660, loss = 0.64 (251.2 examples/sec; 0.510 sec/batch)
2018-03-22 23:00:17.203438: step 82670, loss = 0.64 (291.2 examples/sec; 0.440 sec/batch)
2018-03-22 23:00:22.248976: step 82680, loss = 0.75 (253.7 examples/sec; 0.505 sec/batch)
2018-03-22 23:00:26.407399: step 82690, loss = 0.70 (307.8 examples/sec; 0.416 sec/batch)
2018-03-22 23:00:30.833230: step 82700, loss = 0.79 (289.2 examples/sec; 0.443 sec/batch)
2018-03-22 23:00:35.421746: step 82710, loss = 0.64 (279.0 examples/sec; 0.459 sec/batch)
2018-03-22 23:00:40.142497: step 82720, loss = 0.75 (271.1 examples/sec; 0.472 sec/batch)
2018-03-22 23:00:44.989413: step 82730, loss = 0.73 (264.1 examples/sec; 0.485 sec/batch)
2018-03-22 23:00:49.827397: step 82740, loss = 0.78 (264.6 examples/sec; 0.484 sec/batch)
2018-03-22 23:00:54.501941: step 82750, loss = 0.79 (273.8 examples/sec; 0.467 sec/batch)
2018-03-22 23:00:58.919770: step 82760, loss = 0.67 (289.7 examples/sec; 0.442 sec/batch)
2018-03-22 23:01:03.539410: step 82770, loss = 0.72 (277.1 examples/sec; 0.462 sec/batch)
2018-03-22 23:01:07.871368: step 82780, loss = 0.68 (295.5 examples/sec; 0.433 sec/batch)
2018-03-22 23:01:12.759080: step 82790, loss = 0.69 (261.9 examples/sec; 0.489 sec/batch)
2018-03-22 23:01:17.167588: step 82800, loss = 0.61 (290.3 examples/sec; 0.441 sec/batch)
2018-03-22 23:01:22.236626: step 82810, loss = 0.74 (252.5 examples/sec; 0.507 sec/batch)
2018-03-22 23:01:27.080375: step 82820, loss = 0.67 (264.3 examples/sec; 0.484 sec/batch)
2018-03-22 23:01:32.240362: step 82830, loss = 0.58 (248.1 examples/sec; 0.516 sec/batch)
2018-03-22 23:01:36.962445: step 82840, loss = 0.71 (271.1 examples/sec; 0.472 sec/batch)
2018-03-22 23:01:41.991547: step 82850, loss = 0.70 (254.5 examples/sec; 0.503 sec/batch)
2018-03-22 23:01:46.855326: step 82860, loss = 0.66 (263.2 examples/sec; 0.486 sec/batch)
2018-03-22 23:01:51.861391: step 82870, loss = 0.69 (255.7 examples/sec; 0.501 sec/batch)
2018-03-22 23:01:56.604431: step 82880, loss = 0.72 (269.9 examples/sec; 0.474 sec/batch)
2018-03-22 23:02:01.838428: step 82890, loss = 0.62 (244.6 examples/sec; 0.523 sec/batch)
2018-03-22 23:02:06.640812: step 82900, loss = 0.67 (266.5 examples/sec; 0.480 sec/batch)
2018-03-22 23:02:11.639717: step 82910, loss = 0.74 (256.1 examples/sec; 0.500 sec/batch)
2018-03-22 23:02:16.401528: step 82920, loss = 0.78 (268.8 examples/sec; 0.476 sec/batch)
2018-03-22 23:02:22.127101: step 82930, loss = 0.63 (223.6 examples/sec; 0.573 sec/batch)
2018-03-22 23:02:27.014347: step 82940, loss = 0.69 (261.9 examples/sec; 0.489 sec/batch)
2018-03-22 23:02:32.286122: step 82950, loss = 0.70 (242.8 examples/sec; 0.527 sec/batch)
2018-03-22 23:02:36.712461: step 82960, loss = 0.76 (289.2 examples/sec; 0.443 sec/batch)
2018-03-22 23:02:41.433629: step 82970, loss = 0.67 (271.1 examples/sec; 0.472 sec/batch)
2018-03-22 23:02:46.217393: step 82980, loss = 0.70 (267.6 examples/sec; 0.478 sec/batch)
2018-03-22 23:02:51.314331: step 82990, loss = 0.72 (251.1 examples/sec; 0.510 sec/batch)
2018-03-22 23:02:56.371300: step 83000, loss = 0.71 (253.1 examples/sec; 0.506 sec/batch)
2018-03-22 23:03:01.021352: step 83010, loss = 0.68 (275.3 examples/sec; 0.465 sec/batch)
2018-03-22 23:03:05.912142: step 83020, loss = 0.74 (261.7 examples/sec; 0.489 sec/batch)
2018-03-22 23:03:10.293240: step 83030, loss = 0.55 (292.2 examples/sec; 0.438 sec/batch)
2018-03-22 23:03:14.780332: step 83040, loss = 0.69 (285.3 examples/sec; 0.449 sec/batch)
2018-03-22 23:03:19.368312: step 83050, loss = 0.74 (279.0 examples/sec; 0.459 sec/batch)
2018-03-22 23:03:24.313176: step 83060, loss = 0.64 (258.9 examples/sec; 0.494 sec/batch)
2018-03-22 23:03:29.043318: step 83070, loss = 0.67 (270.6 examples/sec; 0.473 sec/batch)
2018-03-22 23:03:33.908490: step 83080, loss = 0.81 (263.1 examples/sec; 0.487 sec/batch)
2018-03-22 23:03:38.798926: step 83090, loss = 0.79 (261.7 examples/sec; 0.489 sec/batch)
2018-03-22 23:03:44.495033: step 83100, loss = 0.67 (224.7 examples/sec; 0.570 sec/batch)
2018-03-22 23:03:49.268507: step 83110, loss = 0.72 (268.2 examples/sec; 0.477 sec/batch)
2018-03-22 23:03:53.885652: step 83120, loss = 0.65 (277.2 examples/sec; 0.462 sec/batch)
2018-03-22 23:03:58.356357: step 83130, loss = 0.66 (286.3 examples/sec; 0.447 sec/batch)
2018-03-22 23:04:03.715006: step 83140, loss = 0.66 (238.9 examples/sec; 0.536 sec/batch)
2018-03-22 23:04:08.334365: step 83150, loss = 0.63 (277.1 examples/sec; 0.462 sec/batch)
2018-03-22 23:04:12.945205: step 83160, loss = 0.60 (277.6 examples/sec; 0.461 sec/batch)
2018-03-22 23:04:17.276461: step 83170, loss = 0.65 (295.5 examples/sec; 0.433 sec/batch)
2018-03-22 23:04:21.773565: step 83180, loss = 0.69 (284.6 examples/sec; 0.450 sec/batch)
2018-03-22 23:04:26.040506: step 83190, loss = 0.63 (300.0 examples/sec; 0.427 sec/batch)
2018-03-22 23:04:31.428218: step 83200, loss = 0.73 (237.6 examples/sec; 0.539 sec/batch)
2018-03-22 23:04:36.304351: step 83210, loss = 0.78 (262.5 examples/sec; 0.488 sec/batch)
2018-03-22 23:04:41.359214: step 83220, loss = 0.71 (253.2 examples/sec; 0.505 sec/batch)
2018-03-22 23:04:46.009372: step 83230, loss = 0.67 (275.3 examples/sec; 0.465 sec/batch)
2018-03-22 23:04:50.713083: step 83240, loss = 0.79 (272.1 examples/sec; 0.470 sec/batch)
2018-03-22 23:04:54.626052: step 83250, loss = 0.69 (327.1 examples/sec; 0.391 sec/batch)
2018-03-22 23:04:59.288457: step 83260, loss = 0.75 (274.5 examples/sec; 0.466 sec/batch)
2018-03-22 23:05:03.995047: step 83270, loss = 0.71 (272.0 examples/sec; 0.471 sec/batch)
2018-03-22 23:05:08.046736: step 83280, loss = 0.63 (315.9 examples/sec; 0.405 sec/batch)
2018-03-22 23:05:12.925435: step 83290, loss = 0.78 (262.4 examples/sec; 0.488 sec/batch)
2018-03-22 23:05:18.145856: step 83300, loss = 0.69 (245.2 examples/sec; 0.522 sec/batch)
2018-03-22 23:05:23.739565: step 83310, loss = 0.64 (228.8 examples/sec; 0.559 sec/batch)
2018-03-22 23:05:28.545324: step 83320, loss = 0.84 (266.3 examples/sec; 0.481 sec/batch)
2018-03-22 23:05:33.919842: step 83330, loss = 0.71 (238.2 examples/sec; 0.537 sec/batch)
2018-03-22 23:05:38.486389: step 83340, loss = 0.77 (280.3 examples/sec; 0.457 sec/batch)
2018-03-22 23:05:43.512105: step 83350, loss = 0.55 (254.7 examples/sec; 0.503 sec/batch)
2018-03-22 23:05:48.259559: step 83360, loss = 0.62 (269.6 examples/sec; 0.475 sec/batch)
2018-03-22 23:05:53.385340: step 83370, loss = 0.79 (249.7 examples/sec; 0.513 sec/batch)
2018-03-22 23:05:58.284381: step 83380, loss = 0.76 (261.3 examples/sec; 0.490 sec/batch)
2018-03-22 23:06:03.188173: step 83390, loss = 0.58 (261.0 examples/sec; 0.490 sec/batch)
2018-03-22 23:06:07.840081: step 83400, loss = 0.74 (275.2 examples/sec; 0.465 sec/batch)
2018-03-22 23:06:12.795436: step 83410, loss = 0.71 (258.3 examples/sec; 0.496 sec/batch)
2018-03-22 23:06:17.336355: step 83420, loss = 0.70 (281.9 examples/sec; 0.454 sec/batch)
2018-03-22 23:06:22.553417: step 83430, loss = 0.86 (245.3 examples/sec; 0.522 sec/batch)
2018-03-22 23:06:27.435315: step 83440, loss = 0.69 (262.2 examples/sec; 0.488 sec/batch)
2018-03-22 23:06:32.068478: step 83450, loss = 0.77 (276.3 examples/sec; 0.463 sec/batch)
2018-03-22 23:06:36.587310: step 83460, loss = 0.80 (283.3 examples/sec; 0.452 sec/batch)
2018-03-22 23:06:41.644429: step 83470, loss = 0.85 (253.1 examples/sec; 0.506 sec/batch)
2018-03-22 23:06:46.242365: step 83480, loss = 0.64 (278.4 examples/sec; 0.460 sec/batch)
2018-03-22 23:06:51.071018: step 83490, loss = 0.64 (265.1 examples/sec; 0.483 sec/batch)
2018-03-22 23:06:55.573248: step 83500, loss = 0.58 (284.3 examples/sec; 0.450 sec/batch)
2018-03-22 23:07:00.558769: step 83510, loss = 0.74 (256.7 examples/sec; 0.499 sec/batch)
2018-03-22 23:07:05.014448: step 83520, loss = 0.71 (287.3 examples/sec; 0.446 sec/batch)
2018-03-22 23:07:09.693501: step 83530, loss = 0.74 (273.6 examples/sec; 0.468 sec/batch)
2018-03-22 23:07:14.793415: step 83540, loss = 0.75 (251.0 examples/sec; 0.510 sec/batch)
2018-03-22 23:07:19.627847: step 83550, loss = 0.78 (264.8 examples/sec; 0.483 sec/batch)
2018-03-22 23:07:24.546275: step 83560, loss = 0.57 (260.2 examples/sec; 0.492 sec/batch)
2018-03-22 23:07:29.424348: step 83570, loss = 0.79 (262.4 examples/sec; 0.488 sec/batch)
2018-03-22 23:07:34.373344: step 83580, loss = 0.90 (258.6 examples/sec; 0.495 sec/batch)
2018-03-22 23:07:39.257170: step 83590, loss = 0.65 (262.1 examples/sec; 0.488 sec/batch)
2018-03-22 23:07:44.663045: step 83600, loss = 0.58 (236.8 examples/sec; 0.541 sec/batch)
2018-03-22 23:07:49.280759: step 83610, loss = 0.66 (277.2 examples/sec; 0.462 sec/batch)
2018-03-22 23:07:53.955370: step 83620, loss = 0.74 (273.8 examples/sec; 0.467 sec/batch)
2018-03-22 23:07:58.352344: step 83630, loss = 0.66 (291.1 examples/sec; 0.440 sec/batch)
2018-03-22 23:08:03.322350: step 83640, loss = 0.63 (257.5 examples/sec; 0.497 sec/batch)
2018-03-22 23:08:07.853260: step 83650, loss = 0.64 (282.5 examples/sec; 0.453 sec/batch)
2018-03-22 23:08:12.547407: step 83660, loss = 0.85 (272.7 examples/sec; 0.469 sec/batch)
2018-03-22 23:08:17.135356: step 83670, loss = 0.83 (279.0 examples/sec; 0.459 sec/batch)
2018-03-22 23:08:21.845360: step 83680, loss = 0.74 (271.8 examples/sec; 0.471 sec/batch)
2018-03-22 23:08:26.688400: step 83690, loss = 0.81 (264.3 examples/sec; 0.484 sec/batch)
2018-03-22 23:08:32.186234: step 83700, loss = 0.83 (232.8 examples/sec; 0.550 sec/batch)
2018-03-22 23:08:37.005418: step 83710, loss = 0.67 (265.6 examples/sec; 0.482 sec/batch)
2018-03-22 23:08:41.973335: step 83720, loss = 0.85 (257.7 examples/sec; 0.497 sec/batch)
2018-03-22 23:08:46.435427: step 83730, loss = 0.69 (286.9 examples/sec; 0.446 sec/batch)
2018-03-22 23:08:50.965328: step 83740, loss = 0.70 (282.6 examples/sec; 0.453 sec/batch)
2018-03-22 23:08:55.367111: step 83750, loss = 0.98 (290.8 examples/sec; 0.440 sec/batch)
2018-03-22 23:08:59.734083: step 83760, loss = 0.88 (293.1 examples/sec; 0.437 sec/batch)
2018-03-22 23:09:04.111752: step 83770, loss = 0.62 (292.4 examples/sec; 0.438 sec/batch)
2018-03-22 23:09:08.964414: step 83780, loss = 0.73 (263.8 examples/sec; 0.485 sec/batch)
2018-03-22 23:09:13.780964: step 83790, loss = 0.74 (265.8 examples/sec; 0.482 sec/batch)
2018-03-22 23:09:18.476330: step 83800, loss = 0.87 (272.6 examples/sec; 0.470 sec/batch)
2018-03-22 23:09:23.446580: step 83810, loss = 0.85 (257.5 examples/sec; 0.497 sec/batch)
2018-03-22 23:09:28.335380: step 83820, loss = 0.68 (261.8 examples/sec; 0.489 sec/batch)
2018-03-22 23:09:33.701360: step 83830, loss = 0.76 (238.5 examples/sec; 0.537 sec/batch)
2018-03-22 23:09:38.712382: step 83840, loss = 0.69 (255.4 examples/sec; 0.501 sec/batch)
2018-03-22 23:09:43.652390: step 83850, loss = 0.66 (259.1 examples/sec; 0.494 sec/batch)
2018-03-22 23:09:48.210379: step 83860, loss = 0.79 (280.8 examples/sec; 0.456 sec/batch)
2018-03-22 23:09:53.130422: step 83870, loss = 0.71 (260.2 examples/sec; 0.492 sec/batch)
2018-03-22 23:09:57.700240: step 83880, loss = 0.59 (280.1 examples/sec; 0.457 sec/batch)
2018-03-22 23:10:02.966721: step 83890, loss = 0.84 (243.0 examples/sec; 0.527 sec/batch)
2018-03-22 23:10:07.916312: step 83900, loss = 0.86 (258.6 examples/sec; 0.495 sec/batch)
2018-03-22 23:10:12.830341: step 83910, loss = 0.60 (260.5 examples/sec; 0.491 sec/batch)
2018-03-22 23:10:17.443852: step 83920, loss = 0.67 (277.4 examples/sec; 0.461 sec/batch)
2018-03-22 23:10:22.148360: step 83930, loss = 0.68 (272.1 examples/sec; 0.470 sec/batch)
2018-03-22 23:10:26.547470: step 83940, loss = 0.75 (291.0 examples/sec; 0.440 sec/batch)
2018-03-22 23:10:31.728403: step 83950, loss = 0.65 (247.1 examples/sec; 0.518 sec/batch)
2018-03-22 23:10:36.114577: step 83960, loss = 0.64 (291.8 examples/sec; 0.439 sec/batch)
2018-03-22 23:10:41.143354: step 83970, loss = 0.79 (254.5 examples/sec; 0.503 sec/batch)
2018-03-22 23:10:45.629294: step 83980, loss = 0.68 (285.3 examples/sec; 0.449 sec/batch)
2018-03-22 23:10:50.567125: step 83990, loss = 0.77 (259.2 examples/sec; 0.494 sec/batch)
2018-03-22 23:10:55.286684: step 84000, loss = 0.71 (271.2 examples/sec; 0.472 sec/batch)
2018-03-22 23:11:00.450417: step 84010, loss = 0.70 (247.9 examples/sec; 0.516 sec/batch)
2018-03-22 23:11:05.365334: step 84020, loss = 0.61 (260.4 examples/sec; 0.491 sec/batch)
2018-03-22 23:11:10.197395: step 84030, loss = 0.54 (264.9 examples/sec; 0.483 sec/batch)
2018-03-22 23:11:14.831342: step 84040, loss = 0.76 (276.2 examples/sec; 0.463 sec/batch)
2018-03-22 23:11:19.681325: step 84050, loss = 0.77 (263.9 examples/sec; 0.485 sec/batch)
2018-03-22 23:11:24.355452: step 84060, loss = 0.61 (273.8 examples/sec; 0.467 sec/batch)
2018-03-22 23:11:28.940674: step 84070, loss = 0.71 (279.2 examples/sec; 0.459 sec/batch)
2018-03-22 23:11:34.061383: step 84080, loss = 0.66 (250.0 examples/sec; 0.512 sec/batch)
2018-03-22 23:11:38.619827: step 84090, loss = 0.67 (280.8 examples/sec; 0.456 sec/batch)
2018-03-22 23:11:43.904614: step 84100, loss = 0.88 (242.2 examples/sec; 0.528 sec/batch)
2018-03-22 23:11:48.241785: step 84110, loss = 0.65 (295.1 examples/sec; 0.434 sec/batch)
2018-03-22 23:11:53.320483: step 84120, loss = 0.81 (252.0 examples/sec; 0.508 sec/batch)
2018-03-22 23:11:58.029891: step 84130, loss = 0.77 (271.8 examples/sec; 0.471 sec/batch)
2018-03-22 23:12:03.374622: step 84140, loss = 0.85 (239.5 examples/sec; 0.534 sec/batch)
2018-03-22 23:12:07.754136: step 84150, loss = 0.84 (292.3 examples/sec; 0.438 sec/batch)
2018-03-22 23:12:12.279356: step 84160, loss = 0.75 (282.9 examples/sec; 0.453 sec/batch)
2018-03-22 23:12:16.614043: step 84170, loss = 0.68 (295.3 examples/sec; 0.433 sec/batch)
2018-03-22 23:12:21.684446: step 84180, loss = 0.75 (252.4 examples/sec; 0.507 sec/batch)
2018-03-22 23:12:26.432349: step 84190, loss = 0.65 (269.6 examples/sec; 0.475 sec/batch)
2018-03-22 23:12:31.884586: step 84200, loss = 0.76 (234.8 examples/sec; 0.545 sec/batch)
2018-03-22 23:12:36.157161: step 84210, loss = 0.72 (299.6 examples/sec; 0.427 sec/batch)
2018-03-22 23:12:40.841030: step 84220, loss = 0.73 (273.3 examples/sec; 0.468 sec/batch)
2018-03-22 23:12:44.822341: step 84230, loss = 0.75 (321.5 examples/sec; 0.398 sec/batch)
2018-03-22 23:12:49.196333: step 84240, loss = 0.69 (292.6 examples/sec; 0.437 sec/batch)
2018-03-22 23:12:53.216410: step 84250, loss = 0.63 (318.4 examples/sec; 0.402 sec/batch)
2018-03-22 23:12:57.540286: step 84260, loss = 0.76 (296.0 examples/sec; 0.432 sec/batch)
2018-03-22 23:13:02.801393: step 84270, loss = 0.79 (243.3 examples/sec; 0.526 sec/batch)
2018-03-22 23:13:07.775425: step 84280, loss = 0.82 (257.3 examples/sec; 0.497 sec/batch)
2018-03-22 23:13:13.072397: step 84290, loss = 0.63 (241.6 examples/sec; 0.530 sec/batch)
2018-03-22 23:13:18.095871: step 84300, loss = 0.63 (254.8 examples/sec; 0.502 sec/batch)
2018-03-22 23:13:22.895797: step 84310, loss = 0.82 (266.7 examples/sec; 0.480 sec/batch)
2018-03-22 23:13:27.262293: step 84320, loss = 0.63 (293.1 examples/sec; 0.437 sec/batch)
2018-03-22 23:13:32.294393: step 84330, loss = 0.74 (254.4 examples/sec; 0.503 sec/batch)
2018-03-22 23:13:36.906375: step 84340, loss = 0.70 (277.5 examples/sec; 0.461 sec/batch)
2018-03-22 23:13:42.039401: step 84350, loss = 0.76 (249.4 examples/sec; 0.513 sec/batch)
2018-03-22 23:13:46.803350: step 84360, loss = 0.58 (268.7 examples/sec; 0.476 sec/batch)
2018-03-22 23:13:52.129496: step 84370, loss = 0.65 (240.3 examples/sec; 0.533 sec/batch)
2018-03-22 23:13:56.315789: step 84380, loss = 0.71 (305.8 examples/sec; 0.419 sec/batch)
2018-03-22 23:14:01.628089: step 84390, loss = 0.60 (241.0 examples/sec; 0.531 sec/batch)
2018-03-22 23:14:06.824202: step 84400, loss = 0.61 (246.3 examples/sec; 0.520 sec/batch)
2018-03-22 23:14:11.739435: step 84410, loss = 0.71 (260.4 examples/sec; 0.492 sec/batch)
2018-03-22 23:14:16.430440: step 84420, loss = 0.89 (272.9 examples/sec; 0.469 sec/batch)
2018-03-22 23:14:21.379613: step 84430, loss = 0.64 (258.6 examples/sec; 0.495 sec/batch)
2018-03-22 23:14:26.001400: step 84440, loss = 0.68 (276.9 examples/sec; 0.462 sec/batch)
2018-03-22 23:14:31.000395: step 84450, loss = 0.76 (256.1 examples/sec; 0.500 sec/batch)
2018-03-22 23:14:36.110411: step 84460, loss = 0.45 (250.5 examples/sec; 0.511 sec/batch)
2018-03-22 23:14:41.169382: step 84470, loss = 0.60 (253.0 examples/sec; 0.506 sec/batch)
2018-03-22 23:14:45.785825: step 84480, loss = 0.73 (277.3 examples/sec; 0.462 sec/batch)
2018-03-22 23:14:50.803390: step 84490, loss = 0.81 (255.1 examples/sec; 0.502 sec/batch)
2018-03-22 23:14:55.956076: step 84500, loss = 0.79 (248.4 examples/sec; 0.515 sec/batch)
2018-03-22 23:15:01.196419: step 84510, loss = 0.63 (244.3 examples/sec; 0.524 sec/batch)
2018-03-22 23:15:05.715324: step 84520, loss = 0.71 (283.3 examples/sec; 0.452 sec/batch)
2018-03-22 23:15:10.744884: step 84530, loss = 0.87 (254.5 examples/sec; 0.503 sec/batch)
2018-03-22 23:15:15.500423: step 84540, loss = 0.75 (269.2 examples/sec; 0.476 sec/batch)
2018-03-22 23:15:20.323361: step 84550, loss = 0.85 (265.4 examples/sec; 0.482 sec/batch)
2018-03-22 23:15:25.021393: step 84560, loss = 0.76 (272.5 examples/sec; 0.470 sec/batch)
2018-03-22 23:15:29.963375: step 84570, loss = 0.79 (259.0 examples/sec; 0.494 sec/batch)
2018-03-22 23:15:34.579411: step 84580, loss = 0.80 (277.3 examples/sec; 0.462 sec/batch)
2018-03-22 23:15:39.021312: step 84590, loss = 0.72 (288.2 examples/sec; 0.444 sec/batch)
2018-03-22 23:15:44.182829: step 84600, loss = 0.70 (248.0 examples/sec; 0.516 sec/batch)
2018-03-22 23:15:48.542349: step 84610, loss = 0.73 (293.6 examples/sec; 0.436 sec/batch)
2018-03-22 23:15:53.680008: step 84620, loss = 0.70 (249.1 examples/sec; 0.514 sec/batch)
2018-03-22 23:15:57.977386: step 84630, loss = 0.63 (297.9 examples/sec; 0.430 sec/batch)
2018-03-22 23:16:03.381650: step 84640, loss = 0.74 (236.8 examples/sec; 0.540 sec/batch)
2018-03-22 23:16:07.964752: step 84650, loss = 0.62 (279.3 examples/sec; 0.458 sec/batch)
2018-03-22 23:16:12.836650: step 84660, loss = 0.76 (262.7 examples/sec; 0.487 sec/batch)
2018-03-22 23:16:17.290431: step 84670, loss = 0.70 (287.4 examples/sec; 0.445 sec/batch)
2018-03-22 23:16:22.467378: step 84680, loss = 0.61 (247.2 examples/sec; 0.518 sec/batch)
2018-03-22 23:16:27.230402: step 84690, loss = 0.59 (268.7 examples/sec; 0.476 sec/batch)
2018-03-22 23:16:32.399530: step 84700, loss = 0.78 (247.6 examples/sec; 0.517 sec/batch)
2018-03-22 23:16:37.125412: step 84710, loss = 0.61 (270.8 examples/sec; 0.473 sec/batch)
2018-03-22 23:16:41.734444: step 84720, loss = 0.57 (277.7 examples/sec; 0.461 sec/batch)
2018-03-22 23:16:46.187356: step 84730, loss = 0.70 (287.5 examples/sec; 0.445 sec/batch)
2018-03-22 23:16:51.252426: step 84740, loss = 0.91 (252.7 examples/sec; 0.507 sec/batch)
2018-03-22 23:16:55.964410: step 84750, loss = 0.61 (271.6 examples/sec; 0.471 sec/batch)
2018-03-22 23:17:01.027343: step 84760, loss = 0.63 (252.8 examples/sec; 0.506 sec/batch)
2018-03-22 23:17:05.752327: step 84770, loss = 0.90 (270.9 examples/sec; 0.472 sec/batch)
2018-03-22 23:17:10.666954: step 84780, loss = 0.58 (260.4 examples/sec; 0.491 sec/batch)
2018-03-22 23:17:15.105328: step 84790, loss = 0.64 (288.4 examples/sec; 0.444 sec/batch)
2018-03-22 23:17:20.086157: step 84800, loss = 0.75 (257.0 examples/sec; 0.498 sec/batch)
2018-03-22 23:17:24.605089: step 84810, loss = 0.75 (283.3 examples/sec; 0.452 sec/batch)
2018-03-22 23:17:29.256399: step 84820, loss = 0.80 (275.2 examples/sec; 0.465 sec/batch)
2018-03-22 23:17:34.032358: step 84830, loss = 0.64 (268.0 examples/sec; 0.478 sec/batch)
2018-03-22 23:17:38.622443: step 84840, loss = 0.61 (278.9 examples/sec; 0.459 sec/batch)
2018-03-22 23:17:43.409456: step 84850, loss = 0.69 (267.4 examples/sec; 0.479 sec/batch)
2018-03-22 23:17:48.108403: step 84860, loss = 0.74 (272.4 examples/sec; 0.470 sec/batch)
2018-03-22 23:17:52.972355: step 84870, loss = 0.73 (263.2 examples/sec; 0.486 sec/batch)
2018-03-22 23:17:57.883254: step 84880, loss = 0.86 (260.6 examples/sec; 0.491 sec/batch)
2018-03-22 23:18:02.802449: step 84890, loss = 0.67 (260.2 examples/sec; 0.492 sec/batch)
2018-03-22 23:18:07.555464: step 84900, loss = 0.77 (269.3 examples/sec; 0.475 sec/batch)
2018-03-22 23:18:12.348954: step 84910, loss = 0.72 (267.0 examples/sec; 0.479 sec/batch)
2018-03-22 23:18:17.233407: step 84920, loss = 0.68 (262.1 examples/sec; 0.488 sec/batch)
2018-03-22 23:18:22.516872: step 84930, loss = 0.82 (242.3 examples/sec; 0.528 sec/batch)
2018-03-22 23:18:27.068069: step 84940, loss = 0.69 (281.2 examples/sec; 0.455 sec/batch)
2018-03-22 23:18:32.038164: step 84950, loss = 0.84 (257.5 examples/sec; 0.497 sec/batch)
2018-03-22 23:18:36.250317: step 84960, loss = 0.79 (303.9 examples/sec; 0.421 sec/batch)
2018-03-22 23:18:41.212480: step 84970, loss = 0.75 (258.0 examples/sec; 0.496 sec/batch)
2018-03-22 23:18:45.929367: step 84980, loss = 0.70 (271.4 examples/sec; 0.472 sec/batch)
2018-03-22 23:18:51.088440: step 84990, loss = 0.85 (248.1 examples/sec; 0.516 sec/batch)
2018-03-22 23:18:55.999006: step 85000, loss = 0.74 (260.7 examples/sec; 0.491 sec/batch)
2018-03-22 23:19:00.989397: step 85010, loss = 0.77 (256.5 examples/sec; 0.499 sec/batch)
2018-03-22 23:19:05.251359: step 85020, loss = 0.69 (300.3 examples/sec; 0.426 sec/batch)
2018-03-22 23:19:10.434565: step 85030, loss = 0.75 (247.0 examples/sec; 0.518 sec/batch)
2018-03-22 23:19:15.070407: step 85040, loss = 0.88 (276.1 examples/sec; 0.464 sec/batch)
2018-03-22 23:19:20.183322: step 85050, loss = 0.70 (250.3 examples/sec; 0.511 sec/batch)
2018-03-22 23:19:24.850300: step 85060, loss = 0.67 (274.3 examples/sec; 0.467 sec/batch)
2018-03-22 23:19:29.734819: step 85070, loss = 0.74 (262.1 examples/sec; 0.488 sec/batch)
2018-03-22 23:19:34.337764: step 85080, loss = 0.72 (278.1 examples/sec; 0.460 sec/batch)
2018-03-22 23:19:39.374375: step 85090, loss = 0.70 (254.1 examples/sec; 0.504 sec/batch)
2018-03-22 23:19:44.941077: step 85100, loss = 0.68 (229.9 examples/sec; 0.557 sec/batch)
2018-03-22 23:19:50.248487: step 85110, loss = 0.81 (241.2 examples/sec; 0.531 sec/batch)
2018-03-22 23:19:54.693377: step 85120, loss = 0.71 (288.0 examples/sec; 0.444 sec/batch)
2018-03-22 23:19:58.989336: step 85130, loss = 0.64 (298.0 examples/sec; 0.430 sec/batch)
2018-03-22 23:20:03.409391: step 85140, loss = 0.68 (289.6 examples/sec; 0.442 sec/batch)
2018-03-22 23:20:07.631800: step 85150, loss = 0.79 (303.1 examples/sec; 0.422 sec/batch)
2018-03-22 23:20:11.904819: step 85160, loss = 0.74 (299.6 examples/sec; 0.427 sec/batch)
2018-03-22 23:20:16.695442: step 85170, loss = 0.65 (267.2 examples/sec; 0.479 sec/batch)
2018-03-22 23:20:21.937839: step 85180, loss = 0.76 (244.2 examples/sec; 0.524 sec/batch)
2018-03-22 23:20:26.948409: step 85190, loss = 0.62 (255.5 examples/sec; 0.501 sec/batch)
2018-03-22 23:20:32.365514: step 85200, loss = 0.60 (236.3 examples/sec; 0.542 sec/batch)
2018-03-22 23:20:36.743095: step 85210, loss = 0.56 (292.4 examples/sec; 0.438 sec/batch)
2018-03-22 23:20:41.478379: step 85220, loss = 0.66 (270.3 examples/sec; 0.474 sec/batch)
2018-03-22 23:20:46.066472: step 85230, loss = 0.75 (279.0 examples/sec; 0.459 sec/batch)
2018-03-22 23:20:50.985363: step 85240, loss = 0.59 (260.2 examples/sec; 0.492 sec/batch)
2018-03-22 23:20:55.738304: step 85250, loss = 0.78 (269.3 examples/sec; 0.475 sec/batch)
2018-03-22 23:21:00.697330: step 85260, loss = 0.72 (258.1 examples/sec; 0.496 sec/batch)
2018-03-22 23:21:05.281215: step 85270, loss = 0.75 (279.2 examples/sec; 0.458 sec/batch)
2018-03-22 23:21:10.106421: step 85280, loss = 0.52 (265.3 examples/sec; 0.483 sec/batch)
2018-03-22 23:21:14.493317: step 85290, loss = 0.69 (291.8 examples/sec; 0.439 sec/batch)
2018-03-22 23:21:19.751220: step 85300, loss = 0.68 (243.4 examples/sec; 0.526 sec/batch)
2018-03-22 23:21:24.433401: step 85310, loss = 0.75 (273.4 examples/sec; 0.468 sec/batch)
2018-03-22 23:21:29.467326: step 85320, loss = 0.56 (254.3 examples/sec; 0.503 sec/batch)
2018-03-22 23:21:34.183390: step 85330, loss = 0.82 (271.4 examples/sec; 0.472 sec/batch)
2018-03-22 23:21:39.369877: step 85340, loss = 0.67 (246.8 examples/sec; 0.519 sec/batch)
2018-03-22 23:21:44.159393: step 85350, loss = 0.94 (267.3 examples/sec; 0.479 sec/batch)
2018-03-22 23:21:48.799365: step 85360, loss = 0.73 (275.9 examples/sec; 0.464 sec/batch)
2018-03-22 23:21:54.080402: step 85370, loss = 0.64 (242.4 examples/sec; 0.528 sec/batch)
2018-03-22 23:21:58.770791: step 85380, loss = 0.61 (272.9 examples/sec; 0.469 sec/batch)
2018-03-22 23:22:03.581820: step 85390, loss = 0.71 (266.1 examples/sec; 0.481 sec/batch)
2018-03-22 23:22:08.493613: step 85400, loss = 0.72 (260.6 examples/sec; 0.491 sec/batch)
2018-03-22 23:22:13.230330: step 85410, loss = 0.83 (270.2 examples/sec; 0.474 sec/batch)
2018-03-22 23:22:18.153336: step 85420, loss = 0.74 (260.0 examples/sec; 0.492 sec/batch)
2018-03-22 23:22:23.125365: step 85430, loss = 0.77 (257.4 examples/sec; 0.497 sec/batch)
2018-03-22 23:22:27.640427: step 85440, loss = 0.75 (283.5 examples/sec; 0.452 sec/batch)
2018-03-22 23:22:32.744390: step 85450, loss = 0.76 (250.8 examples/sec; 0.510 sec/batch)
2018-03-22 23:22:37.211270: step 85460, loss = 0.78 (286.6 examples/sec; 0.447 sec/batch)
2018-03-22 23:22:42.644306: step 85470, loss = 0.61 (235.6 examples/sec; 0.543 sec/batch)
2018-03-22 23:22:47.355377: step 85480, loss = 0.71 (271.7 examples/sec; 0.471 sec/batch)
2018-03-22 23:22:52.516422: step 85490, loss = 0.79 (248.0 examples/sec; 0.516 sec/batch)
2018-03-22 23:22:57.305928: step 85500, loss = 0.75 (267.3 examples/sec; 0.479 sec/batch)
2018-03-22 23:23:02.152446: step 85510, loss = 0.63 (264.1 examples/sec; 0.485 sec/batch)
2018-03-22 23:23:07.111337: step 85520, loss = 0.55 (258.1 examples/sec; 0.496 sec/batch)
2018-03-22 23:23:12.247311: step 85530, loss = 0.69 (249.2 examples/sec; 0.514 sec/batch)
2018-03-22 23:23:16.818412: step 85540, loss = 0.60 (280.0 examples/sec; 0.457 sec/batch)
2018-03-22 23:23:21.615951: step 85550, loss = 0.64 (266.8 examples/sec; 0.480 sec/batch)
2018-03-22 23:23:26.347435: step 85560, loss = 0.70 (270.5 examples/sec; 0.473 sec/batch)
2018-03-22 23:23:31.553870: step 85570, loss = 0.80 (245.8 examples/sec; 0.521 sec/batch)
2018-03-22 23:23:36.195311: step 85580, loss = 0.74 (275.8 examples/sec; 0.464 sec/batch)
2018-03-22 23:23:40.796455: step 85590, loss = 0.62 (278.2 examples/sec; 0.460 sec/batch)
2018-03-22 23:23:45.458679: step 85600, loss = 0.57 (274.5 examples/sec; 0.466 sec/batch)
2018-03-22 23:23:50.074002: step 85610, loss = 0.65 (277.3 examples/sec; 0.462 sec/batch)
2018-03-22 23:23:54.222667: step 85620, loss = 0.54 (308.5 examples/sec; 0.415 sec/batch)
2018-03-22 23:23:58.180415: step 85630, loss = 0.65 (323.4 examples/sec; 0.396 sec/batch)
2018-03-22 23:24:02.466979: step 85640, loss = 0.75 (298.6 examples/sec; 0.429 sec/batch)
2018-03-22 23:24:06.422945: step 85650, loss = 0.69 (323.5 examples/sec; 0.396 sec/batch)
2018-03-22 23:24:11.189912: step 85660, loss = 0.81 (268.5 examples/sec; 0.477 sec/batch)
2018-03-22 23:24:15.970320: step 85670, loss = 0.81 (267.8 examples/sec; 0.478 sec/batch)
2018-03-22 23:24:21.592505: step 85680, loss = 0.61 (227.7 examples/sec; 0.562 sec/batch)
2018-03-22 23:24:26.284222: step 85690, loss = 0.77 (272.8 examples/sec; 0.469 sec/batch)
2018-03-22 23:24:31.382323: step 85700, loss = 0.68 (251.1 examples/sec; 0.510 sec/batch)
2018-03-22 23:24:35.799303: step 85710, loss = 0.62 (289.8 examples/sec; 0.442 sec/batch)
2018-03-22 23:24:40.731433: step 85720, loss = 0.85 (259.5 examples/sec; 0.493 sec/batch)
2018-03-22 23:24:45.432355: step 85730, loss = 0.67 (272.3 examples/sec; 0.470 sec/batch)
2018-03-22 23:24:50.740451: step 85740, loss = 0.58 (241.1 examples/sec; 0.531 sec/batch)
2018-03-22 23:24:55.087364: step 85750, loss = 0.61 (294.5 examples/sec; 0.435 sec/batch)
2018-03-22 23:25:00.339183: step 85760, loss = 0.76 (243.7 examples/sec; 0.525 sec/batch)
2018-03-22 23:25:04.916258: step 85770, loss = 0.72 (279.7 examples/sec; 0.458 sec/batch)
2018-03-22 23:25:10.009398: step 85780, loss = 0.81 (251.3 examples/sec; 0.509 sec/batch)
2018-03-22 23:25:14.987256: step 85790, loss = 0.65 (257.1 examples/sec; 0.498 sec/batch)
2018-03-22 23:25:20.094494: step 85800, loss = 0.69 (250.6 examples/sec; 0.511 sec/batch)
2018-03-22 23:25:25.062891: step 85810, loss = 0.76 (257.6 examples/sec; 0.497 sec/batch)
2018-03-22 23:25:29.851389: step 85820, loss = 0.56 (267.3 examples/sec; 0.479 sec/batch)
2018-03-22 23:25:34.819429: step 85830, loss = 0.70 (257.6 examples/sec; 0.497 sec/batch)
2018-03-22 23:25:39.842394: step 85840, loss = 0.64 (254.8 examples/sec; 0.502 sec/batch)
2018-03-22 23:25:44.742084: step 85850, loss = 0.97 (261.2 examples/sec; 0.490 sec/batch)
2018-03-22 23:25:49.654792: step 85860, loss = 0.60 (260.5 examples/sec; 0.491 sec/batch)
2018-03-22 23:25:54.490869: step 85870, loss = 0.58 (264.7 examples/sec; 0.484 sec/batch)
2018-03-22 23:25:59.187591: step 85880, loss = 0.72 (272.5 examples/sec; 0.470 sec/batch)
2018-03-22 23:26:04.219401: step 85890, loss = 0.61 (254.4 examples/sec; 0.503 sec/batch)
2018-03-22 23:26:09.096528: step 85900, loss = 0.70 (262.4 examples/sec; 0.488 sec/batch)
2018-03-22 23:26:13.891690: step 85910, loss = 0.68 (266.9 examples/sec; 0.480 sec/batch)
2018-03-22 23:26:18.929399: step 85920, loss = 0.65 (254.1 examples/sec; 0.504 sec/batch)
2018-03-22 23:26:24.130931: step 85930, loss = 0.91 (246.1 examples/sec; 0.520 sec/batch)
2018-03-22 23:26:28.796170: step 85940, loss = 0.59 (274.4 examples/sec; 0.467 sec/batch)
2018-03-22 23:26:33.938359: step 85950, loss = 0.71 (248.9 examples/sec; 0.514 sec/batch)
2018-03-22 23:26:38.541647: step 85960, loss = 0.71 (278.1 examples/sec; 0.460 sec/batch)
2018-03-22 23:26:43.704355: step 85970, loss = 0.71 (247.9 examples/sec; 0.516 sec/batch)
2018-03-22 23:26:48.425334: step 85980, loss = 0.71 (271.1 examples/sec; 0.472 sec/batch)
2018-03-22 23:26:53.271410: step 85990, loss = 0.73 (264.1 examples/sec; 0.485 sec/batch)
2018-03-22 23:26:58.131810: step 86000, loss = 0.65 (263.4 examples/sec; 0.486 sec/batch)
2018-03-22 23:27:03.305015: step 86010, loss = 0.80 (247.4 examples/sec; 0.517 sec/batch)
2018-03-22 23:27:08.072345: step 86020, loss = 0.69 (268.5 examples/sec; 0.477 sec/batch)
2018-03-22 23:27:13.075910: step 86030, loss = 0.75 (255.8 examples/sec; 0.500 sec/batch)
2018-03-22 23:27:17.891893: step 86040, loss = 0.69 (265.8 examples/sec; 0.482 sec/batch)
2018-03-22 23:27:22.553473: step 86050, loss = 0.82 (274.6 examples/sec; 0.466 sec/batch)
2018-03-22 23:27:27.224372: step 86060, loss = 0.78 (274.0 examples/sec; 0.467 sec/batch)
2018-03-22 23:27:32.251641: step 86070, loss = 0.67 (254.6 examples/sec; 0.503 sec/batch)
2018-03-22 23:27:36.804606: step 86080, loss = 0.67 (281.1 examples/sec; 0.455 sec/batch)
2018-03-22 23:27:41.921516: step 86090, loss = 0.74 (250.2 examples/sec; 0.512 sec/batch)
2018-03-22 23:27:46.256141: step 86100, loss = 0.78 (295.3 examples/sec; 0.433 sec/batch)
2018-03-22 23:27:50.712360: step 86110, loss = 0.76 (287.2 examples/sec; 0.446 sec/batch)
2018-03-22 23:27:54.854503: step 86120, loss = 0.53 (309.0 examples/sec; 0.414 sec/batch)
2018-03-22 23:27:59.362170: step 86130, loss = 0.88 (284.0 examples/sec; 0.451 sec/batch)
2018-03-22 23:28:03.889345: step 86140, loss = 0.60 (282.7 examples/sec; 0.453 sec/batch)
2018-03-22 23:28:08.797305: step 86150, loss = 0.75 (260.8 examples/sec; 0.491 sec/batch)
2018-03-22 23:28:13.949374: step 86160, loss = 0.68 (248.4 examples/sec; 0.515 sec/batch)
2018-03-22 23:28:18.397686: step 86170, loss = 0.64 (287.7 examples/sec; 0.445 sec/batch)
2018-03-22 23:28:23.343171: step 86180, loss = 0.58 (258.8 examples/sec; 0.495 sec/batch)
2018-03-22 23:28:27.302204: step 86190, loss = 0.80 (323.3 examples/sec; 0.396 sec/batch)
2018-03-22 23:28:32.254405: step 86200, loss = 0.85 (258.5 examples/sec; 0.495 sec/batch)
2018-03-22 23:28:36.922348: step 86210, loss = 0.65 (274.2 examples/sec; 0.467 sec/batch)
2018-03-22 23:28:42.019435: step 86220, loss = 0.71 (251.1 examples/sec; 0.510 sec/batch)
2018-03-22 23:28:46.744199: step 86230, loss = 0.62 (270.9 examples/sec; 0.472 sec/batch)
2018-03-22 23:28:51.870314: step 86240, loss = 0.69 (249.7 examples/sec; 0.513 sec/batch)
2018-03-22 23:28:56.518307: step 86250, loss = 0.78 (275.4 examples/sec; 0.465 sec/batch)
2018-03-22 23:29:01.374350: step 86260, loss = 0.68 (263.6 examples/sec; 0.486 sec/batch)
2018-03-22 23:29:06.214448: step 86270, loss = 0.73 (264.5 examples/sec; 0.484 sec/batch)
2018-03-22 23:29:11.269979: step 86280, loss = 0.77 (253.2 examples/sec; 0.506 sec/batch)
2018-03-22 23:29:16.056603: step 86290, loss = 0.71 (267.4 examples/sec; 0.479 sec/batch)
2018-03-22 23:29:20.720552: step 86300, loss = 0.74 (274.4 examples/sec; 0.466 sec/batch)
2018-03-22 23:29:25.516337: step 86310, loss = 0.60 (266.9 examples/sec; 0.480 sec/batch)
2018-03-22 23:29:30.609308: step 86320, loss = 0.62 (251.3 examples/sec; 0.509 sec/batch)
2018-03-22 23:29:35.457356: step 86330, loss = 0.71 (264.0 examples/sec; 0.485 sec/batch)
2018-03-22 23:29:40.949353: step 86340, loss = 0.73 (233.1 examples/sec; 0.549 sec/batch)
2018-03-22 23:29:45.746634: step 86350, loss = 0.81 (266.8 examples/sec; 0.480 sec/batch)
2018-03-22 23:29:50.777745: step 86360, loss = 0.74 (254.4 examples/sec; 0.503 sec/batch)
2018-03-22 23:29:55.330366: step 86370, loss = 0.70 (281.2 examples/sec; 0.455 sec/batch)
2018-03-22 23:30:00.573367: step 86380, loss = 0.66 (244.1 examples/sec; 0.524 sec/batch)
2018-03-22 23:30:05.012288: step 86390, loss = 0.66 (288.4 examples/sec; 0.444 sec/batch)
2018-03-22 23:30:10.047266: step 86400, loss = 0.78 (254.2 examples/sec; 0.503 sec/batch)
2018-03-22 23:30:15.015432: step 86410, loss = 0.80 (257.6 examples/sec; 0.497 sec/batch)
2018-03-22 23:30:19.832493: step 86420, loss = 0.74 (265.7 examples/sec; 0.482 sec/batch)
2018-03-22 23:30:24.818113: step 86430, loss = 0.71 (256.7 examples/sec; 0.499 sec/batch)
2018-03-22 23:30:29.444407: step 86440, loss = 0.65 (276.7 examples/sec; 0.463 sec/batch)
2018-03-22 23:30:34.681535: step 86450, loss = 0.78 (244.4 examples/sec; 0.524 sec/batch)
2018-03-22 23:30:38.838394: step 86460, loss = 0.84 (307.9 examples/sec; 0.416 sec/batch)
2018-03-22 23:30:43.942971: step 86470, loss = 0.83 (250.8 examples/sec; 0.510 sec/batch)
2018-03-22 23:30:48.296716: step 86480, loss = 0.55 (294.0 examples/sec; 0.435 sec/batch)
2018-03-22 23:30:53.652369: step 86490, loss = 0.69 (239.0 examples/sec; 0.536 sec/batch)
2018-03-22 23:30:58.503954: step 86500, loss = 0.62 (263.8 examples/sec; 0.485 sec/batch)
2018-03-22 23:31:03.486358: step 86510, loss = 0.67 (256.9 examples/sec; 0.498 sec/batch)
2018-03-22 23:31:08.067272: step 86520, loss = 0.73 (279.4 examples/sec; 0.458 sec/batch)
2018-03-22 23:31:13.233355: step 86530, loss = 0.71 (247.8 examples/sec; 0.517 sec/batch)
2018-03-22 23:31:18.015368: step 86540, loss = 0.79 (267.7 examples/sec; 0.478 sec/batch)
2018-03-22 23:31:23.171298: step 86550, loss = 0.81 (248.3 examples/sec; 0.516 sec/batch)
2